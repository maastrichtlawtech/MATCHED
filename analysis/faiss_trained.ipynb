{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e5967c78-0ee2-4ef3-b875-5cb347c6d86a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Code Overview\n",
    "# Performs similarity search for all the trained text-only baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a569c",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09621eb2-64ed-4658-bfe8-954ad88fb665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    import os\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65be179-db80-4a7a-8b41-9f8e51f0a297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = faiss.StandardGpuResources()  # use a single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53bb8d-1443-4f2b-8e38-6ab5b6b82049",
   "metadata": {},
   "source": [
    "# Loading the trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c8435c-54ae-4700-a470-aae6cb47eed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_embeddings(model_name, city):\n",
    "    if model_name == \"declutr_ce\":\n",
    "        emb_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_all/\"\n",
    "    elif model_name == \"declutr_supcon\":\n",
    "        emb_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_SupCononly_all_all/\"\n",
    "    elif model_name == \"declutr_triplet\":\n",
    "        emb_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_tripletonly_all/\"\n",
    "    else:\n",
    "        emb_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_CE+contra_all/\"\n",
    "        \n",
    "    train_label_filename = city + \"_labels_train.pt\"\n",
    "    train_data_filename = city + \"_data_train.pt\"\n",
    "    test_label_filename = city + \"_labels_test.pt\"\n",
    "    test_data_filename = city + \"_data_test.pt\"\n",
    "    \n",
    "    train_emb = torch.load(os.path.join(emb_dir, train_data_filename), map_location=torch.device('cpu'))\n",
    "    train_labels = torch.load(os.path.join(emb_dir, train_label_filename), map_location=torch.device('cpu'))\n",
    "\n",
    "    test_emb = torch.load(os.path.join(emb_dir, test_data_filename), map_location=torch.device('cpu'))\n",
    "    test_labels = torch.load(os.path.join(emb_dir, test_label_filename), map_location=torch.device('cpu'))\n",
    "    \n",
    "    return train_emb, train_labels, test_emb, test_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedbacd8-e1b9-4b75-ae2c-ec53731551cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_combine_embeddings(cities, model_name):\n",
    "    combined_train_emb, combined_train_labels = [], []\n",
    "    combined_test_emb, combined_test_labels = [], []\n",
    "\n",
    "    for city in cities:\n",
    "        if city == \"all\":\n",
    "            continue\n",
    "        train_emb, train_labels, test_emb, test_labels = load_embeddings(model_name, city)\n",
    "        combined_train_emb.append(train_emb)\n",
    "        combined_train_labels.append(train_labels)\n",
    "        combined_test_emb.append(test_emb)\n",
    "        combined_test_labels.append(test_labels)\n",
    "\n",
    "    combined_train_emb = torch.cat(combined_train_emb)\n",
    "    combined_train_labels = torch.cat(combined_train_labels)\n",
    "    combined_test_emb = torch.cat(combined_test_emb)\n",
    "    combined_test_labels = torch.cat(combined_test_labels)\n",
    "\n",
    "    return combined_train_emb, combined_train_labels, combined_test_emb, combined_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa472af-ef70-4dca-8670-a3249a4de322",
   "metadata": {},
   "source": [
    "# R-Precision metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddcfa6e-9456-4e0b-a5c2-9d0154bcd301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall_at_k(actual, predicted, k):\n",
    "    recall_list = []\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())  # Convert PyTorch tensor to NumPy array, then to set\n",
    "        pred_set = set(pred[:k].numpy())  # Convert PyTorch tensor to NumPy array, then to set\n",
    "        recall_list.append(round(len(act_set & pred_set) / float(len(act_set)), 2))\n",
    "    return recall_list\n",
    "\n",
    "def generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    dim = train_embeddings.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    \n",
    "    train_embeddings_np = train_embeddings.numpy()\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels.numpy()\n",
    "\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "\n",
    "    r_precision_score = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        \n",
    "        # Ensure test_vendor_embeddings is a 2D array and convert to numpy\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        \n",
    "        # Check if k is within the valid range\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            print(f\"Warning: k ({k}) is greater than the number of training samples ({train_embeddings_np.shape[0]}), adjusting k to maximum possible.\")\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        try:\n",
    "            # Verify k is an integer and not numpy integer\n",
    "            _, I = gpu_index_flat.search(test_vendor_embeddings_np, int(k))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during FAISS search for vendor_id {vendor_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "        \n",
    "        r_precision_score[vendor_id] = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "    \n",
    "    r_precision_mean = np.mean(list(r_precision_score.values()))\n",
    "    r_precision_std = np.std(list(r_precision_score.values()))\n",
    "\n",
    "    print(f\"R precision mean: {round(r_precision_mean, 4)} Â± {round(r_precision_std, 2)}\")\n",
    "    \n",
    "    return r_precision_mean, r_precision_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1402db9-3cec-47d8-98b7-d65d11f3b88f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mrr_at_k(actual, predicted, k_):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR) at K_.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list of Tensors): List of tensors containing the actual labels.\n",
    "        predicted (list of Tensors): List of tensors containing the predicted label indices.\n",
    "        k_ (int): The number of top predictions to consider for computing MRR.\n",
    "    \n",
    "    Returns:\n",
    "        float: The MRR@k.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())\n",
    "        pred_set = pred[:k_].numpy()\n",
    "        for i, p in enumerate(pred_set, 1):\n",
    "            if p in act_set:\n",
    "                mrr += 1 / i\n",
    "                break\n",
    "    return mrr / len(actual) if actual else 0\n",
    "\n",
    "def generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k):\n",
    "    dim = train_embeddings.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "    train_embeddings_np = train_embeddings.numpy()\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "\n",
    "    unique_labels = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels.numpy()\n",
    "\n",
    "    mrr_score = []\n",
    "\n",
    "    for label in tqdm(unique_labels, total=len(unique_labels), desc=f\"Calculating MRR@{k}\"):\n",
    "        label_id = int(label)\n",
    "        test_idx = (test_labels == label_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_embeddings_np = test_embeddings[test_idx].numpy()\n",
    "\n",
    "        D, I = gpu_index_flat.search(test_embeddings_np, 1)  # Search for top-1 nearest neighbors\n",
    "\n",
    "        predicted_label_list = [train_labels[I[index]] for index in range(len(test_idx))]\n",
    "        true_label_list = [train_labels[np.where(train_labels == label_id)[0]] for _ in range(len(test_idx))]\n",
    "        \n",
    "        mrr_score.append(mrr_at_k(true_label_list, predicted_label_list, k_=k))\n",
    "\n",
    "    mrr_mean = np.mean(mrr_score)\n",
    "    mrr_std = np.std(mrr_score)\n",
    "    print(f\"MRR@{k} mean: {round(mrr_mean, 4)} Â± {round(mrr_std, 2)}\")\n",
    "    \n",
    "    return mrr_mean, mrr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b236a7d-6d22-462c-b13c-cfbc50a71a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    dim = train_embeddings.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    \n",
    "    train_embeddings_np = train_embeddings.numpy()\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels.numpy()\n",
    "\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "\n",
    "    r_precision_score = {}\n",
    "    total_samples = 0  # Total number of samples for all classes\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        \n",
    "        # Ensure test_vendor_embeddings is a 2D array and convert to numpy\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        \n",
    "        # Check if k is within the valid range\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            print(f\"Warning: k ({k}) is greater than the number of training samples ({train_embeddings_np.shape[0]}), adjusting k to maximum possible.\")\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        try:\n",
    "            # Verify k is an integer and not numpy integer\n",
    "            _, I = gpu_index_flat.search(test_vendor_embeddings_np, int(k))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during FAISS search for vendor_id {vendor_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "        \n",
    "        r_precision_score[vendor_id] = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "        total_samples += len(test_adsidx)  # Add the number of samples for this class\n",
    "\n",
    "    # Calculate the weighted mean of R-Precision\n",
    "    weighted_r_precision_sum = sum(r_precision_score[vendor_id] * len((test_labels == vendor_id).nonzero(as_tuple=True)[0]) for vendor_id in r_precision_score.keys())\n",
    "    macro_r_precision = weighted_r_precision_sum / total_samples\n",
    "\n",
    "    print(f\"Macro R-precision: {macro_r_precision}\")\n",
    "    \n",
    "    return macro_r_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4b9743-bfa6-4ec8-965a-8e6a4c33e09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(actual, predicted, k):\n",
    "    precision_list = []\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())\n",
    "        pred_set = set(pred[:k].numpy())\n",
    "        precision_list.append(len(act_set & pred_set) / float(k))\n",
    "    return precision_list\n",
    "\n",
    "def f1_at_k(actual, predicted, k):\n",
    "    f1_list = []\n",
    "    precision_list = precision_at_k(actual, predicted, k)\n",
    "    recall_list = recall_at_k(actual, predicted, k)\n",
    "    for precision, recall in zip(precision_list, recall_list):\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        f1_list.append(f1)\n",
    "    return f1_list\n",
    "\n",
    "def generate_macro_f1_at_x_results(test_embeddings, test_labels):\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels_tensor.numpy()\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "    f1_score_list = []\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating Macro-F1@X\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = index.search(test_vendor_embeddings_np, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels_tensor == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "        \n",
    "        f1_score_list.extend(f1_at_k(true_label_list, predicted_label_list, k))\n",
    "\n",
    "    macro_f1_mean = np.mean(f1_score_list)\n",
    "    macro_f1_std = np.std(f1_score_list)\n",
    "\n",
    "    print(f\"Macro F1@X: {round(macro_f1_mean, 4)} Â± {round(macro_f1_std, 2)}\")\n",
    "    \n",
    "    return macro_f1_mean, macro_f1_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09faa254-5776-49e5-a057-d3b1a19f72ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "dim = None\n",
    "index = None\n",
    "train_embeddings_np = None\n",
    "train_labels_np = None\n",
    "\n",
    "# @profile\n",
    "def initialize_globals(train_embeddings, train_labels):\n",
    "    global dim, index, train_embeddings_np, train_labels_tensor\n",
    "    start_time = time.time()\n",
    "    dim = train_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    \n",
    "    train_embeddings_np = train_embeddings.numpy()\n",
    "    batch_size = 10000\n",
    "    num_batches = (train_embeddings_np.shape[0] + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, train_embeddings_np.shape[0])\n",
    "        batch_embeddings = train_embeddings_np[start_idx:end_idx]\n",
    "        index.add(batch_embeddings)\n",
    "        print(f\"Added batch {i+1}/{num_batches} to index.\")\n",
    "    \n",
    "    train_labels_tensor = train_labels\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Global variables initialized in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    recall_list = []\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())\n",
    "        pred_set = set(pred[:k].numpy())\n",
    "        recall_list.append(len(act_set & pred_set) / float(len(act_set)))\n",
    "    return recall_list\n",
    "\n",
    "def generate_rprecision_results(test_embeddings, test_labels):\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels_tensor.numpy()\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "    r_precision_score = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = index.search(test_vendor_embeddings_np, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels_tensor == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "        \n",
    "        r_precision_score[vendor_id] = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "    \n",
    "    r_precision_mean = np.mean(list(r_precision_score.values()))\n",
    "    r_precision_std = np.std(list(r_precision_score.values()))\n",
    "\n",
    "    print(f\"R precision mean: {round(r_precision_mean, 4)} Â± {round(r_precision_std, 2)}\")\n",
    "    # print(f\"R precision std: {r_precision_std}\")\n",
    "    \n",
    "    return r_precision_mean, r_precision_std\n",
    "\n",
    "def mrr_at_k(actual, predicted, k_):\n",
    "    mrr = 0.0\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())\n",
    "        for i, p in enumerate(pred[:k_].numpy(), 1):\n",
    "            if p in act_set:\n",
    "                mrr += 1 / i\n",
    "                break\n",
    "    return mrr / len(actual) if actual else 0\n",
    "\n",
    "def generate_mrr_at_1_results(test_embeddings, test_labels, k):\n",
    "    unique_labels = torch.unique(test_labels)\n",
    "    mrr_score = []\n",
    "\n",
    "    for label in tqdm(unique_labels, total=len(unique_labels), desc=f\"Calculating MRR@{k}\"):\n",
    "        label_id = int(label)\n",
    "        test_idx = (test_labels == label_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_embeddings_np = test_embeddings[test_idx].numpy()\n",
    "\n",
    "        D, I = index.search(test_embeddings_np, 1)\n",
    "\n",
    "        predicted_label_list = [train_labels_tensor[I[index]] for index in range(len(test_idx))]\n",
    "        true_label_list = [train_labels_tensor[torch.where(train_labels_tensor == label_id)[0]] for _ in range(len(test_idx))]\n",
    "        \n",
    "        mrr_score.append(mrr_at_k(true_label_list, predicted_label_list, k_=k))\n",
    "\n",
    "    mrr_mean = np.mean(mrr_score)\n",
    "    mrr_std = np.std(mrr_score)\n",
    "    print(f\"MRR@{k} mean: {round(mrr_mean, 4)} Â± {round(mrr_std, 2)}\")\n",
    "    # print(f\"MRR@{k} std: {mrr_std}\")\n",
    "    \n",
    "    return mrr_mean, mrr_std\n",
    "\n",
    "def generate_macro_rprecision_results(test_embeddings, test_labels):\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels_tensor.numpy()\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "    r_precision_score = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = index.search(test_vendor_embeddings_np, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels_tensor == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "        \n",
    "        r_precision_score[vendor_id] = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "        total_samples += len(test_adsidx)\n",
    "\n",
    "    weighted_r_precision_sum = sum(r_precision_score[vendor_id] * len((test_labels == vendor_id).nonzero(as_tuple=True)[0]) for vendor_id in r_precision_score.keys())\n",
    "    macro_r_precision = weighted_r_precision_sum / total_samples\n",
    "\n",
    "    print(f\"Macro R-precision: {macro_r_precision}\")\n",
    "    \n",
    "    return macro_r_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9968dd5-057d-42d5-b207-19afa9c2628e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "\n",
    "def remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    \"\"\"\n",
    "    Removes duplicate embeddings between train and test sets using hashing.\n",
    "\n",
    "    Args:\n",
    "        train_embeddings (torch.Tensor): Embeddings from the training set.\n",
    "        train_labels (torch.Tensor): Labels corresponding to the training embeddings.\n",
    "        test_embeddings (torch.Tensor): Embeddings from the test set.\n",
    "        test_labels (torch.Tensor): Labels corresponding to the test embeddings.\n",
    "\n",
    "    Returns:\n",
    "        unique_train_embeddings (torch.Tensor): Unique embeddings from the training set.\n",
    "        unique_train_labels (torch.Tensor): Labels corresponding to the unique training embeddings.\n",
    "        unique_test_embeddings (torch.Tensor): Unique embeddings from the test set (excluding duplicates with train set).\n",
    "        unique_test_labels (torch.Tensor): Labels corresponding to the unique test embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def hash_embedding(embedding):\n",
    "        # Ensure the embedding is contiguous in memory and of type float32\n",
    "        embedding = embedding.contiguous().view(-1).float()\n",
    "        # Round to reduce the impact of floating-point precision errors\n",
    "        embedding = torch.round(embedding * 1e6) / 1e6  # Adjust precision as needed\n",
    "        # Convert the embedding to bytes\n",
    "        emb_bytes = embedding.numpy().tobytes()\n",
    "        # Compute MD5 hash\n",
    "        return hashlib.md5(emb_bytes).hexdigest()\n",
    "\n",
    "    # Create dictionaries mapping hashes to embeddings and labels for the train set\n",
    "    train_hash_embedding_label_dict = {}\n",
    "    for emb, label in zip(train_embeddings, train_labels):\n",
    "        emb_hash = hash_embedding(emb)\n",
    "        # Store the embedding and label only if the hash is not already in the dictionary\n",
    "        if emb_hash not in train_hash_embedding_label_dict:\n",
    "            train_hash_embedding_label_dict[emb_hash] = (emb, label.item())\n",
    "\n",
    "    # Create dictionaries mapping hashes to embeddings and labels for the test set\n",
    "    test_hash_embedding_label_dict = {}\n",
    "    for emb, label in zip(test_embeddings, test_labels):\n",
    "        emb_hash = hash_embedding(emb)\n",
    "        # Store the embedding and label only if the hash is not already in the dictionary\n",
    "        if emb_hash not in test_hash_embedding_label_dict:\n",
    "            test_hash_embedding_label_dict[emb_hash] = (emb, label.item())\n",
    "\n",
    "    # Identify common hashes between train and test sets\n",
    "    common_hashes = set(train_hash_embedding_label_dict.keys()).intersection(set(test_hash_embedding_label_dict.keys()))\n",
    "\n",
    "    # Remove duplicates from the test set\n",
    "    unique_test_hashes = set(test_hash_embedding_label_dict.keys()) - common_hashes\n",
    "\n",
    "    # Reconstruct unique embeddings and labels for the train set\n",
    "    unique_train_embeddings_list = [emb_label[0] for emb_label in train_hash_embedding_label_dict.values()]\n",
    "    unique_train_labels_list = [emb_label[1] for emb_label in train_hash_embedding_label_dict.values()]\n",
    "\n",
    "    # Reconstruct unique embeddings and labels for the test set\n",
    "    unique_test_embeddings_list = [test_hash_embedding_label_dict[h][0] for h in unique_test_hashes]\n",
    "    unique_test_labels_list = [test_hash_embedding_label_dict[h][1] for h in unique_test_hashes]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    unique_train_embeddings = torch.stack(unique_train_embeddings_list)\n",
    "    unique_train_labels = torch.tensor(unique_train_labels_list)\n",
    "\n",
    "    unique_test_embeddings = torch.stack(unique_test_embeddings_list)\n",
    "    unique_test_labels = torch.tensor(unique_test_labels_list)\n",
    "\n",
    "    return unique_train_embeddings, unique_train_labels, unique_test_embeddings, unique_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970a6d97-f9f3-47bb-8a5c-9c7328b61b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_members(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    \"\"\"\n",
    "    Filters test_embeddings and test_labels by removing entries whose labels do not exist in train_labels.\n",
    "\n",
    "    Parameters:\n",
    "    - train_embeddings (torch.Tensor): Embeddings for the training data.\n",
    "    - train_labels (torch.Tensor): Labels for the training data.\n",
    "    - test_embeddings (torch.Tensor): Embeddings for the test data.\n",
    "    - test_labels (torch.Tensor): Labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    - train_embeddings (torch.Tensor): Original training embeddings (unchanged).\n",
    "    - train_labels (torch.Tensor): Original training labels (unchanged).\n",
    "    - filtered_test_embeddings (torch.Tensor): Filtered test embeddings.\n",
    "    - filtered_test_labels (torch.Tensor): Filtered test labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure labels are on the same device\n",
    "    if train_labels.device != test_labels.device:\n",
    "        train_labels = train_labels.to(test_labels.device)\n",
    "\n",
    "    # Use torch.isin to create a mask of test labels that exist in train labels\n",
    "    if hasattr(torch, 'isin'):\n",
    "        # torch.isin is available in PyTorch 1.10 and later\n",
    "        mask = torch.isin(test_labels, train_labels)\n",
    "    else:\n",
    "        # For older versions of PyTorch, convert to NumPy arrays\n",
    "        train_labels_np = train_labels.cpu().numpy()\n",
    "        test_labels_np = test_labels.cpu().numpy()\n",
    "        mask_np = np.isin(test_labels_np, train_labels_np)\n",
    "        mask = torch.from_numpy(mask_np).to(test_labels.device)\n",
    "\n",
    "    # Apply the mask to filter test embeddings and labels\n",
    "    filtered_test_embeddings = test_embeddings[mask]\n",
    "    filtered_test_labels = test_labels[mask]\n",
    "\n",
    "    return train_embeddings, train_labels, filtered_test_embeddings, filtered_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ac042-9c21-401e-8958-d81145b12148",
   "metadata": {},
   "source": [
    "# Declutr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcbe61-04dc-40d6-ad7c-97f31a31845f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_triplet\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = find_members(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 1)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44cae26a-484d-4169-be98-737c5a0620bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.04 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 964/964 [00:10<00:00, 96.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9163 Â± 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 964/964 [00:10<00:00, 95.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.8099 Â± 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 964/964 [00:10<00:00, 94.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.8446 Â± 0.24\n",
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 624/624 [00:04<00:00, 145.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.7932 Â± 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 624/624 [00:04<00:00, 142.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6615 Â± 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 624/624 [00:04<00:00, 143.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6544 Â± 0.36\n",
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 259/259 [00:00<00:00, 333.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.868 Â± 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 259/259 [00:00<00:00, 336.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.8013 Â± 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 259/259 [00:00<00:00, 329.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.766 Â± 0.3\n",
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 293/293 [00:00<00:00, 443.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.776 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 293/293 [00:00<00:00, 441.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.707 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 293/293 [00:00<00:00, 440.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.7279 Â± 0.38\n",
      "Mean average: 0.4276 Â± 0.18\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_supcon\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = find_members(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # Normalize training embeddings\n",
    "    faiss.normalize_L2(train_embeddings.detach().cpu().numpy())\n",
    "    # Normalize test embeddings\n",
    "    faiss.normalize_L2(test_embeddings.detach().cpu().numpy())\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 1)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21540fd-50b9-4e31-98c7-19180030d553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 975/975 [00:10<00:00, 93.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7361 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 975/975 [00:10<00:00, 93.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.5557 Â± 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 975/975 [00:10<00:00, 95.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6098 Â± 0.35\n",
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 633/633 [00:04<00:00, 146.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.5622 Â± 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 633/633 [00:04<00:00, 148.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.4596 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 633/633 [00:04<00:00, 149.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.476 Â± 0.38\n",
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 268/268 [00:00<00:00, 350.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.6179 Â± 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 268/268 [00:00<00:00, 347.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.5842 Â± 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 268/268 [00:00<00:00, 314.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6123 Â± 0.35\n",
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 309/309 [00:00<00:00, 448.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.5558 Â± 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 309/309 [00:00<00:00, 467.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.4944 Â± 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 309/309 [00:00<00:00, 462.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.5042 Â± 0.42\n",
      "Mean average: 0.3146 Â± 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_ce\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 10)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a5719d-90a9-432e-b83a-231d005bde00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.04 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 964/964 [00:10<00:00, 96.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.8759 Â± 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 964/964 [00:10<00:00, 94.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.685 Â± 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 964/964 [00:10<00:00, 93.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.7171 Â± 0.31\n",
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 624/624 [00:04<00:00, 141.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7303 Â± 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 624/624 [00:04<00:00, 142.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6074 Â± 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 624/624 [00:04<00:00, 142.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6012 Â± 0.38\n",
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 259/259 [00:00<00:00, 342.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.8155 Â± 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 259/259 [00:00<00:00, 328.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.7463 Â± 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 259/259 [00:00<00:00, 332.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.7242 Â± 0.32\n",
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 293/293 [00:00<00:00, 460.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7332 Â± 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 293/293 [00:00<00:00, 439.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6401 Â± 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 293/293 [00:00<00:00, 448.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6542 Â± 0.4\n",
      "Mean average: 0.3852 Â± 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_ce\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = find_members(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # Normalize training embeddings\n",
    "    faiss.normalize_L2(train_embeddings.detach().cpu().numpy())\n",
    "    # Normalize test embeddings\n",
    "    faiss.normalize_L2(test_embeddings.detach().cpu().numpy())\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 10)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a41fb85-eba1-42a1-82e1-b904003b16e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 975/975 [00:10<00:00, 90.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.8729 Â± 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 975/975 [00:10<00:00, 90.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.7673 Â± 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 975/975 [00:10<00:00, 90.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.8157 Â± 0.27\n",
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 633/633 [00:04<00:00, 150.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7527 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 633/633 [00:04<00:00, 150.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6346 Â± 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 633/633 [00:04<00:00, 151.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6333 Â± 0.36\n",
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 268/268 [00:00<00:00, 344.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.8051 Â± 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 268/268 [00:00<00:00, 342.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.7612 Â± 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 268/268 [00:00<00:00, 349.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.7408 Â± 0.31\n",
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 309/309 [00:00<00:00, 484.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7131 Â± 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 309/309 [00:00<00:00, 479.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6707 Â± 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 309/309 [00:00<00:00, 491.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.695 Â± 0.39\n",
      "Mean average: 0.4121 Â± 0.19\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_supcon\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 10)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80c57b44-6ea4-481e-b4f9-3d9b343e94e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 964/964 [00:10<00:00, 95.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.9163 Â± 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 964/964 [00:10<00:00, 93.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.8099 Â± 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 964/964 [00:10<00:00, 92.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.8446 Â± 0.24\n",
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 624/624 [00:04<00:00, 144.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.7932 Â± 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 624/624 [00:04<00:00, 146.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.6615 Â± 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 624/624 [00:04<00:00, 144.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.6544 Â± 0.36\n",
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 259/259 [00:00<00:00, 341.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.868 Â± 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 259/259 [00:00<00:00, 326.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.8013 Â± 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 259/259 [00:00<00:00, 327.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.766 Â± 0.3\n",
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 293/293 [00:00<00:00, 448.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 mean: 0.776 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 293/293 [00:00<00:00, 435.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.707 Â± 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Macro-F1@X: 100%|ââââââââââ| 293/293 [00:00<00:00, 442.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1@X: 0.7279 Â± 0.38\n",
      "Mean average: 0.4276 Â± 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "r_precision_dict, mrr_dict1, mrr_dict10, mrr_dict100 = ({} for i in range(4))\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_supcon\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = find_members(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # Normalize training embeddings\n",
    "    faiss.normalize_L2(train_embeddings.detach().cpu().numpy())\n",
    "    # Normalize test embeddings\n",
    "    faiss.normalize_L2(test_embeddings.detach().cpu().numpy())\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mean, std = generate_mrr_at_1_results(test_embeddings, test_labels, 10)\n",
    "        mean, std = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        mean, std  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        mean_total += mean\n",
    "        std_total += std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # _, _ = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    # mrr_mean_1, mrr_std_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=1)\n",
    "    # _, _ = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels, k=10)\n",
    "    # mrr_mean_100, mrr_std_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, k=100)\n",
    "    # macro_f_1 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 1)\n",
    "    # macro_f_10 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "    # macro_f_100 = generate_macro_f1_at_k_results(train_embeddings, train_labels, test_embeddings, test_labels, 100)\n",
    "    # _ = generate_macro_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    # r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    # mrr_dict1[city_] = (mrr_mean_1, mrr_std_1)\n",
    "    # mrr_dict10[city_] = (mrr_mean_10, mrr_std_10)\n",
    "    # mrr_dict100[city_] = (mrr_mean_100, mrr_std_100)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "#    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr1.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr10.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mrr_dict10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr100.pickle', 'wb') as handle:\n",
    "#    pickle.dump(mrr_dict100, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Mean average: {round(float(mean_total / 7), 4)} Â± {round(float(std_total / 7), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "478c2cfd-9452-44b7-91f7-2f53939d2dee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 490/490 [00:00<00:00, 2575.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9616027318499994\n",
      "R precision std: 0.17722079615936118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 490/490 [00:00<00:00, 3591.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9472761037539362\n",
      "MRR@1 std: 0.2189324097600345\n",
      "City: atlanta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 471/471 [00:00<00:00, 3663.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9577687978687979\n",
      "R precision std: 0.177517277763828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 471/471 [00:00<00:00, 4254.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.936395805185614\n",
      "MRR@1 std: 0.24103923617661122\n",
      "City: detroit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 153/153 [00:00<00:00, 3908.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9662666666666666\n",
      "R precision std: 0.14521499769498866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 153/153 [00:00<00:00, 4500.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9673202614379085\n",
      "MRR@1 std: 0.1777970001141888\n",
      "City: houston\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 422/422 [00:00<00:00, 3042.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9538112315672145\n",
      "R precision std: 0.18651454272879214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 422/422 [00:00<00:00, 4032.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9395734597156398\n",
      "MRR@1 std: 0.23226006208877056\n",
      "City: dallas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 377/377 [00:00<00:00, 3775.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9497255440637793\n",
      "R precision std: 0.20059073886018863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 377/377 [00:00<00:00, 4337.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.925287356321839\n",
      "MRR@1 std: 0.25940053363222754\n",
      "City: ny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 309/309 [00:00<00:00, 4381.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9736405005688281\n",
      "R precision std: 0.13597950278647924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 309/309 [00:00<00:00, 4759.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9352750809061489\n",
      "MRR@1 std: 0.24603984218444291\n",
      "City: df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 268/268 [00:00<00:00, 3575.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.9575560506441259\n",
      "R precision std: 0.19128800864810627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating MRR@1: 100%|ââââââââââ| 268/268 [00:00<00:00, 4049.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.9291044776119403\n",
      "MRR@1 std: 0.2566502431196663\n",
      "City: canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 149/149 [00:00<00:00, 3611.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.8974538690476191\n",
      "R precision std: 0.27297316875569755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 149/149 [00:00<00:00, 4639.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.8559843400447427\n",
      "MRR@1 std: 0.34306553343642904\n"
     ]
    }
   ],
   "source": [
    "# all\n",
    "r_precision_dict, mrr_dict = ({} for i in range(2))\n",
    "for city_ in [\"chicago\", \"atlanta\", \"detroit\", \"houston\", \"dallas\", \"ny\", \"df\", \"canada\"]:\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr\", city=city_)\n",
    "    r_precision_mean, r_precision_std = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    mrr_mean, mrr_std = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    mrr_dict[city_] = (mrr_mean, mrr_std)\n",
    "    \n",
    "with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/rprecision.pickle', 'wb') as handle:\n",
    "    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../models/pickled/embeddings/pooled_trained_declutr_chicago/mrr.pickle', 'wb') as handle:\n",
    "    pickle.dump(mrr_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1955c0b-e76a-49f6-af56-6c35ebb90cb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Style Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e38aadf-5269-4698-b27f-ccb5a94e8c07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 490/490 [00:00<00:00, 2650.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.23570085502060392\n",
      "R precision std: 0.3501274544330388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 490/490 [00:00<00:00, 3643.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.26969009826152684\n",
      "MRR@1 std: 0.4225519481173043\n",
      "City: atlanta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 471/471 [00:00<00:00, 3690.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.17019464305535734\n",
      "R precision std: 0.28286919583435144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 471/471 [00:00<00:00, 4248.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.1995018428139447\n",
      "MRR@1 std: 0.36392880819794504\n",
      "City: detroit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 153/153 [00:00<00:00, 4009.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.18867699134199134\n",
      "R precision std: 0.29214815049472875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 153/153 [00:00<00:00, 4530.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.22020249177111925\n",
      "MRR@1 std: 0.38491030684397226\n",
      "City: houston\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 422/422 [00:00<00:00, 3339.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.12468577303016073\n",
      "R precision std: 0.2483092055923549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 422/422 [00:00<00:00, 4005.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.15329012746074355\n",
      "MRR@1 std: 0.32737294316489385\n",
      "City: dallas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 377/377 [00:00<00:00, 3614.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.15565606686461567\n",
      "R precision std: 0.2659244798996246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 377/377 [00:00<00:00, 4307.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.20627963202603875\n",
      "MRR@1 std: 0.3736196093193114\n",
      "City: ny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 309/309 [00:00<00:00, 4340.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.18537437022590605\n",
      "R precision std: 0.30974707348257785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 309/309 [00:00<00:00, 4853.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.18756356911696717\n",
      "MRR@1 std: 0.3691417264328915\n",
      "City: df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 268/268 [00:00<00:00, 2366.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.21147590539446448\n",
      "R precision std: 0.3147223322755013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 268/268 [00:00<00:00, 3244.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.21212465651117976\n",
      "MRR@1 std: 0.3843673084707061\n",
      "City: canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating R-precision: 100%|ââââââââââ| 149/149 [00:00<00:00, 4351.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R precision mean: 0.07708698979591837\n",
      "R precision std: 0.25209573579789857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@1: 100%|ââââââââââ| 149/149 [00:00<00:00, 5080.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 mean: 0.07088926174496644\n",
      "MRR@1 std: 0.24583972547768784\n"
     ]
    }
   ],
   "source": [
    "r_precision_dict, mrr_dict = ({} for i in range(2))\n",
    "for city_ in [\"chicago\", \"atlanta\", \"detroit\", \"houston\", \"dallas\", \"ny\", \"df\", \"canada\"]:\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"styleEmbedding\", city=city_)\n",
    "    r_precision_mean, r_precision_std = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    mrr_mean, mrr_std = generate_mrr_at_1_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    r_precision_dict[city_] = (r_precision_mean, r_precision_std)\n",
    "    mrr_dict[city_] = (mrr_mean, mrr_std)\n",
    "    \n",
    "with open('../models/pickled/embeddings/trained_styleEmbedding_chicago/rprecision.pickle', 'wb') as handle:\n",
    "    pickle.dump(r_precision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../models/pickled/embeddings/trained_styleEmbedding_chicago/mrr.pickle', 'wb') as handle:\n",
    "    pickle.dump(mrr_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019ca9c-e0d7-4136-be95-880344e0b581",
   "metadata": {},
   "source": [
    "# Modified scripts with class frequencies and returned dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d5164d7-4451-4986-ad29-ffd4e9ed4819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall_at_k(actual, predicted, k):\n",
    "    recall_list = []\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        act_set = set(act.numpy())  # Convert PyTorch tensor to NumPy array, then to set\n",
    "        pred_set = set(pred[:k].numpy())  # Convert PyTorch tensor to NumPy array, then to set\n",
    "        \n",
    "        # Check if the actual set is empty to avoid division by zero\n",
    "        if len(act_set) == 0:\n",
    "            recall_list.append(0.0)  # Assign recall as 0 if there are no true labels\n",
    "        else:\n",
    "            recall_list.append(round(len(act_set & pred_set) / float(len(act_set)), 2))\n",
    "    \n",
    "    return recall_list\n",
    "\n",
    "\n",
    "def generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    # Convert embeddings from PyTorch tensors to NumPy arrays\n",
    "    train_embeddings_np = train_embeddings.cpu().numpy()\n",
    "    test_embeddings_np = test_embeddings.cpu().numpy()\n",
    "    \n",
    "    dim = train_embeddings_np.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "\n",
    "    # Combine train and test labels to compute class frequency across the entire dataset\n",
    "    combined_labels = torch.cat([train_labels, test_labels], dim=0)\n",
    "    unique_vendors = torch.unique(combined_labels)\n",
    "    \n",
    "    # Compute class frequencies from the entire dataset (train + test)\n",
    "    vendor_freq_dict = {int(vendor_id): (combined_labels == vendor_id).sum().item() for vendor_id in unique_vendors}\n",
    "\n",
    "    # Initialize a dict to store scores by frequency\n",
    "    frequency_score_dict = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings_np[test_adsidx]\n",
    "\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = np.expand_dims(test_vendor_embeddings, axis=0)\n",
    "\n",
    "        k = vendor_freq_dict[vendor_id]  # Use the frequency from the combined dataset\n",
    "        if k == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            _, I = gpu_index_flat.search(test_vendor_embeddings, k)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error searching for vendor_id {vendor_id} with shape {test_vendor_embeddings.shape}: {e}\")\n",
    "            continue\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "\n",
    "        score = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "\n",
    "        # Track performance score by frequency of instances\n",
    "        freq = vendor_freq_dict[vendor_id]  # Get the frequency from the combined dataset\n",
    "        if freq not in frequency_score_dict:\n",
    "            frequency_score_dict[freq] = []\n",
    "        frequency_score_dict[freq].append(score)\n",
    "\n",
    "    # Calculate average score for each frequency\n",
    "    frequency_avg_score_dict = {freq: np.mean(scores) for freq, scores in frequency_score_dict.items()}\n",
    "\n",
    "    return frequency_avg_score_dict\n",
    "\n",
    "\n",
    "def calculate_mrr_at_k(train_embeddings, train_labels, test_embeddings, test_labels, k=10):\n",
    "    # Convert embeddings from PyTorch tensors to NumPy arrays\n",
    "    train_embeddings_np = train_embeddings.cpu().numpy()\n",
    "    test_embeddings_np = test_embeddings.cpu().numpy()\n",
    "    \n",
    "    dim = train_embeddings_np.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "\n",
    "    # Combine train and test labels to compute class frequency across the entire dataset\n",
    "    combined_labels = torch.cat([train_labels, test_labels], dim=0)\n",
    "    unique_vendors = torch.unique(combined_labels)\n",
    "\n",
    "    # Compute class frequencies from the entire dataset (train + test)\n",
    "    vendor_freq_dict = {int(vendor_id): (combined_labels == vendor_id).sum().item() for vendor_id in unique_vendors}\n",
    "\n",
    "    # Initialize a dict to store scores by frequency\n",
    "    frequency_score_dict = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=f\"Calculating MRR@{k}\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "        if len(test_adsidx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings_np[test_adsidx]\n",
    "\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = np.expand_dims(test_vendor_embeddings, axis=0)\n",
    "\n",
    "        try:\n",
    "            D, I = gpu_index_flat.search(test_vendor_embeddings, k)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error searching for vendor_id {vendor_id} with shape {test_vendor_embeddings.shape}: {e}\")\n",
    "            continue\n",
    "\n",
    "        scores = []\n",
    "        for idx, indices in enumerate(I):\n",
    "            correct_indices = np.where(train_labels.numpy() == vendor_id)[0]\n",
    "            for rank, index in enumerate(indices, start=1):\n",
    "                if index in correct_indices:\n",
    "                    scores.append(1.0 / rank)\n",
    "                    break\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "\n",
    "        # Track performance score by frequency of instances\n",
    "        freq = vendor_freq_dict[vendor_id]  # Use the frequency from the combined dataset\n",
    "        if freq not in frequency_score_dict:\n",
    "            frequency_score_dict[freq] = []\n",
    "        frequency_score_dict[freq].extend(scores)\n",
    "\n",
    "    # Calculate average MRR score for each frequency\n",
    "    frequency_avg_mrr_dict = {freq: np.mean(scores) for freq, scores in frequency_score_dict.items()}\n",
    "\n",
    "    return frequency_avg_mrr_dict\n",
    "\n",
    "\n",
    "def generate_macro_f1_at_x_results(train_embeddings, train_labels, test_embeddings, test_labels):\n",
    "    # Convert embeddings from PyTorch tensors to NumPy arrays\n",
    "    train_embeddings_np = train_embeddings.cpu().numpy()\n",
    "    test_embeddings_np = test_embeddings.cpu().numpy()\n",
    "    \n",
    "    dim = train_embeddings_np.shape[1]\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    gpu_index_flat.add(train_embeddings_np)\n",
    "\n",
    "    # Combine train and test labels to compute class frequency across the entire dataset\n",
    "    combined_labels = torch.cat([train_labels, test_labels], dim=0)\n",
    "    unique_vendors = torch.unique(combined_labels)\n",
    "\n",
    "    # Compute class frequencies from the entire dataset (train + test)\n",
    "    vendor_freq_dict = {int(vendor_id): (combined_labels == vendor_id).sum().item() for vendor_id in unique_vendors}\n",
    "\n",
    "    # Initialize a dict to store scores by frequency\n",
    "    frequency_score_dict = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating Macro-F1@X\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings_np[test_adsidx]\n",
    "\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = np.expand_dims(test_vendor_embeddings, axis=0)\n",
    "\n",
    "        k = vendor_freq_dict[vendor_id]  # Use the frequency from the combined dataset\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = gpu_index_flat.search(test_vendor_embeddings, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "\n",
    "        f1_scores = f1_at_k(true_label_list, predicted_label_list, k)\n",
    "\n",
    "        # Track performance score by frequency of instances\n",
    "        freq = vendor_freq_dict[vendor_id]  # Use the frequency from the combined dataset\n",
    "        if freq not in frequency_score_dict:\n",
    "            frequency_score_dict[freq] = []\n",
    "        frequency_score_dict[freq].extend(f1_scores)\n",
    "\n",
    "    # Calculate average F1 score for each frequency\n",
    "    frequency_avg_f1_dict = {freq: np.mean(scores) for freq, scores in frequency_score_dict.items()}\n",
    "\n",
    "    return frequency_avg_f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45bcbe21-ef96-49ea-9974-e5299c6a37d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 5327.20it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3773.61it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3746.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: midwest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 6908.93it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 5056.07it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 5006.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: west\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 520/520 [00:00<00:00, 9556.94it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 520/520 [00:00<00:00, 6788.25it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 520/520 [00:00<00:00, 6729.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: northeast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 591/591 [00:00<00:00, 10672.07it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 591/591 [00:00<00:00, 8218.40it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 591/591 [00:00<00:00, 8771.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_ce\", city=city_)\n",
    "    \n",
    "    try:\n",
    "        mrr = calculate_mrr_at_k(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "        rprecision = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        macro  = generate_macro_f1_at_x_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city_}: {e}\")\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/mrr/trained_ce_mrr_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(mrr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/rprecision/trained_ce_rprecision_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(rprecision, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/macro-f1/trained_ce_macro_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(macro, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aaa6f7f4-c393-41c5-a20b-57850c042b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 5513.89it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3802.41it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3755.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: midwest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 7218.78it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 5031.86it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 5003.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: west\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 520/520 [00:00<00:00, 9912.41it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 520/520 [00:00<00:00, 6660.17it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 520/520 [00:00<00:00, 6812.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: northeast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 591/591 [00:00<00:00, 10978.64it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 591/591 [00:00<00:00, 8231.53it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 591/591 [00:00<00:00, 8793.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_supcon\", city=city_)\n",
    "    \n",
    "    try:\n",
    "        mrr = calculate_mrr_at_k(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "        rprecision = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        macro  = generate_macro_f1_at_x_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city_}: {e}\")\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/mrr/trained_supcon_mrr_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(mrr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/rprecision/trained_supcon_rprecision_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(rprecision, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/macro-f1/trained_supcon_macro_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(macro, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ca13358-9fe8-44d2-b5e8-92b6d336ebe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 5560.75it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3799.29it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1463/1463 [00:00<00:00, 3740.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: midwest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 7035.34it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 4926.93it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 1033/1033 [00:00<00:00, 4996.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: west\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 520/520 [00:00<00:00, 9394.91it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 520/520 [00:00<00:00, 7003.23it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 520/520 [00:00<00:00, 6772.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: northeast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 591/591 [00:00<00:00, 10885.30it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 591/591 [00:00<00:00, 8173.07it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 591/591 [00:00<00:00, 8767.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_ce+supcon\", city=city_)\n",
    "    \n",
    "    try:\n",
    "        mrr = calculate_mrr_at_k(train_embeddings, train_labels, test_embeddings, test_labels, 10)\n",
    "        rprecision = generate_rprecision_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "        # _, _ = generate_macro_rprecision_results(test_embeddings, test_labels)\n",
    "        macro  = generate_macro_f1_at_x_results(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city_}: {e}\")\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/mrr/trained_ce+supcon_mrr_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(mrr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/rprecision/trained_ce+supcon_rprecision_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(rprecision, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'/workspace/persistent/HTClipper/results/retrieval/text/macro-f1/trained_ce+supcon_macro_{city_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(macro, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del train_embeddings, train_labels, test_embeddings, test_labels\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be3b5b33-666d-418c-89f1-896f9728f79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/workspace/persistent/HTClipper/results/retrieval/text/mrr/zs_mrr_northeast.pickle', 'rb') as handle:\n",
    "    mrr_zs = pickle.load(handle)\n",
    "    \n",
    "with open('/workspace/persistent/HTClipper/results/retrieval/text/rprecision/zs_rprecision_northeast.pickle', 'rb') as handle:\n",
    "    rprecision_zs = pickle.load(handle)\n",
    "    \n",
    "with open('/workspace/persistent/HTClipper/results/retrieval/text/macro-f1/zs_macro_northeast.pickle', 'rb') as handle:\n",
    "    macro_zs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c4ea0-8f50-45e8-a63a-260ceb358793",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ad3164-f138-4bc1-bade-99fb7c23e82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mrr_at_1_results(test_embeddings, test_labels, k):\n",
    "    unique_labels = torch.unique(test_labels)\n",
    "    mrr_score_dict = {}\n",
    "\n",
    "    for label in tqdm(unique_labels, total=len(unique_labels), desc=f\"Calculating MRR@{k}\"):\n",
    "        label_id = int(label)\n",
    "        test_idx = (test_labels == label_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        test_embeddings_np = test_embeddings[test_idx].numpy()\n",
    "\n",
    "        D, I = index.search(test_embeddings_np, k)\n",
    "\n",
    "        predicted_label_list = [train_labels_tensor[I[index]] for index in range(len(test_idx))]\n",
    "        true_label_list = [train_labels_tensor[torch.where(train_labels_tensor == label_id)[0]] for _ in range(len(test_idx))]\n",
    "\n",
    "        mrr_score = mrr_at_k(true_label_list, predicted_label_list, k_=k)\n",
    "        mrr_score_dict[label_id] = round(mrr_score, 4)\n",
    "\n",
    "    return mrr_score_dict\n",
    "\n",
    "\n",
    "def generate_rprecision_results(test_embeddings, test_labels):\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels_tensor.numpy()\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "    r_precision_score_dict = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating R-precision\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = index.search(test_vendor_embeddings_np, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels_tensor == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "\n",
    "        r_precision_score = np.mean(recall_at_k(true_label_list, predicted_label_list, k))\n",
    "        r_precision_score_dict[vendor_id] = round(r_precision_score, 4)\n",
    "\n",
    "    return r_precision_score_dict\n",
    "\n",
    "def generate_macro_f1_at_x_results(test_embeddings, test_labels):\n",
    "    unique_vendors = torch.unique(test_labels)\n",
    "    train_labels_np = train_labels_tensor.numpy()\n",
    "    vendor_dict = {int(vendor_id): np.sum(train_labels_np == int(vendor_id)) for vendor_id in unique_vendors}\n",
    "    f1_score_dict = {}\n",
    "\n",
    "    for vendor_id in tqdm(unique_vendors, total=len(unique_vendors), desc=\"Calculating Macro-F1@X\"):\n",
    "        vendor_id = int(vendor_id)\n",
    "        test_adsidx = (test_labels == vendor_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(test_adsidx) == 0 or vendor_dict[vendor_id] == 0:\n",
    "            continue\n",
    "\n",
    "        test_vendor_embeddings = test_embeddings[test_adsidx]\n",
    "        if test_vendor_embeddings.ndim == 1:\n",
    "            test_vendor_embeddings = test_vendor_embeddings.unsqueeze(0)\n",
    "        test_vendor_embeddings_np = test_vendor_embeddings.numpy()\n",
    "\n",
    "        k = vendor_dict[vendor_id]\n",
    "        if k > train_embeddings_np.shape[0]:\n",
    "            k = train_embeddings_np.shape[0]\n",
    "\n",
    "        _, I = index.search(test_vendor_embeddings_np, int(k))\n",
    "\n",
    "        predicted_label_list = [torch.tensor(I[index]) for index in range(len(test_adsidx))]\n",
    "        true_label_list = [torch.where(train_labels_tensor == vendor_id)[0] for _ in range(len(test_adsidx))]\n",
    "\n",
    "        f1_score = np.mean(f1_at_k(true_label_list, predicted_label_list, k))\n",
    "        f1_score_dict[vendor_id] = round(f1_score, 4)\n",
    "\n",
    "    return f1_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e752ad6c-ec86-4e50-8963-97e3c2df07a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae30517-0f16-4748-b0c7-e28637b8126e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_serializable(data):\n",
    "    \"\"\"\n",
    "    Recursively convert NumPy objects in dictionaries to Python-native types.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in data.items()}\n",
    "    elif isinstance(data, (np.ndarray, np.generic)):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy().tolist()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb630986-a1f4-4d09-9f0a-935da99926a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: south\n",
      "Added batch 1/2 to index.\n",
      "Added batch 2/2 to index.\n",
      "Global variables initialized in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 964/964 [00:09<00:00, 98.66it/s] \n",
      "Calculating R-precision: 100%|ââââââââââ| 964/964 [00:10<00:00, 93.74it/s] \n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 964/964 [00:10<00:00, 94.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: midwest\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 624/624 [00:04<00:00, 153.76it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 624/624 [00:04<00:00, 150.55it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 624/624 [00:04<00:00, 148.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: west\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 259/259 [00:00<00:00, 349.05it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 259/259 [00:00<00:00, 317.84it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 259/259 [00:00<00:00, 339.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "City: northeast\n",
      "Added batch 1/1 to index.\n",
      "Global variables initialized in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MRR@10: 100%|ââââââââââ| 293/293 [00:00<00:00, 468.54it/s]\n",
      "Calculating R-precision: 100%|ââââââââââ| 293/293 [00:00<00:00, 459.96it/s]\n",
      "Calculating Macro-F1@X: 100%|ââââââââââ| 293/293 [00:00<00:00, 459.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# All\n",
    "mean_total, std_total = 0, 0\n",
    "for city_ in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50)\n",
    "    print(\"City:\", city_)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = load_embeddings(model_name=\"declutr_ce+SupCon\", city=city_)\n",
    "    \n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = remove_duplicate_embeddings(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    train_embeddings, train_labels, test_embeddings, test_labels = find_members(train_embeddings, train_labels, test_embeddings, test_labels)\n",
    "    \n",
    "    initialize_globals(train_embeddings, train_labels)\n",
    "    try:\n",
    "        mrr_dict = generate_mrr_at_1_results(test_embeddings, test_labels, 10)\n",
    "        rprecision_dict = generate_rprecision_results(test_embeddings, test_labels)\n",
    "        macro_f1_dict  = generate_macro_f1_at_x_results(test_embeddings, test_labels)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {city}: {e}\")\n",
    "        \n",
    "    # Convert to serializable formats before saving\n",
    "    mrr_dict = convert_to_serializable(mrr_dict)\n",
    "    rprecision_dict = convert_to_serializable(rprecision_dict)\n",
    "    macro_f1_dict = convert_to_serializable(macro_f1_dict)\n",
    "        \n",
    "    with open(f'../error_analysis/declutr_{city_}_mrr.pickle', 'wb') as handle:\n",
    "        pickle.dump(mrr_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'../error_analysis/declutr_{city_}_rprecision.pickle', 'wb') as handle:\n",
    "        pickle.dump(rprecision_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open(f'../error_analysis/declutr_{city_}_macro.pickle', 'wb') as handle:\n",
    "        pickle.dump(macro_f1_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Clear cache and free memory\n",
    "    del mrr_dict, rprecision_dict, macro_f1_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76846808-6a3a-4440-ac96-ef2136f0f4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
