{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33684c22-6cc4-402e-b043-f9be70ed54b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Code Overview\n",
    "# Extracts text, vision, and multmodal embeddings from the ViLT and VisualBERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1460f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from transformers import ViltProcessor, ViltModel\n",
    "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer, ViTModel, ViTFeatureExtractor\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from utilities import map_images_with_text, augment_image_training_data\n",
    "from loadData import ViLTMultimodalDataset, MultimodalDataset\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from viltLayer import ViLTClassifier\n",
    "from visualBERTLayer import VisualBERTClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TorchDynamo errors and fall back to eager execution\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc07910-d532-494f-8ae6-946dfa980120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cac4e81-c4d0-4ef8-bc5e-acd555c24935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "class Args:\n",
    "    \"\"\"Encapsulates arguments into an object for script and notebook compatibility.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Trains a multimodal classifier (VisualBERT or ViLT) for Multimodal Authorship tasks on Backpage advertisements.\")\n",
    "\n",
    "    # Common arguments\n",
    "    parser.add_argument('--logged_entry_name', type=str, default=\"multimodal-latent-fusion-seed:1111\", help=\"Logged entry name visible on weights and biases\")\n",
    "    parser.add_argument('--data_dir', type=str, default='/workspace/persistent/HTClipper/data/processed', help=\"Data directory\")\n",
    "    parser.add_argument('--city', type=str, default='chicago', help=\"Demography of data, can be only between chicago, atlanta, houston, dallas, detroit, ny, sf or all\")\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help=\"Batch Size\")\n",
    "    parser.add_argument('--nb_epochs', type=int, default=40, help=\"Number of Epochs\")\n",
    "    parser.add_argument('--patience', type=int, default=3, help=\"Patience for Early Stopping\")\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='Random seed value')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=0, help=\"Warmup proportion\")\n",
    "    parser.add_argument('--grad_steps', type=int, default=1, help=\"Gradient accumulating step\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=6e-4, help=\"Learning rate\")\n",
    "    parser.add_argument('--train_data_percentage', type=float, default=1.0, help=\"Percentage of training data to be used\")\n",
    "    parser.add_argument('--adam_epsilon', type=float, default=1e-6, help=\"Epsilon value for Adam optimizer\")\n",
    "    parser.add_argument('--min_delta_change', type=float, default=0.01, help=\"Minimum change in delta in validation loss for Early Stopping\")\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01, help=\"Weight decay\")\n",
    "    parser.add_argument('--augment_data', type=bool, default=False, help='Enables data augmentation')\n",
    "    parser.add_argument('--nb_augmented_samples', type=int, default=1, help='Number of augmented samples to be generated')\n",
    "\n",
    "    # Model-specific arguments\n",
    "    parser.add_argument('--model_type', type=str, choices=['visualbert', 'vilt'], default='visualbert', help=\"Choose the model type: visualbert or vilt\")\n",
    "    parser.add_argument('--save_dir', type=str, default=None, help=\"Directory for models to be saved\")\n",
    "    parser.add_argument('--model_dir_name', type=str, default=None, help=\"Save the model with the folder name as mentioned.\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Dynamically set the default save_dir based on model type\n",
    "    if args.save_dir is None:\n",
    "        args.save_dir = os.path.join(\n",
    "            os.getcwd(),\n",
    "            f\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/{args.model_type}/\"\n",
    "        )\n",
    "\n",
    "    return args\n",
    "\n",
    "# Use this in a script\n",
    "# args = parse_arguments()\n",
    "\n",
    "# Use this in a Jupyter Notebook\n",
    "args_dict = {\n",
    "    \"logged_entry_name\": \"multimodal-latent-fusion-seed:1111\",\n",
    "    \"data_dir\": \"/workspace/persistent/HTClipper/data/processed\",\n",
    "    \"city\": \"south\",\n",
    "    \"batch_size\": 32,\n",
    "    \"nb_epochs\": 40,\n",
    "    \"patience\": 3,\n",
    "    \"seed\": 1111,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"grad_steps\": 1,\n",
    "    \"learning_rate\": 6e-4,\n",
    "    \"train_data_percentage\": 1.0,\n",
    "    \"adam_epsilon\": 1e-6,\n",
    "    \"min_delta_change\": 0.01,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"augment_data\": False,\n",
    "    \"nb_augmented_samples\": 1,\n",
    "    \"model_type\": \"visualbert\",  # Change to 'vilt' if needed\n",
    "    \"save_dir\": \"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/visualbert/\",\n",
    "    \"model_dir_name\": None,\n",
    "}\n",
    "\n",
    "args = Args(**args_dict)  # Convert the dictionary to an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4780fb40-76a4-4112-84b4-a78f6c9b5432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set matrix multiplication precision\n",
    "# This setting offers a balance between precision and performance. Itâ€™s typically a good starting point for mixed precision training\n",
    "#  with FP16.\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert args.city in [\"chicago\", \"atlanta\", \"dallas\", \"detroit\", \"houston\", \"sf\", \"ny\", \"all\", \"midwest\", \"northeast\", \"south\", \"west\"]\n",
    "\n",
    "# Creating directories\n",
    "if args.model_dir_name == None:\n",
    "    directory = os.path.join(args.save_dir, args.city, \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate))\n",
    "else:\n",
    "    directory = os.path.join(args.save_dir, args.city, \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate), args.model_dir_name)\n",
    "Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %% Load your DataFrame\n",
    "data_dir = os.path.join(args.data_dir, args.city + \".csv\")\n",
    "args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", args.city, \"image\", \"image\")\n",
    "df = pd.read_csv(data_dir)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "# Identify and keep vendors with at least 2 instances\n",
    "class_counts = df['VENDOR'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "# Re-encode labels after filtering\n",
    "df_filtered['VENDOR'] = label_encoder.fit_transform(df_filtered['VENDOR'])\n",
    "\n",
    "df_filtered = df_filtered[[\"TEXT\", \"IMAGES\", \"VENDOR\"]].drop_duplicates()\n",
    "\n",
    "# Split the data into train, validation, and test sets without mapping images to text yet\n",
    "train_df, test_df = train_test_split(\n",
    "    df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered['VENDOR'], shuffle=True)\n",
    "\n",
    "# Adjust the validation split size based on the number of unique vendors\n",
    "min_val_size = len(df_filtered['VENDOR'].unique()) / len(train_df)\n",
    "val_size = max(0.05, min_val_size)  # Choose a larger value if needed, e.g., 0.05 or 5%\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=val_size, random_state=args.seed, stratify=train_df['VENDOR']\n",
    ")\n",
    "\n",
    "# Apply map_images_with_text separately to avoid overlap of text-image pairs across splits\n",
    "train_df = map_images_with_text(train_df).drop_duplicates()\n",
    "val_df = map_images_with_text(val_df).drop_duplicates()\n",
    "test_df = map_images_with_text(test_df).drop_duplicates()\n",
    "\n",
    "# Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9c8bb25-afde-4d80-a408-76a09682f34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python version: 3.10\n",
    "Description: Contains the architectural implementation of visualBERT based multimodal classifier trained with concatenation based\n",
    "            fusion techniques.\n",
    "Reference: https://arxiv.org/pdf/1908.03557 \n",
    "\"\"\"\n",
    "\n",
    "# %% Importing libraries\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from transformers import ViTModel, get_linear_schedule_with_warmup\n",
    "\n",
    "class VisualBERTClassifier(pl.LightningModule):\n",
    "    def __init__(self, visualbert_model, vit_model, learning_rate, num_classes, weight_decay, eps, warmup_steps, num_training_steps, max_seq_length=512, \n",
    "                max_visual_tokens=197):\n",
    "        super(VisualBERTClassifier, self).__init__()\n",
    "        self.visualbert_model = visualbert_model\n",
    "        self.vit_model = vit_model\n",
    "        self.classifier = nn.Linear(self.visualbert_model.config.hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # Max sequence length for transformers model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # Since we using the ViT-patch 16, max_visual_tokens = (Image Height/Patch Size) x (Image Height/Patch Size)\n",
    "        # = (224/16) x (224/16) = 196\n",
    "        self.max_visual_tokens = max_visual_tokens  # This is standard for ViT with 224x224 images\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Extract visual embeddings using ViT\n",
    "        vit_outputs = self.vit_model(pixel_values)\n",
    "        visual_embeds = vit_outputs.last_hidden_state\n",
    "\n",
    "        # Ensure the visual_embeds shape matches expected shape by VisualBERT\n",
    "        batch_size, num_visual_tokens, hidden_dim = visual_embeds.shape\n",
    "\n",
    "        # Trim or pad visual embeddings\n",
    "        if num_visual_tokens > self.max_visual_tokens:  # Trim if the size exceeds expected\n",
    "            visual_embeds = visual_embeds[:, :self.max_visual_tokens, :]\n",
    "        elif num_visual_tokens < self.max_visual_tokens:  # Pad if the size is less than expected\n",
    "            padding = torch.zeros((batch_size, self.max_visual_tokens - num_visual_tokens, hidden_dim), device=visual_embeds.device)\n",
    "            visual_embeds = torch.cat((visual_embeds, padding), dim=1)\n",
    "\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(input_ids.device)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(input_ids.device)\n",
    "\n",
    "        # Adjust input_ids and attention_mask to ensure the total length is within the limit\n",
    "        total_length = input_ids.size(1) + self.max_visual_tokens\n",
    "        if total_length > self.max_seq_length:\n",
    "            excess_length = total_length - self.max_seq_length\n",
    "            input_ids = input_ids[:, :-excess_length]\n",
    "            attention_mask = attention_mask[:, :-excess_length]\n",
    "\n",
    "        # Concatenate text and visual embeddings\n",
    "        text_embeds = self.visualbert_model.embeddings.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.visualbert_model.embeddings.token_type_embeddings(\n",
    "            torch.cat((torch.zeros_like(input_ids), visual_token_type_ids), dim=1))\n",
    "        position_ids = torch.arange(text_embeds.size(1) + visual_embeds.size(1), dtype=torch.long, device=input_ids.device)\n",
    "        position_embeddings = self.visualbert_model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = torch.cat((text_embeds, visual_embeds), dim=1)\n",
    "        embeddings += token_type_embeddings + position_embeddings\n",
    "        embeddings = self.visualbert_model.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.visualbert_model.embeddings.dropout(embeddings)\n",
    "\n",
    "        # Concatenate attention masks\n",
    "        combined_attention_mask = torch.cat((attention_mask, visual_attention_mask), dim=1)\n",
    "        combined_attention_mask = combined_attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        encoder_outputs = self.visualbert_model.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            head_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask, pixel_values)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask, pixel_values)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        self.log('val_f1_weighted', f1_weighted)\n",
    "        self.log('val_f1_micro', f1_micro)\n",
    "        self.log('val_f1_macro', f1_macro)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask, pixel_values)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "\n",
    "        self.test_outputs.append({\"acc\": acc, \"f1_weighted\": f1_weighted, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"labels\": labels.cpu(), \"preds\": preds.cpu()})\n",
    "\n",
    "        return {\"acc\": acc, \"f1_weighted\": f1_weighted, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_acc = torch.tensor([x['acc'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_weighted = torch.tensor([x['f1_weighted'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_micro = torch.tensor([x['f1_micro'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_macro = torch.tensor([x['f1_macro'] for x in self.test_outputs]).mean()\n",
    "\n",
    "        labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
    "        preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
    "\n",
    "        self.log('test_acc', avg_acc)\n",
    "        self.log('test_f1_weighted', avg_f1_weighted)\n",
    "        self.log('test_f1_micro', avg_f1_micro)\n",
    "        self.log('test_f1_macro', avg_f1_macro)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':self.weight_decay}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': self.weight_decay}]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "\n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def extract_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Re-implement the forward method up to the point before classification to get the pooled output\n",
    "            # Extract visual embeddings using ViT\n",
    "            vit_outputs = self.vit_model(pixel_values)\n",
    "            visual_embeds = vit_outputs.last_hidden_state\n",
    "\n",
    "            # Ensure the visual_embeds shape matches expected shape by VisualBERT\n",
    "            batch_size, num_visual_tokens, hidden_dim = visual_embeds.shape\n",
    "\n",
    "            # Trim or pad visual embeddings\n",
    "            if num_visual_tokens > self.max_visual_tokens:\n",
    "                visual_embeds = visual_embeds[:, :self.max_visual_tokens, :]\n",
    "            elif num_visual_tokens < self.max_visual_tokens:\n",
    "                padding = torch.zeros((batch_size, self.max_visual_tokens - num_visual_tokens, hidden_dim), device=visual_embeds.device)\n",
    "                visual_embeds = torch.cat((visual_embeds, padding), dim=1)\n",
    "\n",
    "            visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(input_ids.device)\n",
    "            visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(input_ids.device)\n",
    "\n",
    "            # Adjust input_ids and attention_mask\n",
    "            total_length = input_ids.size(1) + self.max_visual_tokens\n",
    "            if total_length > self.max_seq_length:\n",
    "                excess_length = total_length - self.max_seq_length\n",
    "                input_ids = input_ids[:, :-excess_length]\n",
    "                attention_mask = attention_mask[:, :-excess_length]\n",
    "\n",
    "            # Concatenate text and visual embeddings\n",
    "            text_embeds = self.visualbert_model.embeddings.word_embeddings(input_ids)\n",
    "            token_type_embeddings = self.visualbert_model.embeddings.token_type_embeddings(\n",
    "                torch.cat((torch.zeros_like(input_ids), visual_token_type_ids), dim=1))\n",
    "            position_ids = torch.arange(text_embeds.size(1) + visual_embeds.size(1), dtype=torch.long, device=input_ids.device)\n",
    "            position_embeddings = self.visualbert_model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "            embeddings = torch.cat((text_embeds, visual_embeds), dim=1)\n",
    "            embeddings += token_type_embeddings + position_embeddings\n",
    "            embeddings = self.visualbert_model.embeddings.LayerNorm(embeddings)\n",
    "            embeddings = self.visualbert_model.embeddings.dropout(embeddings)\n",
    "\n",
    "            # Concatenate attention masks\n",
    "            combined_attention_mask = torch.cat((attention_mask, visual_attention_mask), dim=1)\n",
    "            extended_attention_mask = combined_attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "            encoder_outputs = self.visualbert_model.encoder(\n",
    "                embeddings,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            # Extract the pooled output (e.g., [CLS] token)\n",
    "            pooled_output = encoder_outputs.last_hidden_state[:, 0]  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "            return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f227b02a-be8f-4407-b5f8-55595bce82c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python version: 3.10\n",
    "Description: Contains the architectural implementation of ViLT based multimodal classifier trained with concatenation based\n",
    "            fusion techniques.\n",
    "Reference: https://arxiv.org/abs/2102.03334\n",
    "\"\"\"\n",
    "\n",
    "# %% Importing libraries\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from transformers import ViTModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# %% Model Definition for ViLT\n",
    "class ViLTClassifier(pl.LightningModule):\n",
    "    def __init__(self, vilt_model, learning_rate, num_classes, weight_decay, eps, warmup_steps, num_training_steps):\n",
    "        super(ViLTClassifier, self).__init__()\n",
    "        self.vilt_model = vilt_model\n",
    "        self.classifier = nn.Linear(self.vilt_model.config.hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.test_outputs = [] \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        outputs = self.vilt_model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output  # ViLT provides a pooled output directly\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'], batch['pixel_values'])\n",
    "        loss = self.criterion(logits, batch['label'])\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask, pixel_values)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_f1_weighted', f1_weighted, prog_bar=True)\n",
    "        self.log('val_f1_micro', f1_micro, prog_bar=True)\n",
    "        self.log('val_f1_macro', f1_macro, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        logits = self(input_ids, attention_mask, pixel_values)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "\n",
    "        self.test_outputs.append({\"acc\": acc, \"f1_weighted\": f1_weighted, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"labels\": labels.cpu(), \"preds\": preds.cpu()})\n",
    "\n",
    "        return {\"acc\": acc, \"f1_weighted\": f1_weighted, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_acc = torch.tensor([x['acc'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_weighted = torch.tensor([x['f1_weighted'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_micro = torch.tensor([x['f1_micro'] for x in self.test_outputs]).mean()\n",
    "        avg_f1_macro = torch.tensor([x['f1_macro'] for x in self.test_outputs]).mean()\n",
    "\n",
    "        labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
    "        preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
    "\n",
    "        self.log('test_acc', avg_acc)\n",
    "        self.log('test_f1_weighted', avg_f1_weighted)\n",
    "        self.log('test_f1_micro', avg_f1_micro)\n",
    "        self.log('test_f1_macro', avg_f1_macro)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':self.weight_decay}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': self.weight_decay}]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "\n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def extract_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the standard ViLT forward method with both text and image inputs\n",
    "            outputs = self.vilt_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "\n",
    "            # Extract the pooled output, which is the multimodal embedding before classification\n",
    "            pooled_output = outputs.pooler_output  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "            return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3982b8ef-3304-418a-b71a-5f8fa9838644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_tokenizer, image_processor, label_encoder, image_dir, augment=False, image_size=(224, 224)):\n",
    "        self.dataframe = dataframe\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.augment = augment\n",
    "        self.augmentation_pipelines = get_augmentation_pipeline() if augment else None\n",
    "        self.image_size = image_size\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        # Remove rows with missing image files\n",
    "        self.dataframe = self.dataframe[self.dataframe['IMAGES'].apply(lambda x: os.path.exists(os.path.join(self.image_dir, x)))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['TEXT']\n",
    "        image_path = row['IMAGES']\n",
    "        label = row['VENDOR']\n",
    "        \n",
    "        text_inputs = self.text_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(os.path.join(self.image_dir, image_path)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a default image or handle the error as needed\n",
    "            image = Image.new('RGB', self.image_size, (255, 255, 255))\n",
    "\n",
    "        # Resize the image to a consistent size\n",
    "        image = image.resize(self.image_size)\n",
    "        \n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # Ensure the image array has the correct dimensions (H, W, C)\n",
    "        if image_array.shape[-1] != 3:\n",
    "            image_array = np.stack((image_array,) * 3, axis=-1)\n",
    "        \n",
    "        if self.augment and 'AUGMENT' in row and row['AUGMENT'] >= 0:\n",
    "            augment_idx = row['AUGMENT']\n",
    "            augmented = self.augmentation_pipelines[augment_idx](image=image_array)\n",
    "            image_array = augmented['image']\n",
    "        \n",
    "        # Ensure image dimensions are (C, H, W)\n",
    "        if image_array.shape[-1] == 3:\n",
    "            image_array = np.transpose(image_array, (2, 0, 1))\n",
    "        \n",
    "        image_tensor = torch.tensor(image_array, dtype=torch.float)\n",
    "        image_tensor = self.image_processor(images=image_tensor, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "        \n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': image_tensor,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70dd64ce-c033-437c-9a68-24964678b08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_trained_model(model_name):\n",
    "\n",
    "    if model_name == \"visualBERT\":\n",
    "        text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        image_processor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224', size=224, do_resize=True)\n",
    "        vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "        visualbert_config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vcr')\n",
    "        visualbert_model = VisualBertModel.from_pretrained('uclanlp/visualbert-vcr', config=visualbert_config)\n",
    "\n",
    "        # Create the datasets and dataloaders\n",
    "        train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "        val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "        test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        num_training_steps = args.nb_epochs * len(train_dataloader)\n",
    "        # Setting the warmup steps to 1/10th the size of training data\n",
    "        warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "        # %% Loading the model\n",
    "        model = VisualBERTClassifier(visualbert_model=visualbert_model, vit_model=vit_model, learning_rate=args.learning_rate, \n",
    "                                    num_classes=len(label_encoder.classes_), weight_decay=args.weight_decay, eps=args.adam_epsilon, \n",
    "                                    warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/visualBERT/south/seed:1111/lr-0.0001/final_model.ckpt\")\n",
    "\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    else:\n",
    "\n",
    "        # %% Load the processor and model for ViLT\n",
    "        vilt_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        vilt_model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "        # Define train, validation, and test datasets and loaders\n",
    "        train_dataset = ViLTMultimodalDataset(train_df, vilt_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "        val_dataset = ViLTMultimodalDataset(val_df, vilt_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "        test_dataset = ViLTMultimodalDataset(test_df, vilt_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        num_training_steps = args.nb_epochs * len(train_dataloader)\n",
    "        # Setting the warmup steps to 1/10th the size of training data\n",
    "        warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "        model = ViLTClassifier(vilt_model=vilt_model, learning_rate=args.learning_rate, num_classes=len(label_encoder.classes_), weight_decay=args.weight_decay,\n",
    "                            eps=args.adam_epsilon, warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/vilt/south/seed:1111/lr-0.0001/final_model.ckpt\")\n",
    "\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "255d5cfd-41bd-4c08-af11-f48a317cf52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset_for_ClassifierModel(region_name, data_dir, model, model_name, filter_by=\"vendor\", batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    assert filter_by in [\"vendor\", \"id\"]\n",
    "    \n",
    "    image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", region_name, \"image\", \"image\")\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "\n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "    # Identify and keep vendors with at least 2 instances\n",
    "    class_counts = df['VENDOR'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "    # Re-encode labels after filtering\n",
    "    df_filtered['VENDOR'] = label_encoder.fit_transform(df_filtered['VENDOR'])\n",
    "\n",
    "    df_filtered = df_filtered[[\"TEXT\", \"IMAGES\", \"VENDOR\", \"region\"]].drop_duplicates()\n",
    "\n",
    "    # Dynamically adjust test_size based on the number of classes\n",
    "    min_test_size = len(df_filtered['VENDOR'].unique()) / len(df_filtered)\n",
    "    test_size = max(0.2, min_test_size)  # Ensure the test size is at least 20% or large enough to include all classes\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_filtered, test_size=test_size, random_state=1111, stratify=df_filtered['VENDOR'], shuffle=True\n",
    "    )\n",
    "\n",
    "    # Apply map_images_with_text_fn to avoid overlap of text-image pairs across splits\n",
    "    train_df = map_images_with_text(train_df).drop_duplicates()\n",
    "    test_df = map_images_with_text(test_df).drop_duplicates()\n",
    "    \n",
    "    if model_name == \"visualBERT\":\n",
    "        text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        image_processor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224', size=224, do_resize=True)\n",
    "        vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "        # Create the datasets and dataloaders\n",
    "        train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=image_dir, augment=False)\n",
    "        test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=image_dir, augment=False)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Load the processor and model for ViLT\n",
    "        vilt_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        vilt_model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "        # Define train and test datasets and loaders\n",
    "        train_dataset = ViLTMultimodalDataset(train_df, vilt_processor, label_encoder, image_dir=image_dir, augment=False)\n",
    "        test_dataset = ViLTMultimodalDataset(test_df, vilt_processor, label_encoder, image_dir=image_dir, augment=False)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Helper function to fetch embeddings\n",
    "    def get_embeddings(dataloader):\n",
    "        multimodal_embeddings = []\n",
    "        labels = []\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Fetching Embeddings', leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                batch_labels = batch['label']\n",
    "\n",
    "                # Extract multimodal embeddings\n",
    "                embeddings = model.extract_embeddings(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    pixel_values=pixel_values\n",
    "                )\n",
    "                multimodal_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "                # Append labels\n",
    "                labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        # Concatenate embeddings across all batches\n",
    "        multimodal_embeddings = np.concatenate(multimodal_embeddings)\n",
    "        labels = np.array(labels)\n",
    "        return multimodal_embeddings, labels\n",
    "\n",
    "    # Get embeddings for train and test sets\n",
    "    train_multimodal_embeddings, train_labels = get_embeddings(train_dataloader)\n",
    "    test_multimodal_embeddings, test_labels = get_embeddings(test_dataloader)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(\n",
    "        \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/multimodal_baselines/E2E\", model_name)\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    np.save(os.path.join(output_dir, f'train_multimodal_embeddings_{region_name}_{filter_by}.npy'), train_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_labels_{region_name}_{filter_by}.npy'), train_labels)\n",
    "\n",
    "    np.save(os.path.join(output_dir, f'test_multimodal_embeddings_{region_name}_{filter_by}.npy'), test_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_labels_{region_name}_{filter_by}.npy'), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaf5f263-a44e-4355-8bce-2938b7d41d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:visualBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------south-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------midwest-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------west-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------northeast-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:ViLT\n",
      "-----------------------------------------------south-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------midwest-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------west-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------northeast-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "for model_name in [\"visualBERT\", \"ViLT\"]:\n",
    "    \n",
    "    model = None\n",
    "    print(f\"model_name:{model_name}\")\n",
    "    model = load_trained_model(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    for region in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "        print(f\"-----------------------------------------------{region}-------------------------------------------------------\")\n",
    "        process_dataset_for_ClassifierModel(region_name=region, data_dir=args.data_dir, model=model.to(device), model_name=model_name, filter_by=\"vendor\", batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
