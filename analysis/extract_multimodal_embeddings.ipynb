{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d1802-38be-44eb-a209-4d8ed199d5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Code Overview\n",
    "# Extracts text, vision, and multmodal embeddings from the multimodal DeCLUTR-ViT backbone trained with CE, triplet, CE+triplet, and CE+SupCon objectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d256ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python version: 3.10\n",
    "Description: Trains a Declutr-small and ViT-patch16 based classifier to establish baselines for Multimodal Authorship tasks on Backpage advertisements.\n",
    "\"\"\"\n",
    "\n",
    "# %% Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, ViTModel, ViTImageProcessor\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from utilities import map_images_with_text, augment_image_training_data\n",
    "from loadData import MultimodalDataset\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from multimodalLayer import multimodalFusionModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TorchDynamo errors and fall back to eager execution\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5d5fb0-5317-4847-b68c-3680c5d9bdc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b126a0-fe3c-4a70-a959-a18b464de1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # %% Setting up the Argparser\n",
    "    parser = argparse.ArgumentParser(description=\"Trains a Declutr-small and ViT-patch16 based classifier to establish baselines for Multimodal Authorship tasks on Backpage advertisements.\")\n",
    "    parser.add_argument('--logged_entry_name', type=str, default=\"multimodal-latent-fusion-seed:1111\", help=\"Logged entry name visible on weights and biases\")\n",
    "    parser.add_argument('--data_dir', type=str, default='/workspace/persistent/HTClipper/data/processed', help=\"\"\"Data directory\"\"\")\n",
    "    parser.add_argument('--city', type=str, default='south', help=\"\"\"Demography of data, can be only between chicago, atlanta, houston, dallas, detroit, ny, sf or all\"\"\")\n",
    "    parser.add_argument('--fusion_technique', type=str, default='mean', help=\"\"\"Kind of fusion technique to use. Can be amongst mean, concat, add, multiply, attention, or learned_fusion\"\"\")\n",
    "    parser.add_argument('--save_dir', type=str, default=os.path.join(os.getcwd(), \"/workspace/persistent/HTClipper/models/multimodal-baselines/latent_fusion/\"), help=\"\"\"Directory for models to be saved\"\"\")\n",
    "    parser.add_argument('--model_dir_name', type=str, default=None, help=\"Save the model with the folder name as mentioned.\")\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help=\"Batch Size\")\n",
    "    parser.add_argument('--nb_epochs', type=int, default=40, help=\"Number of Epochs\")\n",
    "    parser.add_argument('--patience', type=int, default=3, help=\"Patience for Early Stopping\")\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='Random seed value')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=0, help=\"Warmup proportion\")\n",
    "    parser.add_argument('--grad_steps', type=int, default=4, help=\"Gradient accumulating step\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=6e-4, help=\"learning rate\")\n",
    "    parser.add_argument('--train_data_percentage', type=float, default=1.0, help=\"Percentage of training data to be used\")\n",
    "    parser.add_argument('--adam_epsilon', type=float, default=1e-6, help=\"Epsilon value for adam optimizer\")\n",
    "    parser.add_argument('--min_delta_change', type=float, default=0.01, help=\"Minimum change in delta in validation loss for Early Stopping\")\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01, help=\"Weight decay\")\n",
    "    parser.add_argument('--augment_data', type=bool, default=False, help='Enables data augmentation')\n",
    "    parser.add_argument('--nb_augmented_samples', type=int, default=1, help='Number of augmented samples to be generated')\n",
    "    parser.add_argument('--loss', type=str, default='CE+SupCon', help='Loss function to use. Can be CE, CE+SupCon, or CE+SupCon+ITM')\n",
    "    parser.add_argument('--temp', type=float, default=0.5, help=\"Tempertaure variable for the Constrastive loss function\")\n",
    "    \n",
    "    # Check if running in Jupyter\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        args = parser.parse_args([])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4951361-4f9d-482c-9c18-51fe23a8640c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1111\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Setting seed value for reproducibility    \n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "# Set TOKENIZERS_PARALLELISM to false to disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Set matrix multiplication precision\n",
    "# This setting offers a balance between precision and performance. Itâ€™s typically a good starting point for mixed precision training\n",
    "#  with FP16.\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# assert args.city in [\"chicago\", \"atlanta\", \"dallas\", \"detroit\", \"houston\", \"sf\", \"ny\", \"all\"]\n",
    "assert args.fusion_technique in [\"mean\", \"add\", \"concat\", \"multiply\", \"attention\", \"learned_fusion\"]\n",
    "assert args.loss in [\"CE\", \"CE+SupCon\", \"CE+SupCon+ITM\"]\n",
    "\n",
    "# Creating directories\n",
    "if args.model_dir_name == None:\n",
    "    directory = os.path.join(args.save_dir, args.city, \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate), args.loss, str(args.temp), args.fusion_technique)\n",
    "else:\n",
    "    directory = os.path.join(args.save_dir, args.city, \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate), args.model_dir_name, args.loss, args.temp, args.fusion_technique)\n",
    "Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %% Load your DataFrame\n",
    "data_dir = os.path.join(args.data_dir, args.city + \".csv\")\n",
    "args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", args.city, \"image\", \"image\")\n",
    "df = pd.read_csv(data_dir)\n",
    "\n",
    "# mapping every image to it's corresponding text\n",
    "df = map_images_with_text(df)\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "# Identify and remove classes with fewer than 2 instances\n",
    "# Since we use stratify during splitting, we should atleast have one training example in training and one in test dataset\n",
    "class_counts = df['VENDOR'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered['VENDOR'])\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.05, random_state=args.seed, stratify=train_df['VENDOR'])\n",
    "\n",
    "# Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "\n",
    "# Augment the training data by adding multiple entries for each image\n",
    "# train_df = augment_image_training_data(train_df)\n",
    "\n",
    "# %% Intializing the tokenizers and models\n",
    "# Since these are the two models that performed individually on the text and image modalities, we establish them as benchmarks and\n",
    "# only run use them in our further experiments.\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "text_model = AutoModel.from_pretrained('johngiorgi/declutr-small')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "# val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "num_training_steps = args.nb_epochs * len(train_dataloader)\n",
    "# Setting the warmup steps to 1/10th the size of training data\n",
    "warmup_steps = int(0.1 * num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9da28ff9-700c-4f1d-97c6-5f2b3235f359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = multimodalFusionModel(\n",
    "    text_model=text_model,\n",
    "    image_model=image_model,\n",
    "    fusion_technique=\"mean\",\n",
    "    num_classes=len(label_encoder.classes_),\n",
    "    learning_rate=args.learning_rate,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    num_training_steps=num_training_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    temperature=args.temp,\n",
    "    loss_function=\"CE\",\n",
    "    ce_weight = 1.0,\n",
    "    supcon_weight = 1.0,\n",
    "    itm_weight = 1.0,\n",
    "    ntxent_weight = 1.0,\n",
    "    num_hard_negatives = 5\n",
    ")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/south/seed:1111/lr-0.0001/CE/0.5/mean/final_model.ckpt\")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b775f4-c0a8-41a4-b439-2d91bc29cfd9",
   "metadata": {},
   "source": [
    "# Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c143fe-78e5-4bdf-ab85-f19a10928478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %% Intializing the tokenizers and models\n",
    "# Since these are the two models that performed individually on the text and image modalities, we establish them as benchmarks and\n",
    "# only run use them in our further experiments.\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "text_model = AutoModel.from_pretrained('johngiorgi/declutr-small')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423cf237-a4e2-4929-b47d-7f99f6bb2ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(model, city, dataloader):\n",
    "    # Initialize lists to store embeddings\n",
    "    all_multimodal_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_image_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Iterate through all batches in the dataloader with a progress bar\n",
    "    for batch in tqdm(dataloader, desc=f\"Extracting embeddings for {city}\"):\n",
    "        input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "\n",
    "        # Extract multimodal embeddings\n",
    "        multimodal_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=pixel_values.to(device))\n",
    "        all_multimodal_embeddings.append(multimodal_embeddings.cpu().detach())\n",
    "\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        all_text_embeddings.append(text_embeddings.cpu().detach())\n",
    "\n",
    "        # Extract image embeddings\n",
    "        image_embeddings = model.extract_embeddings(pixel_values=pixel_values.to(device))\n",
    "        all_image_embeddings.append(image_embeddings.cpu().detach())\n",
    "        \n",
    "        # Extract labels\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "    # Concatenate all embeddings into 2D tensors\n",
    "    all_multimodal_embeddings = torch.cat(all_multimodal_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_multimodal_embeddings, all_text_embeddings, all_image_embeddings, all_labels\n",
    "\n",
    "def generate_embeddings_for_city(model, city, folder_name):    \n",
    "    # %% Load your DataFrame\n",
    "    pickled_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/multimodal_baselines/\"\n",
    "    data_dir = os.path.join(args.data_dir, city + \".csv\")\n",
    "    args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", city, \"image\", \"image\")\n",
    "    df = pd.read_csv(data_dir)\n",
    "\n",
    "    # Encode the labels\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "    # Identify and keep vendors with at least 2 instances\n",
    "    class_counts = df['VENDOR'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "    # Re-encode labels after filtering\n",
    "    # df_filtered['VENDOR'] = label_encoder.fit_transform(df_filtered['VENDOR'])\n",
    "\n",
    "    df_filtered = df_filtered[[\"TEXT\", \"IMAGES\", \"VENDOR\"]].drop_duplicates()\n",
    "\n",
    "    # Dynamically adjust test_size based on the number of classes\n",
    "    min_test_size = len(df_filtered['VENDOR'].unique()) / len(df_filtered)\n",
    "    test_size = max(0.2, min_test_size)  # Ensure the test size is at least 20% or large enough to include all classes\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_filtered, test_size=test_size, random_state=args.seed, stratify=df_filtered['VENDOR'], shuffle=True\n",
    "    )\n",
    "\n",
    "    # Apply map_images_with_text separately to avoid overlap of text-image pairs across splits\n",
    "    train_df = map_images_with_text(train_df).drop_duplicates()\n",
    "    test_df = map_images_with_text(test_df).drop_duplicates()\n",
    "\n",
    "    # Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "    train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "\n",
    "    # Augment the training data by adding multiple entries for each image\n",
    "    # train_df = augment_image_training_data(train_df)\n",
    "\n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "    # val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "    test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    directory = os.path.join(pickled_dir, folder_name)\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    multimodal_embeddings, text_embeddings, image_embeddings, labels = generate_embeddings(model, city, train_dataloader)\n",
    "    label_filename = city + \"_labels_train.pt\"\n",
    "    multimodal_data_filename = city + \"_multimodaldata_train.pt\"\n",
    "    text_data_filename = city + \"_textdata_train.pt\"\n",
    "    image_data_filename = city + \"_imagedata_train.pt\"\n",
    "    \n",
    "    \n",
    "    torch.save(multimodal_embeddings, os.path.join(directory, multimodal_data_filename))\n",
    "    torch.save(text_embeddings, os.path.join(directory, text_data_filename))\n",
    "    torch.save(image_embeddings, os.path.join(directory, image_data_filename))\n",
    "    torch.save(labels, os.path.join(directory, label_filename))\n",
    "    \n",
    "    multimodal_embeddings, text_embeddings, image_embeddings, labels = generate_embeddings(model, city, test_dataloader)\n",
    "    label_filename = city + \"_labels_test.pt\"\n",
    "    multimodal_data_filename = city + \"_multimodaldata_test.pt\"\n",
    "    text_data_filename = city + \"_textdata_test.pt\"\n",
    "    image_data_filename = city + \"_imagedata_test.pt\"\n",
    "    torch.save(multimodal_embeddings, os.path.join(directory, multimodal_data_filename))\n",
    "    torch.save(text_embeddings, os.path.join(directory, text_data_filename))\n",
    "    torch.save(image_embeddings, os.path.join(directory, image_data_filename))\n",
    "    torch.save(labels, os.path.join(directory, label_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "940af084-234f-478c-9b76-efda892827e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_embeddings_for_city(city, folder_name):    \\n    # Load your DataFrame\\n    pickled_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit\"\\n    data_dir = os.path.join(args.data_dir, city + \".csv\")\\n    args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", city, \"image\", \"image\")\\n    df = pd.read_csv(data_dir)\\n\\n    # Mapping every image to its corresponding text\\n    df = map_images_with_text(df)\\n    # Encode the labels\\n    label_encoder = LabelEncoder()\\n    df[\\'VENDOR\\'] = label_encoder.fit_transform(df[\\'VENDOR\\'])\\n\\n    # Identify and remove classes with fewer than 2 instances\\n    class_counts = df[\\'VENDOR\\'].value_counts()\\n    valid_classes = class_counts[class_counts >= 3].index\\n    df_filtered = df[df[\\'VENDOR\\'].isin(valid_classes)]\\n\\n    # Split the data into train and test sets (80-20 split)\\n    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered[\\'VENDOR\\'])\\n\\n    # Replacing all the numbers in the training dataset with the letter \"N\"\\n    train_df[\\'TEXT\\'] = train_df[\\'TEXT\\'].apply(lambda x: re.sub(r\\'\\\\d\\', \\'N\\', str(x)))\\n\\n    # Augment the training data by adding multiple entries for each image\\n    train_df = augment_image_training_data(train_df)\\n\\n    # Create the datasets and dataloaders\\n    train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\\n    test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\\n\\n    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\\n    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\\n    \\n    directory = os.path.join(pickled_dir, folder_name)\\n    Path(directory).mkdir(parents=True, exist_ok=True)\\n    \\n    # Generate embeddings for the training data\\n    multimodal_embeddings_train, text_embeddings_train, image_embeddings_train, text_labels_train, image_labels_train, all_labels_train = generate_embeddings(city, train_dataloader)\\n    torch.save(multimodal_embeddings_train, os.path.join(directory, f\"{city}_multimodaldata_train.pt\"))\\n    torch.save(text_embeddings_train, os.path.join(directory, f\"{city}_textdata_train.pt\"))\\n    torch.save(image_embeddings_train, os.path.join(directory, f\"{city}_imagedata_train.pt\"))\\n    torch.save(text_labels_train, os.path.join(directory, f\"{city}_labels_text_train.pt\"))  # Save text labels separately\\n    torch.save(image_labels_train, os.path.join(directory, f\"{city}_labels_image_train.pt\"))  # Save image labels separately\\n    torch.save(all_labels_train, os.path.join(directory, f\"{city}_labels_multimodal_train.pt\"))  # Save image labels separately\\n    \\n    # Generate embeddings for the testing data\\n    multimodal_embeddings_test, text_embeddings_test, image_embeddings_test, text_labels_test, image_labels_test, all_labels_test = generate_embeddings(city, test_dataloader)\\n    torch.save(multimodal_embeddings_test, os.path.join(directory, f\"{city}_multimodaldata_test.pt\"))\\n    torch.save(text_embeddings_test, os.path.join(directory, f\"{city}_textdata_test.pt\"))\\n    torch.save(image_embeddings_test, os.path.join(directory, f\"{city}_imagedata_test.pt\"))\\n    torch.save(text_labels_test, os.path.join(directory, f\"{city}_labels_text_test.pt\"))  # Save text labels separately\\n    torch.save(image_labels_test, os.path.join(directory, f\"{city}_labels_image_test.pt\"))  # Save image labels separately\\n    torch.save(all_labels_test, os.path.join(directory, f\"{city}_labels_multimodal_test.pt\"))  # Save image labels separately\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def generate_embeddings(city, dataloader):\n",
    "    # Initialize lists to store embeddings and labels\n",
    "    all_multimodal_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_image_embeddings = []\n",
    "    all_text_labels = []  # Separate list for text labels\n",
    "    all_image_labels = []  # Separate list for image labels\n",
    "    all_labels = []\n",
    "    \n",
    "    seen_text_embeddings = set()  # To track unique text embeddings\n",
    "    seen_image_embeddings = set()  # To track unique image embeddings\n",
    "\n",
    "    # Iterate through all batches in the dataloader with a progress bar\n",
    "    for batch in tqdm(dataloader, desc=f\"Extracting embeddings for {city}\"):\n",
    "        input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "\n",
    "        # Extract multimodal embeddings\n",
    "        multimodal_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=pixel_values.to(device))\n",
    "        all_multimodal_embeddings.append(multimodal_embeddings.cpu().detach())\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        text_embedding_tuple = tuple(text_embeddings.cpu().detach().numpy().flatten())\n",
    "\n",
    "        if text_embedding_tuple not in seen_text_embeddings:\n",
    "            seen_text_embeddings.add(text_embedding_tuple)  # Track the unique embedding\n",
    "            all_text_embeddings.append(text_embeddings.cpu().detach())\n",
    "            all_text_labels.append(labels.cpu().detach())  # Append label corresponding to the text\n",
    "\n",
    "        # Extract image embeddings\n",
    "        image_embeddings = model.extract_embeddings(pixel_values=pixel_values.to(device))\n",
    "        image_embedding_tuple = tuple(image_embeddings.cpu().detach().numpy().flatten())\n",
    "\n",
    "        if image_embedding_tuple not in seen_image_embeddings:\n",
    "            seen_image_embeddings.add(image_embedding_tuple)  # Track the unique embedding\n",
    "            all_image_embeddings.append(image_embeddings.cpu().detach())\n",
    "            all_image_labels.append(labels.cpu().detach())  # Append label corresponding to the image\n",
    "        \n",
    "    # Concatenate all embeddings into 2D tensors\n",
    "    all_multimodal_embeddings = torch.cat(all_multimodal_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_labels = torch.cat(all_text_labels, dim=0)\n",
    "    all_image_labels = torch.cat(all_image_labels, dim=0)\n",
    "    \n",
    "    return all_multimodal_embeddings, all_text_embeddings, all_image_embeddings, all_text_labels, all_image_labels, all_labels\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def generate_embeddings_for_city(city, folder_name):    \n",
    "    # Load your DataFrame\n",
    "    pickled_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit\"\n",
    "    data_dir = os.path.join(args.data_dir, city + \".csv\")\n",
    "    args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", city, \"image\", \"image\")\n",
    "    df = pd.read_csv(data_dir)\n",
    "\n",
    "    # Mapping every image to its corresponding text\n",
    "    df = map_images_with_text(df)\n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "    # Identify and remove classes with fewer than 2 instances\n",
    "    class_counts = df['VENDOR'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 3].index\n",
    "    df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "    # Split the data into train and test sets (80-20 split)\n",
    "    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered['VENDOR'])\n",
    "\n",
    "    # Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "    train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "\n",
    "    # Augment the training data by adding multiple entries for each image\n",
    "    train_df = augment_image_training_data(train_df)\n",
    "\n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "    test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    directory = os.path.join(pickled_dir, folder_name)\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate embeddings for the training data\n",
    "    multimodal_embeddings_train, text_embeddings_train, image_embeddings_train, text_labels_train, image_labels_train, all_labels_train = generate_embeddings(city, train_dataloader)\n",
    "    torch.save(multimodal_embeddings_train, os.path.join(directory, f\"{city}_multimodaldata_train.pt\"))\n",
    "    torch.save(text_embeddings_train, os.path.join(directory, f\"{city}_textdata_train.pt\"))\n",
    "    torch.save(image_embeddings_train, os.path.join(directory, f\"{city}_imagedata_train.pt\"))\n",
    "    torch.save(text_labels_train, os.path.join(directory, f\"{city}_labels_text_train.pt\"))  # Save text labels separately\n",
    "    torch.save(image_labels_train, os.path.join(directory, f\"{city}_labels_image_train.pt\"))  # Save image labels separately\n",
    "    torch.save(all_labels_train, os.path.join(directory, f\"{city}_labels_multimodal_train.pt\"))  # Save image labels separately\n",
    "    \n",
    "    # Generate embeddings for the testing data\n",
    "    multimodal_embeddings_test, text_embeddings_test, image_embeddings_test, text_labels_test, image_labels_test, all_labels_test = generate_embeddings(city, test_dataloader)\n",
    "    torch.save(multimodal_embeddings_test, os.path.join(directory, f\"{city}_multimodaldata_test.pt\"))\n",
    "    torch.save(text_embeddings_test, os.path.join(directory, f\"{city}_textdata_test.pt\"))\n",
    "    torch.save(image_embeddings_test, os.path.join(directory, f\"{city}_imagedata_test.pt\"))\n",
    "    torch.save(text_labels_test, os.path.join(directory, f\"{city}_labels_text_test.pt\"))  # Save text labels separately\n",
    "    torch.save(image_labels_test, os.path.join(directory, f\"{city}_labels_image_test.pt\"))  # Save image labels separately\n",
    "    torch.save(all_labels_test, os.path.join(directory, f\"{city}_labels_multimodal_test.pt\"))  # Save image labels separately\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3148af-105f-4f53-8640-074b5674b944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for city in [\"chicago\", \"atlanta\", \"detroit\", \"houston\", \"dallas\", \"ny\", \"sf\"]:\n",
    "    generate_embeddings_for_city(city, \"CE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf0dc3-9c8d-421a-987e-b5d69dd70477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for city in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50 + city + \"-\"*50)\n",
    "    generate_embeddings_for_city(city, \"E2E/CE-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8565c904-c0bd-4492-a9eb-cdfeb8602b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_e2e_ce_model(fusion_technique):\n",
    "    # Create an instance of the model\n",
    "    model = multimodalFusionModel(\n",
    "        text_model=text_model,\n",
    "        image_model=image_model,\n",
    "        fusion_technique=fusion_technique,\n",
    "        num_classes=len(label_encoder.classes_),\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        num_training_steps=num_training_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        temperature=args.temp,\n",
    "        loss_function=\"CE\",\n",
    "        ce_weight = 1.0,\n",
    "        supcon_weight = 1.0,\n",
    "        itm_weight = 1.0,\n",
    "        ntxent_weight = 1.0,\n",
    "        num_hard_negatives = 5\n",
    "    )\n",
    "\n",
    "    # Load the checkpoload_e2e_ce_modelint\n",
    "    checkpoint = torch.load(f\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/south/seed:1111/lr-0.0001/CE/0.5/{fusion_technique}/final_model.ckpt\")\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c4d0158-fef6-44f0-a398-94e8e639e469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion technique: attention\n",
      "model:None\n",
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:31<00:00,  2.70it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [01:26<00:00,  2.65it/s]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:23<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:35<00:00,  2.42it/s]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:02<00:00,  1.45it/s]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:26<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion technique: concat\n",
      "model:None\n",
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:32<00:00,  2.69it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [01:28<00:00,  2.59it/s]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:23<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:35<00:00,  2.48it/s]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:36<00:00,  2.49it/s]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:12<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion technique: learned_fusion\n",
      "model:None\n",
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:34<00:00,  2.65it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [01:26<00:00,  2.64it/s]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:25<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:35<00:00,  2.44it/s]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:37<00:00,  2.41it/s]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:12<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion technique: mean\n",
      "model:None\n",
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:33<00:00,  2.67it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [01:30<00:00,  2.55it/s]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:24<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:34<00:00,  2.51it/s]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:36<00:00,  2.45it/s]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:12<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for technique in [\"attention\", \"concat\", \"learned_fusion\", \"mean\"]:\n",
    "    print(f\"fusion technique: {technique}\")\n",
    "    model = None\n",
    "    print(f\"model:{model}\")\n",
    "    \n",
    "    model = load_e2e_ce_model(technique)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move the model to the desired device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for city in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "        print(\"-\"*50 + city + \"-\"*50)\n",
    "        generate_embeddings_for_city(model, city, f\"E2E/CE-{technique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8a82c5-20bf-49d5-a814-35c7ec94f825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_e2e_ce_supcon_model(temp, fusion_technique=\"mean\"):\n",
    "    # Create an instance of the model\n",
    "    model = multimodalFusionModel(\n",
    "        text_model=text_model,\n",
    "        image_model=image_model,\n",
    "        fusion_technique=fusion_technique,\n",
    "        num_classes=len(label_encoder.classes_),\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        num_training_steps=num_training_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        temperature=temp,\n",
    "        loss_function=\"CE\",\n",
    "        ce_weight = 1.0,\n",
    "        supcon_weight = 1.0,\n",
    "        itm_weight = 1.0,\n",
    "        ntxent_weight = 1.0,\n",
    "        num_hard_negatives = 5\n",
    "    )\n",
    "\n",
    "    # Load the checkpoload_e2e_ce_modelint\n",
    "    checkpoint = torch.load(f\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/south/seed:1111/lr-0.0001/CE+SupCon/{temp}/mean/final_model.ckpt\")\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf65c43-6f86-4707-a4ad-61ee48c139f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 0.5\n",
      "model:None\n",
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [06:34<00:00,  1.04it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [01:42<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [04:43<00:00,  1.23s/it]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [01:10<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [01:45<00:00,  1.21s/it]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:29<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:48<00:00,  1.21s/it]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:33<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.5]:\n",
    "    print(f\"temp: {temp}\")\n",
    "    model = None\n",
    "    print(f\"model:{model}\")\n",
    "    \n",
    "    model = load_e2e_ce_supcon_model(temp, fusion_technique=\"mean\")\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move the model to the desired device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for city in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "        print(\"-\"*50 + city + \"-\"*50)\n",
    "        generate_embeddings_for_city(model, city, f\"E2E/CE-SupCon-mean-{temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae0826fb-f0d4-4cf7-b3d6-3d06f537a224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = multimodalFusionModel(\n",
    "    text_model=text_model,\n",
    "    image_model=image_model,\n",
    "    fusion_technique=\"mean\",\n",
    "    num_classes=len(label_encoder.classes_),\n",
    "    learning_rate=args.learning_rate,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    num_training_steps=num_training_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    temperature=args.temp,\n",
    "    loss_function=\"CE+NTXent\",\n",
    "    ce_weight=1.0,\n",
    "    supcon_weight=1.0,\n",
    "    itm_weight=1.0,\n",
    "    num_hard_negatives=5\n",
    ")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/multimodal-baselines/latent_fusion/chicago/seed:1111/lr-0.0001/CE+NTXent/0.3/mean/final_model.ckpt\")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d66eb8-762b-4447-85bf-d2e8f4d95337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for chicago:   1%|          | 8/1314 [00:09<15:50,  1.37it/s]  "
     ]
    }
   ],
   "source": [
    "for city in [\"chicago\", \"atlanta\", \"detroit\", \"houston\", \"dallas\", \"ny\", \"sf\"]:\n",
    "    generate_embeddings_for_city(city, \"CE-NTXent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75542066-fd96-4a21-9a36-c29ec8a802f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = multimodalFusionModel(\n",
    "    text_model=text_model,\n",
    "    image_model=image_model,\n",
    "    fusion_technique=args.fusion_technique,\n",
    "    num_classes=len(label_encoder.classes_),\n",
    "    learning_rate=args.learning_rate,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    num_training_steps=num_training_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    temperature=args.temp,\n",
    "    loss_function=args.loss\n",
    ")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/multimodal-baselines/latent_fusion/chicago/seed:1111/lr-0.0001/CE+SupCon+ITM/0.1/mean/final_model.ckpt\")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57165ab2-aff3-4a14-820f-08fb0866ff99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for chicago: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1314/1314 [06:28<00:00,  3.38it/s]\n",
      "Extracting embeddings for chicago: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:16<00:00,  2.82it/s]\n",
      "Extracting embeddings for atlanta:  19%|â–ˆâ–Š        | 170/916 [01:58<06:11,  2.01it/s]"
     ]
    }
   ],
   "source": [
    "for city in [\"chicago\", \"atlanta\", \"detroit\", \"houston\", \"dallas\", \"ny\", \"sf\"]:\n",
    "    generate_embeddings_for_city(city, \"CE+SupCon+ITM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d02c1-e3c2-453d-af3b-3e1ba3d74702",
   "metadata": {},
   "source": [
    "# Generating true positive and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076ea4df-a8da-479e-a337-d26672309f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_e2e_ce_supcon_model(temp=0.1, fusion_technique=\"mean\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efdb904b-7964-4463-850f-e4bfe29e16cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "trainer = L.Trainer(max_epochs=32, accelerator=\"gpu\", fast_dev_run=False, \n",
    "                    accumulate_grad_batches = 4, # To run the backward step after n batches, helps to increase the batch size\n",
    "                    benchmark = True, # Fastens the training process\n",
    "                    deterministic=True, # Ensures reproducibility \n",
    "                    limit_train_batches=1.0, # trains on 10% of the data,\n",
    "                    check_val_every_n_epoch = 1, # run val loop every 1 training epochs\n",
    "                    # callbacks=[model_checkpoint, early_stop_callback], # Enables model checkpoint and early stopping\n",
    "                    # callbacks=[early_stop_callback],\n",
    "                    # logger = wandb_logger,\n",
    "                    # strategy=DeepSpeedStrategy(stage=3, offload_optimizer=True, offload_parameters=True, offload_params_device='cpu'), # Enable CPU Offloading, and offload parameters to CPU\n",
    "                    # plugins=DeepSpeedPrecisionPlugin(precision='16-mixed') # Mixed Precision system\n",
    "                    precision='16-mixed' # Mixed Precision system\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e362e0d-f582-43c4-ad2e-8ef2cc1c85d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [01:01<00:00,  6.62it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc             0.940451443195343\n",
      "      test_f1_macro         0.9314517378807068\n",
      "      test_f1_micro          0.959435760974884\n",
      "    test_f1_weighted        0.9595904350280762\n",
      "        test_loss           0.2999718487262726\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2999718487262726,\n",
       "  'test_acc': 0.940451443195343,\n",
       "  'test_f1_weighted': 0.9595904350280762,\n",
       "  'test_f1_micro': 0.959435760974884,\n",
       "  'test_f1_macro': 0.9314517378807068}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d39b8-3859-49e3-8dd6-dba6ce3d1cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Train predictions:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1356/1639 [1:04:47<14:29,  3.07s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for batch in tqdm(train_dataloader, desc=\"Extracting Train predictions\"):    \n",
    "    input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "    logits, _ = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27d5ab-6daa-4b77-a888-ae2fa9f4217b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pred_labels = [int(item) for array in pred for item in array]\n",
    "train_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00943531-59c7-4e13-a224-805a100f1c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Extracting Test predictions\"):    \n",
    "    input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "    logits, _ = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5258595-1a57-48fa-ba01-e61932c6ad06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7e6d9-abb1-464b-b515-331ff6fe1515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../error_analysis/multimodal_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_actual_labels, f)\n",
    "    \n",
    "with open('../error_analysis/multimodal_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/multimodal_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0572e7-80ef-4a90-9357-d867cc344070",
   "metadata": {},
   "source": [
    "# Saving TP and FP results for images with and without faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49a8d26c-be30-4a04-ba88-ff57d34503e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Load your DataFrame\n",
    "df = pd.read_csv(\"../data/processed/south.csv\")\n",
    "df['region'] = \"south\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e248545c-04a6-4cc6-a9db-9e9a34dc89d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to map images with text for CLIP model\n",
    "def map_images_with_text_for_clip_model(df, img_dir=\"/workspace/persistent/HTClipper/data/IMAGES\", filter_by=\"vendor\"):\n",
    "    # Initialize a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['TEXT']\n",
    "        all_images = str(row['IMAGES']).split('|')\n",
    "        characteristics = str(row['FACES']).split('|')\n",
    "        if filter_by == \"vendor\":\n",
    "            vendor = row['VENDOR']\n",
    "        elif filter_by == \"id\":\n",
    "            vendor = row['ID']\n",
    "        region = row['region']\n",
    "        \n",
    "        # Create a new entry for each image\n",
    "        for index, image in enumerate(all_images):\n",
    "            full_image_path = os.path.join(img_dir, region, \"image\", \"image\", image)\n",
    "            \n",
    "            # Only add the row if the image exists at the specified path\n",
    "            if os.path.exists(full_image_path):\n",
    "                new_rows.append({\n",
    "                    'TEXT': text,\n",
    "                    'IMAGES': full_image_path,  # Store the full image path\n",
    "                    'VENDOR': vendor,\n",
    "                    'region' : region,\n",
    "                    'FACES' : characteristics[index]\n",
    "                })\n",
    "\n",
    "    # Create a new dataframe from the list of new rows\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f6c6f40-4852-47e4-b186-db4f8596152a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping every image to it's corresponding text\n",
    "df = map_images_with_text_for_clip_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd594f8-91f5-41fc-ab45-184f0cd44529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "# Identify and remove classes with fewer than 2 instances\n",
    "# Since we use stratify during splitting, we should atleast have one training example in training and one in test dataset\n",
    "class_counts = df['VENDOR'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df_filtered = df[df['VENDOR'].isin(valid_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e00976c2-e1a3-4e95-aa9a-a175f6530ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered['VENDOR'])\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.05, random_state=args.seed, stratify=train_df['VENDOR'])\n",
    "\n",
    "# Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "\n",
    "# Augment the training data by adding multiple entries for each image\n",
    "# train_df = augment_image_training_data(train_df)\n",
    "\n",
    "# %% Intializing the tokenizers and models\n",
    "# Since these are the two models that performed individually on the text and image modalities, we establish them as benchmarks and\n",
    "# only run use them in our further experiments.\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "text_model = AutoModel.from_pretrained('johngiorgi/declutr-small')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7583b04b-d285-4007-8560-001309ad4119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_face_vendor_dict = dict(Counter(train_df[train_df['FACES'] == \"yes\"]['VENDOR']))\n",
    "train_noface_vendor_dict = dict(Counter(train_df[train_df['FACES'] == \"no\"]['VENDOR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cff75f0-8dc2-4615-8acb-82d2d6a3e2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../error_analysis/multimodal_face_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_face_vendor_dict, f)\n",
    "    \n",
    "with open('../error_analysis/multimodal_noface_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_noface_vendor_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b0bfdcc-a898-4098-bdaa-b28aae697805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Faces Dataset\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "# val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "test_dataset = MultimodalDataset(test_df[test_df.FACES == \"yes\"], text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebb02e70-d9a3-4f93-8a05-646138af3ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Test predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:51<00:00,  3.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Extracting Test predictions\"):    \n",
    "    input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "    \n",
    "    logits, _ = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=pixel_values.to(device))\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "412b633a-6c1c-4bb5-82dd-acae0bc37166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a60d5877-11db-4d78-a7b9-425bcf842611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../error_analysis/multimodal_faceclassification_text_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/mulitmodal_faceclassification_text_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e160dd7-e9c2-4858-a8cd-e961cf1516d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No Faces Dataset\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "# val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "test_dataset = MultimodalDataset(test_df[test_df.FACES == \"no\"], text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8031eb58-9e2e-4d97-bda0-063f478c865b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Test predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [01:09<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Extracting Test predictions\"):    \n",
    "    input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "    \n",
    "    logits, _ = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=pixel_values.to(device))\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78ad9d74-343d-43ae-83e9-6c09c63e261e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e79c832-9ded-4781-9f7a-0d4e6feddb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../error_analysis/multimodal_nofaceclassification_text_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/mulitmodal_nofaceclassification_text_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d16b74-b561-4d1a-b322-7cc8df91dbf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Retrieval Data for Multimodal Systems with Images fAces and no faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1900ef35-4bab-44ee-827e-f8fc541c0fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_e2e_ce_supcon_model(temp=0.5, fusion_technique=\"mean\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b879d79f-076b-414b-b6c3-09796d18a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_faces_embeddings(model, city, dataloader):\n",
    "    # Initialize lists to store embeddings\n",
    "    all_multimodal_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_image_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Iterate through all batches in the dataloader with a progress bar\n",
    "    for batch in tqdm(dataloader, desc=f\"Extracting embeddings for {city}\"):\n",
    "        input_ids, attention_mask, pixel_values, labels = batch.get('input_ids'), batch.get('attention_mask'), batch.get('pixel_values'), batch['label']\n",
    "\n",
    "        # Extract multimodal embeddings\n",
    "        multimodal_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=pixel_values.to(device))\n",
    "        all_multimodal_embeddings.append(multimodal_embeddings.cpu().detach())\n",
    "\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = model.extract_embeddings(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        all_text_embeddings.append(text_embeddings.cpu().detach())\n",
    "\n",
    "        # Extract image embeddings\n",
    "        image_embeddings = model.extract_embeddings(pixel_values=pixel_values.to(device))\n",
    "        all_image_embeddings.append(image_embeddings.cpu().detach())\n",
    "        \n",
    "        # Extract labels\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "    # Concatenate all embeddings into 2D tensors\n",
    "    all_multimodal_embeddings = torch.cat(all_multimodal_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_multimodal_embeddings, all_text_embeddings, all_image_embeddings, all_labels\n",
    "\n",
    "def generate_face_embeddings_for_city(model, city, mode=\"face\"):    \n",
    "    # %% Load your DataFrame\n",
    "    pickled_dir = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/multimodal_baselines/\"\n",
    "    data_dir = os.path.join(args.data_dir, city + \".csv\")\n",
    "    args.image_dir = os.path.join(\"/workspace/persistent/HTClipper/data/IMAGES\", city, \"image\", \"image\")\n",
    "    df = pd.read_csv(data_dir)\n",
    "    df['region'] = city\n",
    "\n",
    "    df = map_images_with_text_for_clip_model(df)\n",
    "    \n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "    # Identify and remove classes with fewer than 2 instances\n",
    "    # Since we use stratify during splitting, we should atleast have one training example in training and one in test dataset\n",
    "    class_counts = df['VENDOR'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "    \n",
    "    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=args.seed, stratify=df_filtered['VENDOR'])\n",
    "\n",
    "    # Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "    train_df['TEXT'] = train_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "\n",
    "    # Augment the training data by adding multiple entries for each image\n",
    "    # train_df = augment_image_training_data(train_df)\n",
    "    \n",
    "    # Since these are the two models that performed individually on the text and image modalities, we establish them as benchmarks and\n",
    "    # only run use them in our further experiments.\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "    text_model = AutoModel.from_pretrained('johngiorgi/declutr-small')\n",
    "    image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    image_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    if mode == \"face\":\n",
    "        test_df = test_df[test_df.FACES == \"yes\"]\n",
    "    else:\n",
    "        test_df = test_df[test_df.FACES == \"no\"]\n",
    "    \n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=args.augment_data)\n",
    "    # val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "    test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, label_encoder, image_dir=args.image_dir, augment=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    file_dir = f\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/error_analysis/multimodal_baseline/trained_declutr-vit/{mode}\"\n",
    "    Path(file_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    multimodal_embeddings, text_embeddings, image_embeddings, labels = generate_faces_embeddings(model, city, train_dataloader)\n",
    "    label_filename = city + \"_labels_train.pt\"\n",
    "    multimodal_data_filename = city + \"_multimodaldata_train.pt\"\n",
    "    text_data_filename = city + \"_textdata_train.pt\"\n",
    "    image_data_filename = city + \"_imagedata_train.pt\"\n",
    "    \n",
    "    \n",
    "    torch.save(multimodal_embeddings, os.path.join(file_dir, multimodal_data_filename))\n",
    "    torch.save(text_embeddings, os.path.join(file_dir, text_data_filename))\n",
    "    torch.save(image_embeddings, os.path.join(file_dir, image_data_filename))\n",
    "    torch.save(labels, os.path.join(file_dir, label_filename))\n",
    "    \n",
    "    multimodal_embeddings, text_embeddings, image_embeddings, labels = generate_faces_embeddings(model, city, test_dataloader)\n",
    "    label_filename = city + \"_labels_test.pt\"\n",
    "    multimodal_data_filename = city + \"_multimodaldata_test.pt\"\n",
    "    text_data_filename = city + \"_textdata_test.pt\"\n",
    "    image_data_filename = city + \"_imagedata_test.pt\"\n",
    "    torch.save(multimodal_embeddings, os.path.join(file_dir, multimodal_data_filename))\n",
    "    torch.save(text_embeddings, os.path.join(file_dir, text_data_filename))\n",
    "    torch.save(image_embeddings, os.path.join(file_dir, image_data_filename))\n",
    "    torch.save(labels, os.path.join(file_dir, label_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce61949d-afeb-456a-a014-591189c2466f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [09:39<00:00,  1.41s/it]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:09<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [05:03<00:00,  1.33s/it]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:54<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [01:55<00:00,  1.32s/it]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:20<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [02:33<00:00,  1.65s/it]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:30<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for city in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50 + city + \"-\"*50)\n",
    "    generate_face_embeddings_for_city(model, city, mode=\"face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f88d72fd-9d34-47f7-8fbe-6515368d31e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [03:43<00:00,  1.83it/s]\n",
      "Extracting embeddings for south: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [01:30<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------midwest--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [02:10<00:00,  1.75it/s]\n",
      "Extracting embeddings for midwest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:44<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------west--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:52<00:00,  1.67it/s]\n",
      "Extracting embeddings for west: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:20<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------northeast--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:50<00:00,  1.86it/s]\n",
      "Extracting embeddings for northeast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:18<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for city in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "    print(\"-\"*50 + city + \"-\"*50)\n",
    "    generate_face_embeddings_for_city(model, city, mode=\"noface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf4bd1-1adf-4152-9856-0f8e5cd645e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
