{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3e8f4-f36a-442e-bff6-0944f0b93b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code Overview\n",
    "# Extracts text, vision, and multmodal embeddings from the multimodal DeCLUTR-ViT backbone trained with CLIP, CLIP-ITM, and BLIP2 objectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa93272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python version: 3.10\n",
    "Description: Performs Multimodal Authorship Attribution using CLIP training strategy\n",
    "\"\"\"\n",
    "\n",
    "# %% Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, ViTImageProcessor\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from utilities import map_images_with_text_for_clip_model\n",
    "from loadData import FineTuneCLIPstyleModelDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "058ece35-b570-482f-9995-7f0e63f03dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress TorchDynamo errors and fall back to eager execution\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2a0d564-f92d-4a97-bae6-39c9e6fef6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.logged_entry_name = \"multimodal-latent-fusion-seed:1111\"\n",
    "        self.data_dir = '/workspace/persistent/HTClipper/data/processed'\n",
    "        self.image_dir = \"/workspace/persistent/HTClipper/data/IMAGES\"\n",
    "        self.save_dir = os.path.join(os.getcwd(), \"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/\")\n",
    "        self.model_dir_name = None\n",
    "        self.pairing_mode = \"non-associated\"\n",
    "        self.model_type = \"CLIP\"  # Can be \"CLIP\", \"BLIP2\", or \"CLIPITM\"\n",
    "        self.batch_size = 32\n",
    "        self.nb_epochs = 40\n",
    "        self.patience = 3\n",
    "        self.nb_negatives = 1\n",
    "        self.seed = 1111\n",
    "        self.warmup_steps = 0\n",
    "        self.grad_steps = 1\n",
    "        self.learning_rate = 6e-4\n",
    "        self.train_data_percentage = 1.0\n",
    "        self.adam_epsilon = 1e-6\n",
    "        self.min_delta_change = 0.01\n",
    "        self.weight_decay = 0.01\n",
    "        self.augment_data = False\n",
    "        self.nb_augmented_samples = 1\n",
    "        self.loss = 'CE'\n",
    "        self.temp = 0.5\n",
    "        self.repr = \"CLS\"\n",
    "\n",
    "# Instantiate the arguments\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f724a25c-2bec-41dd-8622-7d7a8c9d1588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FineTuneCLIPClassifier(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model, finetune_mode, extract_representation_from, num_classes, weight_decay, eps, warmup_steps, num_training_steps, \n",
    "                learning_rate, loss_fn, temperature):\n",
    "        super(FineTuneCLIPClassifier, self).__init__()\n",
    "        self.text_model = pretrained_model.text_model\n",
    "        self.image_model = pretrained_model.image_model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.loss_fn_name = loss_fn\n",
    "        self.extract_representation_from = extract_representation_from\n",
    "        self.temperature = temperature\n",
    "        self.finetune_mode = finetune_mode\n",
    "\n",
    "        self.validation_outputs = []  # To store validation outputs\n",
    "        self.test_outputs = []  # To store test outputs\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
    "\n",
    "        # Loss function\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            self.supcon_loss = SupConLoss(self.temperature)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Get text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if self.extract_representation_from == \"CLS\":\n",
    "            # Use CLS token embedding\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "        elif self.extract_representation_from == \"EOS\":\n",
    "            # Get the positions of the last non-padding tokens\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            text_embeddings = text_outputs.last_hidden_state[torch.arange(input_ids.size(0)), sequence_lengths, :]\n",
    "        else:\n",
    "            raise ValueError(\"extract_representation_from must be 'CLS' or 'EOS'\")\n",
    "\n",
    "        # Get image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "\n",
    "        # Take the mean of text and image embeddings\n",
    "        embeddings = (text_embeddings + image_embeddings) / 2\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits, embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "\n",
    "        self.validation_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # self.log(\"val_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.test_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "        self.log(\"test_loss\", loss)\n",
    "        # self.log(\"test_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    # At the end of validation epoch, calculate accuracy and F1 scores\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_preds = torch.cat([x['preds'] for x in self.validation_outputs])\n",
    "        val_labels = torch.cat([x['labels'] for x in self.validation_outputs])\n",
    "        val_acc = balanced_accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_f1_weighted = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='weighted')\n",
    "        val_f1_micro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "        val_f1_macro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='macro')\n",
    "        self.log('val_acc', val_acc, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_weighted', val_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_micro', val_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_macro', val_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    # At the end of the test epoch, calculate accuracy and F1 scores\n",
    "    def on_test_epoch_end(self):\n",
    "        test_preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
    "        test_labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
    "        test_acc = balanced_accuracy_score(test_labels.cpu().numpy(), test_preds.cpu().numpy())\n",
    "        test_f1_weighted = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='weighted')\n",
    "        test_f1_micro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1_macro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='macro')\n",
    "        self.log('test_acc', test_acc, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_weighted', test_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_micro', test_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_macro', test_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Freeze layers if needed\n",
    "        if self.finetune_mode == \"finetune_layers\":\n",
    "            for name, param in self.text_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif self.finetune_mode == \"all\":\n",
    "            # Unfreeze all layers\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.num_training_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def get_embedding(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tokenized input text (for text embeddings).\n",
    "            attention_mask: Attention mask for input text (for text embeddings).\n",
    "            pixel_values: Preprocessed image (for image embeddings).\n",
    "            embedding_type: Specify 'text', 'image', or 'multimodal' to generate the respective embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings as torch.Tensor.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use EOS token embedding\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, -1, :]\n",
    "            # text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            # Use CLS token embedding\n",
    "            image_embeddings = image_outputs.last_hidden_state[:, 0, :]\n",
    "            # image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "            return image_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.get_embedding(input_ids, attention_mask, embedding_type='text')\n",
    "            image_embeddings = self.get_embedding(pixel_values=pixel_values, embedding_type='image')\n",
    "            # Average embeddings\n",
    "            embeddings = (text_embeddings + image_embeddings) / 2\n",
    "            # embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "            return embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b1d648d-19b9-4eef-a25e-2decde01f0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FineTuneCLIPITMClassifier(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model, finetune_mode, num_classes, weight_decay, eps, warmup_steps, num_training_steps, \n",
    "                learning_rate, loss_fn, temperature):\n",
    "        super(FineTuneCLIPITMClassifier, self).__init__()\n",
    "        self.text_model = pretrained_model.text_model\n",
    "        self.image_model = pretrained_model.image_model\n",
    "        self.qformer = pretrained_model.qformer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.loss_fn_name = loss_fn\n",
    "        self.finetune_mode = finetune_mode\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.validation_outputs = []  # To store validation outputs\n",
    "        self.test_outputs = []  # To store test outputs\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
    "\n",
    "        # Loss function\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            self.supcon_loss = SupConLoss(self.temperature)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the custom Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        query_embeddings = query_embeddings.mean(dim=1)  # Mean pooling over queries\n",
    "\n",
    "        # Take the mean of text and image embeddings\n",
    "        embeddings = (text_embeddings + query_embeddings) / 2\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits, embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.validation_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # self.log(\"val_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.test_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        # self.log(\"test_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_preds = torch.cat([x['preds'] for x in self.validation_outputs])\n",
    "        val_labels = torch.cat([x['labels'] for x in self.validation_outputs])\n",
    "        val_acc = balanced_accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_f1_weighted = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='weighted')\n",
    "        val_f1_micro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "        val_f1_macro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='macro')\n",
    "        self.log('val_acc', val_acc, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_weighted', val_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_micro', val_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_macro', val_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    # At the end of the test epoch, calculate accuracy and F1 scores\n",
    "    def on_test_epoch_end(self):\n",
    "        test_preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
    "        test_labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
    "        test_acc = balanced_accuracy_score(test_labels.cpu().numpy(), test_preds.cpu().numpy())\n",
    "        test_f1_weighted = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='weighted')\n",
    "        test_f1_micro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1_macro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='macro')\n",
    "        self.log('test_acc', test_acc, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_weighted', test_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_micro', test_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_macro', test_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Freeze layers if needed\n",
    "        if self.finetune_mode == \"finetune_layers\":\n",
    "            for name, param in self.text_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            for name, param in self.qformer.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif self.finetune_mode == \"all\":\n",
    "            # Unfreeze all layers\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.qformer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.num_training_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_text_embeddings(self, input_ids, attention_mask):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return text_embeddings\n",
    "\n",
    "    def get_image_embeddings(self, pixel_values):\n",
    "        # Extract image embeddings before Q-Former\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return image_embeddings\n",
    "\n",
    "    def get_multimodal_embeddings(self, pixel_values):\n",
    "        # Get image embeddings from the image model\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        # Aggregate the query embeddings, e.g., mean pooling\n",
    "        multimodal_embeddings = query_embeddings.mean(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "        return multimodal_embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the get_embeddings function\n",
    "    def get_embedding(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tokenized input text (for text embeddings).\n",
    "            attention_mask: Attention mask for input text (for text embeddings).\n",
    "            pixel_values: Preprocessed image (for image embeddings).\n",
    "            embedding_type: Specify 'text', 'image', or 'multimodal' to generate the respective embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings as torch.Tensor.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use CLS token embedding\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "            # Pass through Q-Former\n",
    "            query_embeddings = self.qformer(image_embeddings)\n",
    "            # Mean pooling over queries\n",
    "            image_embeddings = query_embeddings.mean(dim=1)\n",
    "            image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "            return image_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.get_embedding(input_ids, attention_mask, embedding_type='text')\n",
    "            image_embeddings = self.get_embedding(pixel_values=pixel_values, embedding_type='image')\n",
    "            # Average embeddings\n",
    "            embeddings = (text_embeddings + image_embeddings) / 2\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "            return embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99341bb8-586c-4051-b934-921612ceee73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from transformers import AutoModel, ViTModel, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "# import bitsandbytes as bnb\n",
    "# from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "sys.path.append(\"../architectures/\")\n",
    "from multimodalLayer import SupConLoss\n",
    "\n",
    "class FineTuneBLIP2Classifier(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model, finetune_mode, num_classes, weight_decay, eps, warmup_steps, num_training_steps, \n",
    "                learning_rate, loss_fn, temperature):        \n",
    "        super(FineTuneBLIP2Classifier, self).__init__()\n",
    "        self.text_model = pretrained_model.text_model\n",
    "        self.image_model = pretrained_model.image_model\n",
    "        self.qformer = pretrained_model.qformer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.loss_fn_name = loss_fn\n",
    "        self.finetune_mode = finetune_mode\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
    "\n",
    "        self.validation_outputs = []  # To store validation outputs\n",
    "        self.test_outputs = []  # To store test outputs\n",
    "\n",
    "        # Loss function\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            self.supcon_loss = SupConLoss(self.temperature)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the custom Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        query_embeddings = query_embeddings.mean(dim=1)  # Mean pooling over queries\n",
    "\n",
    "        # Take the mean of text and image embeddings\n",
    "        embeddings = (text_embeddings + query_embeddings) / 2\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits, embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        if self.loss_fn_name == \"CE+SupCon\":\n",
    "            features = F.normalize(embeddings, dim=1)\n",
    "            supcon_loss = self.supcon_loss(features, batch[\"labels\"])\n",
    "            loss += supcon_loss\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.validation_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # self.log(\"val_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits, embeddings = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"]\n",
    "        )\n",
    "        loss = self.ce_loss(logits, batch[\"labels\"])\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.test_outputs.append({'preds': preds, 'labels': batch[\"labels\"]})\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        # self.log(\"test_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_preds = torch.cat([x['preds'] for x in self.validation_outputs])\n",
    "        val_labels = torch.cat([x['labels'] for x in self.validation_outputs])\n",
    "        val_acc = balanced_accuracy_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        val_f1_weighted = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='weighted')\n",
    "        val_f1_micro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='micro')\n",
    "        val_f1_macro = f1_score(val_labels.cpu().numpy(), val_preds.cpu().numpy(), average='macro')\n",
    "        self.log('val_acc', val_acc, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_weighted', val_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_micro', val_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1_macro', val_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    # At the end of the test epoch, calculate accuracy and F1 scores\n",
    "    def on_test_epoch_end(self):\n",
    "        test_preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
    "        test_labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
    "        test_acc = balanced_accuracy_score(test_labels.cpu().numpy(), test_preds.cpu().numpy())\n",
    "        test_f1_weighted = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='weighted')\n",
    "        test_f1_micro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='micro')\n",
    "        test_f1_macro = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='macro')\n",
    "        self.log('test_acc', test_acc, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_weighted', test_f1_weighted, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_micro', test_f1_micro, on_step=False, on_epoch=True)\n",
    "        self.log('test_f1_macro', test_f1_macro, on_step=False, on_epoch=True)\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Freeze layers if needed\n",
    "        if self.finetune_mode == \"finetune_layers\":\n",
    "            for name, param in self.text_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            for name, param in self.qformer.named_parameters():\n",
    "                layer_number = int(name.split(\".\")[2]) if \"layer\" in name else None\n",
    "                if layer_number not in args.layers_to_finetune:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif self.finetune_mode == \"all\":\n",
    "            # Unfreeze all layers\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.image_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.qformer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if p.requires_grad and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.num_training_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_text_embeddings(self, input_ids, attention_mask):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return text_embeddings\n",
    "\n",
    "    def get_image_embeddings(self, pixel_values):\n",
    "        # Extract image embeddings before Q-Former\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return image_embeddings\n",
    "\n",
    "    def get_multimodal_embeddings(self, pixel_values):\n",
    "        # Get image embeddings from the image model\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        # Aggregate the query embeddings, e.g., mean pooling\n",
    "        multimodal_embeddings = query_embeddings.mean(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "        return multimodal_embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the get_embeddings function\n",
    "    def get_embedding(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tokenized input text (for text embeddings).\n",
    "            attention_mask: Attention mask for input text (for text embeddings).\n",
    "            pixel_values: Preprocessed image (for image embeddings).\n",
    "            embedding_type: Specify 'text', 'image', or 'multimodal' to generate the respective embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings as torch.Tensor.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use CLS token embedding\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "            # Pass through Q-Former\n",
    "            query_embeddings = self.qformer(image_embeddings)\n",
    "            # Mean pooling over queries\n",
    "            image_embeddings = query_embeddings.mean(dim=1)\n",
    "            image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "            return image_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.get_embedding(input_ids, attention_mask, embedding_type='text')\n",
    "            image_embeddings = self.get_embedding(pixel_values=pixel_values, embedding_type='image')\n",
    "            # Average embeddings\n",
    "            embeddings = (text_embeddings + image_embeddings) / 2\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "            return embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dad130de-7d38-4565-a64c-4cb3443c05d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "df = pd.read_csv(\"/workspace/persistent/HTClipper/data/processed/south.csv\")\n",
    "df['region'] = 'south'\n",
    "\n",
    "# Map images with text\n",
    "df = map_images_with_text_for_clip_model(df, img_dir=args.image_dir).drop_duplicates()\n",
    "\n",
    "# Identify and remove classes with fewer than 2 instances\n",
    "class_counts = df['VENDOR'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 3].index\n",
    "df_filtered = df[df['VENDOR'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_filtered['label'] = label_encoder.fit_transform(df_filtered['VENDOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6dd2ad4f-407b-4ce4-b5bd-6d9ddd04d997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 1463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "803abb85-e0e5-405d-af62-fc15eac835fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.model_type = \"CLIP\"\n",
    "args.repr = \"EOS\"\n",
    "args.loss = \"CE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2aadf428-f4c4-4893-b8bb-677b4302d29c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_clipstyle_models(args):\n",
    "\n",
    "    # Initialize the model\n",
    "    if args.model_type == \"CLIP\":\n",
    "\n",
    "        sys.path.append('../architectures/')\n",
    "        from CLIPLayer import CLIPModel\n",
    "\n",
    "        # Initialize the pre-trained model\n",
    "        model = CLIPModel(weight_decay=args.weight_decay, eps=args.adam_epsilon, warmup_steps=100, num_training_steps=1000)\n",
    "        checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/CLIP/non-associated/seed:1111/lr-0.0001/NTXENT/0.1/negatives-5\", \"final_model.ckpt\"), map_location=device)\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "        # Loading the classifier\n",
    "        model = FineTuneCLIPClassifier(pretrained_model=model, finetune_mode=\"all\", num_classes=num_classes, weight_decay=args.weight_decay, eps=args.adam_epsilon, \n",
    "                                       warmup_steps=100, num_training_steps=1000, learning_rate=args.learning_rate, loss_fn=args.loss, temperature=0.1, \n",
    "                                       extract_representation_from=args.repr)\n",
    "        if args.loss == \"CE\" and args.repr == \"CLS\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/CLIP/finetune-all/representations_CLS/seed:1111/lr-0.0001/temp-0.1/CE\", \"final_model.ckpt\"))\n",
    "        elif args.loss == \"CE+SupCon\" and args.repr == \"CLS\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/CLIP/finetune-all/representations_CLS/seed:1111/lr-0.0001/temp-0.1/CE+SupCon\", \"final_model.ckpt\"))\n",
    "        elif args.loss == \"CE+SupCon\" and args.repr == \"EOS\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/CLIP/finetune-all/representations_EOS/seed:1111/lr-0.0001/temp-0.1/CE+SupCon\", \"final_model.ckpt\"))\n",
    "        else:\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/CLIP/finetune-all/representations_EOS/seed:1111/lr-0.0001/temp-0.1/CE\", \"final_model.ckpt\"))\n",
    "\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False) \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Move the model to the desired device\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    if args.model_type == \"CLIPITM\":\n",
    "\n",
    "        sys.path.append('../architectures/')\n",
    "        from CLIPITMLayer import CLIPITMModel\n",
    "\n",
    "        model = CLIPITMModel(weight_decay=args.weight_decay, eps=args.adam_epsilon, warmup_steps=100, num_training_steps=1000)\n",
    "        checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/CLIPITM/non-associated/seed:1111/lr-0.0001/NTXENT/BERTqformer/0.1/negatives-5\", \"final_model.ckpt\"), map_location=device)\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # Loading the classifier\n",
    "        model = FineTuneCLIPITMClassifier(pretrained_model=model, finetune_mode=\"all\", num_classes=num_classes, weight_decay=args.weight_decay, eps=args.adam_epsilon, \n",
    "                                       warmup_steps=100, num_training_steps=1000, learning_rate=args.learning_rate, loss_fn=args.loss, temperature=0.1)\n",
    "\n",
    "        if args.loss == \"CE\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/CLIPITM/finetune-all/representations_CLS/seed:1111/lr-0.0001/temp-0.1/CE\", \"final_model.ckpt\"))\n",
    "        else:\n",
    "            raise Exception(\"Model Still to be trained ....\")\n",
    "\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False) \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Move the model to the desired device\n",
    "        model = model.to(device)\n",
    "\n",
    "    elif args.model_type == \"BLIP2\":\n",
    "        sys.path.append('../architectures/')\n",
    "        from BLIP2Layer import BLIP2Model\n",
    "\n",
    "        model = BLIP2Model(weight_decay=args.weight_decay, eps=args.adam_epsilon, warmup_steps=100, num_training_steps=1000)\n",
    "        checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/BLIP2/non-associated/seed:1111/lr-0.0001/NTXENT/0.1/negatives-5\", \"final_model.ckpt\"), map_location=device)\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # Loading the classifier\n",
    "        model = FineTuneBLIP2Classifier(pretrained_model=model, finetune_mode=\"all\", num_classes=num_classes, weight_decay=args.weight_decay, eps=args.adam_epsilon, \n",
    "                                       warmup_steps=100, num_training_steps=1000, learning_rate=args.learning_rate, loss_fn=args.loss, temperature=0.1)  \n",
    "\n",
    "        if args.loss == \"CE\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/BLIP2/finetune-all/representations_CLS/seed:1111/lr-0.0001/temp-0.1/CE\", \"final_model.ckpt\"))\n",
    "        elif args.loss == \"CE+SupCon\":\n",
    "            checkpoint = torch.load(os.path.join(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/classification/finetuned/BLIP2/finetune-all/representations_CLS/seed:1111/lr-0.0001/temp-0.1/CE+SupCon\", \"final_model.ckpt\"))\n",
    "        else:\n",
    "            raise Exception(\"Model Still to be trained ....\")\n",
    "\n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False) \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a77f47bf-fae8-4db4-a6a6-f407074365da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "args.model_type = \"CLIP\"\n",
    "args.repr = \"EOS\"\n",
    "args.loss = \"CE\"\n",
    "\n",
    "model = load_clipstyle_models(args)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e3e83-ace7-4b1e-a039-6b8f0522bd3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b5c0493-cd71-4e30-9bd9-f1daa57063ea",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e0cb314-da70-4857-b1af-6afb7b65a7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b56c5c5-612a-4ccd-8258-ba5e5fcd4d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Initialize the tokenizers and models\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8bfc22f-4ae6-448f-a31a-ef1839a19f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to map images with text for CLIP model\n",
    "def map_images_with_text_for_clip_model(df, img_dir, filter_by=\"vendor\"):\n",
    "    # Initialize a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['TEXT']\n",
    "        all_images = str(row['IMAGES']).split('|')\n",
    "        if filter_by == \"vendor\":\n",
    "            vendor = row['VENDOR']\n",
    "        elif filter_by == \"id\":\n",
    "            vendor = row['ID']\n",
    "        region = row['region']\n",
    "        \n",
    "        # Create a new entry for each image\n",
    "        for image in all_images:\n",
    "            full_image_path = os.path.join(img_dir, region, \"image\", \"image\", image)\n",
    "            \n",
    "            # Only add the row if the image exists at the specified path\n",
    "            if os.path.exists(full_image_path):\n",
    "                new_rows.append({\n",
    "                    'TEXT': text,\n",
    "                    'IMAGES': full_image_path,  # Store the full image path\n",
    "                    'VENDOR': vendor,\n",
    "                    'region' : region\n",
    "                })\n",
    "\n",
    "    # Create a new dataframe from the list of new rows\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1c2da96-7f10-47ef-96d4-b32df85ba68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to process each dataset\n",
    "def process_dataset_for_ClassifierModel(region_name, data_dir, image_dir, model, model_name, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    assert model_name in [\"CLIP\", \"CLIP-EOS\", \"CLIP-EOS-CESupCon\", \"CLIPITM\", \"BLIP2\", \"BLIP2-CESupCon\"]\n",
    "    assert filter_by in [\"vendor\", \"id\"]\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_clip_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    # Filter the dataframe if needed\n",
    "    df_filtered = df\n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_filtered['label'] = label_encoder.fit_transform(df_filtered['VENDOR'])\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=1111)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = FineTuneCLIPstyleModelDataset(train_df, text_tokenizer, image_processor)\n",
    "    test_dataset = FineTuneCLIPstyleModelDataset(test_df, text_tokenizer, image_processor)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Collect embeddings and labels\n",
    "    def get_embeddings(dataloader):\n",
    "        image_embeddings = []\n",
    "        text_embeddings = []\n",
    "        multimodal_embeddings = []\n",
    "        labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Fetching Embeddings', leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                batch_labels = batch['labels']\n",
    "\n",
    "                # Get embeddings\n",
    "                text_embeds = model.get_embedding(input_ids=input_ids, attention_mask=attention_mask, embedding_type='text')\n",
    "                image_embeds = model.get_embedding(pixel_values=pixel_values, embedding_type='image')\n",
    "                # Compute multimodal embeddings\n",
    "                multimodal_embeds = model.get_embedding(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    pixel_values=pixel_values,\n",
    "                    embedding_type='multimodal'\n",
    "                )\n",
    "\n",
    "                text_embeddings.append(text_embeds.cpu().numpy())\n",
    "                image_embeddings.append(image_embeds.cpu().numpy())\n",
    "                multimodal_embeddings.append(multimodal_embeds.cpu().numpy())\n",
    "                labels.extend(batch_labels)\n",
    "\n",
    "        text_embeddings = np.concatenate(text_embeddings)\n",
    "        image_embeddings = np.concatenate(image_embeddings)\n",
    "        multimodal_embeddings = np.concatenate(multimodal_embeddings)\n",
    "        return text_embeddings, image_embeddings, multimodal_embeddings, labels\n",
    "\n",
    "    # Get embeddings for train and test sets\n",
    "    train_text_embeddings, train_image_embeddings, train_multimodal_embeddings, train_labels = get_embeddings(train_dataloader)\n",
    "    test_text_embeddings, test_image_embeddings, test_multimodal_embeddings, test_labels = get_embeddings(test_dataloader)\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/finetuned_declutr_vit/\", model_name)\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_{filter_by}.npy'), train_text_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_{filter_by}.npy'), train_image_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_multimodal_embeddings_{region_name}_{filter_by}.npy'), train_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_labels_{region_name}_{filter_by}.npy'), train_labels)\n",
    "\n",
    "    np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_{filter_by}.npy'), test_text_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_{filter_by}.npy'), test_image_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_multimodal_embeddings_{region_name}_{filter_by}.npy'), test_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_labels_{region_name}_{filter_by}.npy'), test_labels)\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_multimodal_embeddings, train_labels, test_text_embeddings, test_image_embeddings, test_multimodal_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37eb8521-f12e-4951-b168-a82307e93257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------south-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed region: south\n",
      "Number of training samples: 52435\n",
      "Number of testing samples: 13109\n",
      "\n",
      "------------------------------midwest-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed region: midwest\n",
      "Number of training samples: 29297\n",
      "Number of testing samples: 7325\n",
      "\n",
      "------------------------------west-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed region: west\n",
      "Number of training samples: 11104\n",
      "Number of testing samples: 2777\n",
      "\n",
      "------------------------------northeast-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed region: northeast\n",
      "Number of training samples: 11797\n",
      "Number of testing samples: 2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of regions to process\n",
    "regions = ['south', 'midwest', 'west', 'northeast']\n",
    "\n",
    "# , 'northeast'\n",
    "# Process each region\n",
    "for region in regions:\n",
    "    print(\"------------------------------\" + region + \"-------------------------\")\n",
    "    train_text_embeddings, train_image_embeddings, train_multimodal_embeddings, train_labels, test_text_embeddings, test_image_embeddings, test_multimodal_embeddings, test_labels = process_dataset_for_ClassifierModel(\n",
    "    region_name=region,\n",
    "    data_dir=args.data_dir,\n",
    "    image_dir=args.image_dir,\n",
    "    model=model,\n",
    "    model_name=\"CLIP-EOS-CESupCon\",\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    # t5_tokenizer=t5_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    filter_by = \"vendor\",\n",
    "    batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fc43f-5041-46ec-943b-3bf3dfc360b3",
   "metadata": {},
   "source": [
    "# New functions that only extracts emnbeddings from unique text and image ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d9c52aae-ea45-469d-8c29-a2f591032060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_for_CLIPModel(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    assert filter_by in [\"vendor\", \"ids\"]\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_clip_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"CLIP\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), np.array(train_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), np.array(train_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), np.array(test_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), np.array(test_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "        \n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), np.array(train_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), np.array(train_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), np.array(test_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), np.array(test_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels\n",
    "\n",
    "def process_dataset_for_CLIPITMModel(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_clip_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"CLIPITM\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "\n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels\n",
    "\n",
    "def process_dataset_for_BLIP2Model(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_blip2_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"BLIP2\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "\n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568f82-4e6d-4db0-91b1-dec9df710796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Text Embeddings: 100%|██████████| 13677/13677 [02:05<00:00, 109.31it/s]\n",
      "Extracting Image Embeddings:  16%|█▌        | 10529/65544 [09:36<52:20, 17.52it/s]  "
     ]
    }
   ],
   "source": [
    "# List of regions to process\n",
    "regions = ['south', 'midwest', 'west', 'northeast']\n",
    "\n",
    "# , 'northeast'\n",
    "# Process each regiond\n",
    "for region in regions:\n",
    "    print(\"-\"*50 + region + \"-\"*50)\n",
    "    _, _, _, _, _, _, _, _ = process_dataset_for_BLIP2Model(\n",
    "        region_name=region,\n",
    "        data_dir=args.data_dir,\n",
    "        image_dir=args.image_dir,\n",
    "        model=model,\n",
    "        text_tokenizer=text_tokenizer,\n",
    "        image_processor=image_processor,\n",
    "        filter_by = \"vendor\",\n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798159cb-5681-4a23-adfc-30e6ab56a57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of regions to process\n",
    "regions = ['south', 'midwest', 'west', 'northeast']\n",
    "\n",
    "# , 'northeast'\n",
    "# Process each region\n",
    "for region in regions:\n",
    "    print(\"-\"*50 + region + \"-\"*50)\n",
    "    _, _, _, _, _, _, _, _ = process_dataset_for_BLIP2Model(\n",
    "        region_name=region,\n",
    "        data_dir=args.data_dir,\n",
    "        image_dir=args.image_dir,\n",
    "        model=model,\n",
    "        text_tokenizer=text_tokenizer,\n",
    "        image_processor=image_processor,\n",
    "        filter_by = \"ids\",\n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b557bb33-d357-4c08-912b-4313cdaf5964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1528, 768), torch.Size([1528]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_embeddings.shape, torch.tensor(test_image_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba47ef91-a4d5-4b6e-ae27-65ceb1eda0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/persistent/HTClipper/data/IMAGES'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map images with text for CLIP model\n",
    "def map_images_with_text_for_clip_model(df, img_dir, filter_by=\"vendor\"):\n",
    "    # Initialize a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['TEXT']\n",
    "        all_images = str(row['IMAGES']).split('|')\n",
    "        if filter_by == \"vendor\":\n",
    "            vendor = row['VENDOR']\n",
    "        elif filter_by == \"id\":\n",
    "            vendor = row['ID']\n",
    "        region = row['region']\n",
    "        \n",
    "        # Create a new entry for each image\n",
    "        for image in all_images:\n",
    "            full_image_path = os.path.join(img_dir, region, \"image\", \"image\", image)\n",
    "            \n",
    "            # Only add the row if the image exists at the specified path\n",
    "            if os.path.exists(full_image_path):\n",
    "                new_rows.append({\n",
    "                    'TEXT': text,\n",
    "                    'IMAGES': full_image_path,  # Store the full image path\n",
    "                    'VENDOR': vendor,\n",
    "                    'region' : region\n",
    "                })\n",
    "\n",
    "    # Create a new dataframe from the list of new rows\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "24c60d89-0cec-43a9-a5c9-d5c3ff69a1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to process each dataset\n",
    "def process_dataset_for_ClassifierModel(region_name, data_dir, image_dir, model, model_name, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # assert model_name in [\"CLIP\", \"CLIP-EOS\", \"CLIP-EOS-CESupCon\", \"CLIPITM\", \"BLIP2\", \"BLIP2-CESupCon\"]\n",
    "    assert filter_by in [\"vendor\", \"id\"]\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    \n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['VENDOR'] = label_encoder.fit_transform(df['VENDOR'])\n",
    "\n",
    "    # Identify and keep vendors with at least 2 instances\n",
    "    class_counts = df['VENDOR'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df_filtered = df[df['VENDOR'].isin(valid_classes)]\n",
    "\n",
    "    # Re-encode labels after filtering\n",
    "    df_filtered['VENDOR'] = label_encoder.fit_transform(df_filtered['VENDOR'])\n",
    "\n",
    "    df_filtered = df_filtered[[\"TEXT\", \"IMAGES\", \"VENDOR\", \"region\"]].drop_duplicates()\n",
    "\n",
    "    # Dynamically adjust test_size based on the number of classes\n",
    "    min_test_size = len(df_filtered['VENDOR'].unique()) / len(df_filtered)\n",
    "    test_size = max(0.2, min_test_size)  # Ensure the test size is at least 20% or large enough to include all classes\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_filtered, test_size=test_size, random_state=args.seed, stratify=df_filtered['VENDOR'], shuffle=True\n",
    "    )\n",
    "\n",
    "    # Apply map_images_with_text separately to avoid overlap of text-image pairs across splits\n",
    "    train_df = map_images_with_text_for_clip_model(train_df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "    test_df = map_images_with_text_for_clip_model(test_df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = FineTuneCLIPstyleModelDataset(train_df, text_tokenizer, image_processor)\n",
    "    test_dataset = FineTuneCLIPstyleModelDataset(test_df, text_tokenizer, image_processor)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Collect embeddings and labels\n",
    "    def get_embeddings(dataloader):\n",
    "        image_embeddings = []\n",
    "        text_embeddings = []\n",
    "        multimodal_embeddings = []\n",
    "        labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Fetching Embeddings', leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                batch_labels = batch['labels']\n",
    "\n",
    "                # Get embeddings\n",
    "                text_embeds = model.get_embedding(input_ids=input_ids, attention_mask=attention_mask, embedding_type='text')\n",
    "                image_embeds = model.get_embedding(pixel_values=pixel_values, embedding_type='image')\n",
    "                # Compute multimodal embeddings\n",
    "                multimodal_embeds = model.get_embedding(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    pixel_values=pixel_values,\n",
    "                    embedding_type='multimodal'\n",
    "                )\n",
    "\n",
    "                text_embeddings.append(text_embeds.cpu().numpy())\n",
    "                image_embeddings.append(image_embeds.cpu().numpy())\n",
    "                multimodal_embeddings.append(multimodal_embeds.cpu().numpy())\n",
    "                labels.extend(batch_labels)\n",
    "\n",
    "        text_embeddings = np.concatenate(text_embeddings)\n",
    "        image_embeddings = np.concatenate(image_embeddings)\n",
    "        multimodal_embeddings = np.concatenate(multimodal_embeddings)\n",
    "        return text_embeddings, image_embeddings, multimodal_embeddings, labels\n",
    "\n",
    "    # Get embeddings for train and test sets\n",
    "    train_text_embeddings, train_image_embeddings, train_multimodal_embeddings, train_labels = get_embeddings(train_dataloader)\n",
    "    test_text_embeddings, test_image_embeddings, test_multimodal_embeddings, test_labels = get_embeddings(test_dataloader)\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/multimodal_baselines/finetuned/\", model_name)\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_{filter_by}.npy'), train_text_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_{filter_by}.npy'), train_image_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_multimodal_embeddings_{region_name}_{filter_by}.npy'), train_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'train_labels_{region_name}_{filter_by}.npy'), train_labels)\n",
    "\n",
    "    np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_{filter_by}.npy'), test_text_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_{filter_by}.npy'), test_image_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_multimodal_embeddings_{region_name}_{filter_by}.npy'), test_multimodal_embeddings)\n",
    "    np.save(os.path.join(output_dir, f'test_labels_{region_name}_{filter_by}.npy'), test_labels)\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_multimodal_embeddings, train_labels, test_text_embeddings, test_image_embeddings, test_multimodal_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a56b7f-351d-4473-86d4-0c16f478a5bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "model_name:CLIP-CE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------south-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------midwest-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------west-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------northeast-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "model_name:CLIPITM-CE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------south-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------midwest-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------west-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------northeast-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "model_name:BLIP2-CE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------south-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------midwest-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Embeddings: 100%|█████████▉| 915/917 [13:41<00:01,  1.16it/s]"
     ]
    }
   ],
   "source": [
    "for model_name in [\"CLIP-CE\", \"CLIPITM-CE\", \"BLIP2-CE\", \"BLIP2-CE-SupCon\"]:\n",
    "    model=None\n",
    "    \n",
    "    print(model)\n",
    "    print(f\"model_name:{model_name}\")\n",
    "    \n",
    "    if model_name == \"CLIP-CE\":\n",
    "        args.model_type = \"CLIP\"\n",
    "        args.repr = \"EOS\"\n",
    "        args.loss = \"CE\"\n",
    "    elif model_name == \"CLIPITM-CE\":\n",
    "        args.model_type = \"CLIPITM\"\n",
    "        args.loss = \"CE\"\n",
    "    elif model_name == \"BLIP2-CE\":\n",
    "        args.model_type = \"BLIP2\"\n",
    "        args.loss = \"CE\"\n",
    "    else:\n",
    "        args.model_type = \"BLIP2\"\n",
    "        args.loss = \"CE+SupCon\"\n",
    "\n",
    "    model = load_clipstyle_models(args)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Move the model to the desired device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for region in [\"south\", \"midwest\", \"west\", \"northeast\"]:\n",
    "        print(f\"------------------------------------------{region}-------------------------\")\n",
    "        process_dataset_for_ClassifierModel(region_name=region, data_dir=args.data_dir, image_dir=args.image_dir, model=model, model_name=model_name, \n",
    "                                            text_tokenizer=text_tokenizer ,image_processor=image_processor, filter_by=\"vendor\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87acfdff-66c0-4e82-97b1-7d8831ee1343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:CLIP-CE-param:169632695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:CLIPITM-CE-param:307506359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:BLIP2-CE-param:307506359\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate total trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for model_name in [\"CLIP-CE\", \"CLIPITM-CE\", \"BLIP2-CE\"]:\n",
    "    model=None    \n",
    "    \n",
    "    if model_name == \"CLIP-CE\":\n",
    "        args.model_type = \"CLIP\"\n",
    "        args.repr = \"EOS\"\n",
    "        args.loss = \"CE\"\n",
    "    elif model_name == \"CLIPITM-CE\":\n",
    "        args.model_type = \"CLIPITM\"\n",
    "        args.loss = \"CE\"\n",
    "    elif model_name == \"BLIP2-CE\":\n",
    "        args.model_type = \"BLIP2\"\n",
    "        args.loss = \"CE\"\n",
    "    else:\n",
    "        args.model_type = \"BLIP2\"\n",
    "        args.loss = \"CE+SupCon\"\n",
    "\n",
    "    model = load_clipstyle_models(args)\n",
    "    \n",
    "    print(f\"model_name:{model_name}-param:{count_trainable_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1101875-ed1a-467b-b893-d5d9e5e277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "169632695"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
