{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Overview\n",
    "# Extracts text, vision, and multimodal embeddings from CLIP, CLIP-ITM, and BLIP2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5070125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python version: 3.10\n",
    "Description: Performs Multimodal Authorship Attribution using CLIP training strategy\n",
    "\"\"\"\n",
    "\n",
    "# %% Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, ViTImageProcessor\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from utilities import map_images_with_text_for_clip_model, map_images_with_text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ece35-b570-482f-9995-7f0e63f03dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress TorchDynamo errors and fall back to eager execution\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0d564-f92d-4a97-bae6-39c9e6fef6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.logged_entry_name = \"multimodal-latent-fusion-seed:1111\"\n",
    "        self.data_dir = '/workspace/persistent/HTClipper/data/processed'\n",
    "        self.image_dir = \"/workspace/persistent/HTClipper/data/IMAGES\"\n",
    "        self.save_dir = os.path.join(os.getcwd(), \"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/\")\n",
    "        self.model_dir_name = None\n",
    "        self.pairing_mode = \"non-associated\"\n",
    "        self.model_type = \"BLIP2\"  # Can be \"CLIP\", \"BLIP2\", or \"CLIPITM\"\n",
    "        self.batch_size = 32\n",
    "        self.nb_epochs = 40\n",
    "        self.patience = 3\n",
    "        self.nb_negatives = 1\n",
    "        self.seed = 1111\n",
    "        self.warmup_steps = 0\n",
    "        self.grad_steps = 1\n",
    "        self.learning_rate = 6e-4\n",
    "        self.train_data_percentage = 1.0\n",
    "        self.adam_epsilon = 1e-6\n",
    "        self.min_delta_change = 0.01\n",
    "        self.weight_decay = 0.01\n",
    "        self.augment_data = False\n",
    "        self.nb_augmented_samples = 1\n",
    "        self.loss = 'NTXENT'\n",
    "        self.temp = 0.5\n",
    "\n",
    "# Instantiate the arguments\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6386e1a-dd3f-4bc0-9c53-3e373ebbc2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, image_processor, num_negatives=5, pairing_mode='associated', city=None):\n",
    "        self.df = df\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.num_negatives = num_negatives\n",
    "        self.pairing_mode = pairing_mode\n",
    "        self.city = city  # City is passed explicitly instead of relying on a 'region' column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select a positive text-image pair\n",
    "        pos_row = self.df.iloc[idx]\n",
    "        pos_text = pos_row['TEXT']\n",
    "        pos_image_path = pos_row['IMAGES']\n",
    "        label = pos_row['VENDOR']  # Assuming 'VENDOR' is the label column\n",
    "\n",
    "        # Tokenize the text and process the positive image\n",
    "        pos_text_inputs = self.text_tokenizer(pos_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        pos_image = self._load_image(pos_image_path)\n",
    "\n",
    "        # Sample negatives (based on pairing_mode)\n",
    "        if self.pairing_mode == 'associated':\n",
    "            neg_texts, neg_image_paths = self._get_associated_negative_samples(pos_row['region'])\n",
    "        elif self.pairing_mode == 'non-associated':\n",
    "            neg_texts, neg_image_paths = self._get_non_associated_negative_samples(pos_row['region'])\n",
    "        else:\n",
    "            raise ValueError(\"pairing_mode must be either 'associated' or 'non-associated'\")\n",
    "\n",
    "        neg_text_inputs = [self.text_tokenizer(neg_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512) for neg_text in neg_texts]\n",
    "        neg_images = [self._load_image(neg_image_path) for neg_image_path in neg_image_paths]\n",
    "\n",
    "        neg_input_ids = torch.stack([neg_text['input_ids'].squeeze(0) for neg_text in neg_text_inputs])\n",
    "        neg_attention_mask = torch.stack([neg_text['attention_mask'].squeeze(0) for neg_text in neg_text_inputs])\n",
    "        neg_pixel_values = torch.stack(neg_images)\n",
    "\n",
    "        return {\n",
    "            'pos_input_ids': pos_text_inputs['input_ids'].squeeze(0),\n",
    "            'pos_attention_mask': pos_text_inputs['attention_mask'].squeeze(0),\n",
    "            'pos_pixel_values': pos_image,\n",
    "            'neg_input_ids': neg_input_ids,\n",
    "            'neg_attention_mask': neg_attention_mask,\n",
    "            'neg_pixel_values': neg_pixel_values,\n",
    "            'label': torch.tensor(label, dtype=torch.long)  # Add the label here\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_associated_negative_samples(self):\n",
    "        # Sample negatives from another part of the dataset (can implement any logic for this)\n",
    "        neg_indices = torch.randint(0, len(self.df), (self.num_negatives,))\n",
    "        neg_texts = self.df.iloc[neg_indices]['TEXT'].values\n",
    "        neg_image_paths = self.df.iloc[neg_indices]['IMAGES'].values\n",
    "        return neg_texts, neg_image_paths\n",
    "\n",
    "    def _get_non_associated_negative_samples(self):\n",
    "        # Sample non-associated negatives from the dataset\n",
    "        neg_indices = torch.randint(0, len(self.df), (self.num_negatives,))\n",
    "        neg_texts = self.df.iloc[neg_indices]['TEXT'].values\n",
    "        neg_image_paths = self.df.iloc[neg_indices]['IMAGES'].values\n",
    "        return neg_texts, neg_image_paths\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "        image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        return image\n",
    "\n",
    "\n",
    "class BLIP2Dataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, t5_tokenizer, image_processor, num_negatives=5, pairing_mode='non-associated'):\n",
    "        self.df = df\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.num_negatives = num_negatives\n",
    "        self.pairing_mode = pairing_mode\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "\n",
    "        # Assume that 'region' is a column in your DataFrame indicating the region of the text-image pair\n",
    "        self.region_groups = df.groupby('region')\n",
    "\n",
    "        # Define a transform to resize the image to 224x224\n",
    "        # self.image_transform = transforms.Compose([\n",
    "        #    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
    "        #    transforms.ToTensor(),  # Convert to tensor\n",
    "        #])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select a positive text-image pair\n",
    "        pos_row = self.df.iloc[idx]\n",
    "        full_text = pos_row['TEXT']\n",
    "        pos_image_path = pos_row['IMAGES']\n",
    "        pos_region = pos_row['region']\n",
    "\n",
    "        # Split the text on [SEP] for text generation\n",
    "        if '[SEP]' in full_text:\n",
    "            conditional_text, target_text = full_text.split('[SEP]', 1)\n",
    "            conditional_text = conditional_text.strip()\n",
    "            target_text = target_text.strip()\n",
    "        else:\n",
    "            # If [SEP] is not present, handle accordingly\n",
    "            conditional_text = ''\n",
    "            target_text = full_text.strip()\n",
    "\n",
    "        # Tokenize the full text for CLIP and ITM losses\n",
    "        pos_text_inputs = self.text_tokenizer(\n",
    "            full_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Tokenize the conditional text (text prompt) for text generation\n",
    "        conditional_text_inputs = self.t5_tokenizer(\n",
    "            conditional_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Tokenize the target text for text generation\n",
    "        target_text_inputs = self.t5_tokenizer(\n",
    "            target_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Process the positive image\n",
    "        pos_image = self._load_image(pos_image_path)\n",
    "\n",
    "        # Sample negatives (based on pairing_mode)\n",
    "        if self.pairing_mode == 'associated':\n",
    "            neg_texts, neg_image_paths = self._get_associated_negative_samples(pos_region)\n",
    "        elif self.pairing_mode == 'non-associated':\n",
    "            neg_texts, neg_image_paths = self._get_non_associated_negative_samples(pos_region)\n",
    "        else:\n",
    "            raise ValueError(\"pairing_mode must be either 'associated' or 'non-associated'\")\n",
    "\n",
    "        # Tokenize negative texts\n",
    "        neg_text_inputs = [self.text_tokenizer(\n",
    "            neg_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512\n",
    "        ) for neg_text in neg_texts]\n",
    "\n",
    "        # Load negative images\n",
    "        neg_images = [self._load_image(neg_image_path) for neg_image_path in neg_image_paths]\n",
    "\n",
    "        # Prepare negative inputs\n",
    "        neg_input_ids = torch.stack([neg_text['input_ids'].squeeze(0) for neg_text in neg_text_inputs])\n",
    "        neg_attention_mask = torch.stack([neg_text['attention_mask'].squeeze(0) for neg_text in neg_text_inputs])\n",
    "        neg_pixel_values = torch.stack(neg_images)\n",
    "\n",
    "        return {\n",
    "            # For CLIP and ITM losses (full text)\n",
    "            'pos_input_ids': pos_text_inputs['input_ids'].squeeze(0),\n",
    "            'pos_attention_mask': pos_text_inputs['attention_mask'].squeeze(0),\n",
    "            # For text generation loss (conditional text and target text)\n",
    "            'conditional_input_ids': conditional_text_inputs['input_ids'].squeeze(0),\n",
    "            'conditional_attention_mask': conditional_text_inputs['attention_mask'].squeeze(0),\n",
    "            'target_input_ids': target_text_inputs['input_ids'].squeeze(0),\n",
    "            'target_attention_mask': target_text_inputs['attention_mask'].squeeze(0),\n",
    "            # Positive image\n",
    "            'pos_pixel_values': pos_image,\n",
    "            # Negative samples\n",
    "            'neg_input_ids': neg_input_ids,\n",
    "            'neg_attention_mask': neg_attention_mask,\n",
    "            'neg_pixel_values': neg_pixel_values,\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_associated_negative_samples(self, pos_region):\n",
    "        # Sample negatives from another region (associated text-image pairs)\n",
    "        other_regions = [region for region in self.region_groups.groups.keys() if region != pos_region]\n",
    "        neg_region = random.choice(other_regions)\n",
    "        neg_df = self.region_groups.get_group(neg_region)\n",
    "\n",
    "        neg_text_indices = torch.randint(0, len(neg_df), (self.num_negatives,))\n",
    "        neg_texts = neg_df.iloc[neg_text_indices]['TEXT'].values\n",
    "        neg_image_paths = neg_df.iloc[neg_text_indices]['IMAGES'].values\n",
    "\n",
    "        return neg_texts, neg_image_paths\n",
    "\n",
    "    def _get_non_associated_negative_samples(self, pos_region):\n",
    "        # Sample negatives from another region (non-associated text-image pairs)\n",
    "        other_regions = [region for region in self.region_groups.groups.keys() if region != pos_region]\n",
    "        neg_region = random.choice(other_regions)\n",
    "        neg_df = self.region_groups.get_group(neg_region)\n",
    "\n",
    "        neg_text_indices = torch.randint(0, len(neg_df), (self.num_negatives,))\n",
    "        neg_image_indices = torch.randint(0, len(neg_df), (self.num_negatives,))\n",
    "\n",
    "        neg_texts = neg_df.iloc[neg_text_indices]['TEXT'].values\n",
    "        neg_image_paths = neg_df.iloc[neg_image_indices]['IMAGES'].values\n",
    "\n",
    "        return neg_texts, neg_image_paths\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "        # Resize image to 224x224\n",
    "        # image = self.image_transform(image)\n",
    "        image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b3806-b896-4276-99f6-e87650332950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting seed value for reproducibility    \n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "# Set TOKENIZERS_PARALLELISM to false to disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Set matrix multiplication precision\n",
    "# This setting offers a balance between precision and performance. Itâ€™s typically a good starting point for mixed precision training\n",
    "#  with FP16.\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert args.loss in [\"NTXENT\"]\n",
    "assert args.pairing_mode in [\"associated\", \"non-associated\"]\n",
    "assert args.model_type in [\"CLIP\", \"CLIPITM\", \"BLIP2\"]\n",
    "\n",
    "num_training_steps = 678\n",
    "# Setting the warmup steps to 1/10th the size of training data\n",
    "warmup_steps = 1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88509dd3-bddf-480c-bf9d-0cc7c6ee9eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoModel, ViTModel, get_linear_schedule_with_warmup\n",
    "\n",
    "class CLIPModel(pl.LightningModule):\n",
    "    def __init__(self, weight_decay, eps, warmup_steps, num_training_steps, text_model_name='johngiorgi/declutr-small', \n",
    "                image_model_name='google/vit-base-patch16-224', learning_rate=0.00001, num_negatives=5, temperature=0.5):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negatives = num_negatives\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "        # Store outputs for validation and testing\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Get text embeddings\n",
    "        \"\"\"\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Get text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        eos_mask = input_ids.eq(self.text_model.config.eos_token_id)\n",
    "        eos_indices = eos_mask.nonzero(as_tuple=False)\n",
    "        text_embeddings = text_outputs.last_hidden_state[eos_indices[:, 0], eos_indices[:, 1]]\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "\n",
    "\n",
    "        # Get image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "        \n",
    "        return text_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings):\n",
    "        # Normalize embeddings\n",
    "        pos_text_embeddings = F.normalize(pos_text_embeddings, p=2, dim=1)\n",
    "        pos_image_embeddings = F.normalize(pos_image_embeddings, p=2, dim=1)\n",
    "        neg_text_embeddings = F.normalize(neg_text_embeddings, p=2, dim=2)  # Normalized over the last dimension\n",
    "        neg_image_embeddings = F.normalize(neg_image_embeddings, p=2, dim=2)  # Normalized over the last dimension\n",
    "\n",
    "        # Positive pairs similarity\n",
    "        pos_sim = torch.exp(torch.sum(pos_text_embeddings * pos_image_embeddings, dim=-1) / self.temperature)\n",
    "        \n",
    "        # Negative pairs similarity (text to image)\n",
    "        neg_sim_text_image = torch.exp(torch.einsum('bij,bj->bi', neg_text_embeddings, pos_image_embeddings) / self.temperature)\n",
    "        # Negative pairs similarity (image to text)\n",
    "        neg_sim_image_text = torch.exp(torch.einsum('bi,bkj->bk', pos_text_embeddings, neg_image_embeddings) / self.temperature)\n",
    "\n",
    "        # Calculate the loss for text-to-image\n",
    "        denominator_text_image = pos_sim + neg_sim_text_image.sum(dim=1)\n",
    "        loss_text_image = -torch.log(pos_sim / denominator_text_image)\n",
    "\n",
    "        # Calculate the loss for image-to-text\n",
    "        denominator_image_text = pos_sim.unsqueeze(1) + neg_sim_image_text\n",
    "        loss_image_text = -torch.log(pos_sim.unsqueeze(1) / denominator_image_text).sum(dim=1)\n",
    "\n",
    "        # Combine both losses\n",
    "        loss = (loss_text_image + loss_image_text).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "\n",
    "        self.test_outputs.append(loss)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if self.test_outputs:\n",
    "            avg_loss = torch.stack(self.test_outputs).mean()\n",
    "            self.log('avg_test_loss', avg_loss)\n",
    "        self.test_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def get_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='text'):\n",
    "        \"\"\"\n",
    "    Generate text, image, or multimodal embeddings for inference.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor, optional): Tokenized input text IDs of shape [batch_size, seq_len].\n",
    "        attention_mask (torch.Tensor, optional): Attention mask for the input text of shape [batch_size, seq_len].\n",
    "        pixel_values (torch.Tensor, optional): Preprocessed image tensor of shape [batch_size, channels, height, width].\n",
    "        embedding_type (str): Specify 'text', 'image', or 'multimodal' to generate the respective embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized embeddings.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "\n",
    "            # Get text embeddings\n",
    "            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the [EOS] token embedding or the last token embedding\n",
    "            if self.text_model.config.eos_token_id is not None:\n",
    "                eos_mask = input_ids.eq(self.text_model.config.eos_token_id)\n",
    "                if torch.any(eos_mask):\n",
    "                    eos_indices = eos_mask.nonzero(as_tuple=False)\n",
    "                    text_embeddings = text_outputs.last_hidden_state[eos_indices[:, 0], eos_indices[:, 1]]\n",
    "                else:\n",
    "                    # If no [EOS] token is found, use the last hidden state\n",
    "                    text_embeddings = text_outputs.last_hidden_state[:, -1, :]\n",
    "            else:\n",
    "                # If eos_token_id is not defined, use the last hidden state\n",
    "                text_embeddings = text_outputs.last_hidden_state[:, -1, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "\n",
    "            # Get image embeddings\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "            image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "            return image_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "\n",
    "            # Get text embeddings\n",
    "            text_embeddings = self.get_embeddings(input_ids=input_ids, attention_mask=attention_mask, embedding_type='text')\n",
    "\n",
    "            # Get image embeddings\n",
    "            image_embeddings = self.get_embeddings(pixel_values=pixel_values, embedding_type='image')\n",
    "\n",
    "            # Compute multimodal embeddings by averaging\n",
    "            multimodal_embeddings = (text_embeddings + image_embeddings) / 2\n",
    "            multimodal_embeddings = F.normalize(multimodal_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "            return multimodal_embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a5158e-2641-4ad2-b57d-7c9a3b46fca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoModel, ViTModel, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_queries=32,\n",
    "        d_model=768,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=12,  # Set to 6 as per CLIPITMModel initialization\n",
    "        intermediate_size=3072,\n",
    "        cross_attention_frequency=1,  # Set to 1 to apply cross-attention in every layer\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(QFormer, self).__init__()\n",
    "\n",
    "        # Learnable query embeddings (similar to Blip-2's Q-Former)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(1, num_queries, d_model))\n",
    "\n",
    "        # Configuration for a Transformer with cross-attention\n",
    "        config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "            is_decoder=True,  # Enable cross-attention by setting is_decoder to True\n",
    "            add_cross_attention=True,\n",
    "        )\n",
    "\n",
    "        # Initialize BertModel with the above configuration\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Cross-attention frequency\n",
    "        self.cross_attention_frequency = cross_attention_frequency\n",
    "\n",
    "    def forward(self, image_embeddings):\n",
    "        \"\"\"\n",
    "        image_embeddings: The output from the vision model (e.g., ViT).\n",
    "        image_embeddings shape: (batch_size, seq_len_image, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_image, d_model = image_embeddings.shape\n",
    "\n",
    "        # Expand query embeddings to match the batch size\n",
    "        query_embeddings = self.query_embeddings.expand(batch_size, -1, -1)  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Initial query_embeddings shape: {query_embeddings.shape}\")  # Expected: [40, 32, 768]\n",
    "        # print(f\"Image embeddings shape: {image_embeddings.shape}\")  # Expected: [40, 197, 768]\n",
    "\n",
    "        # Create attention masks\n",
    "        # Self-attention mask for queries (decoder input)\n",
    "        attention_mask = torch.ones(batch_size, query_embeddings.size(1), device=query_embeddings.device)  # [batch_size, num_queries]\n",
    "        # print(f\"Attention mask shape: {attention_mask.shape}\")  # Expected: [40, 32]\n",
    "\n",
    "        # Encoder attention mask for image embeddings (encoder input)\n",
    "        encoder_attention_mask = torch.ones(batch_size, seq_len_image, device=image_embeddings.device)  # [batch_size, seq_len_image]\n",
    "        # print(f\"Encoder attention mask shape: {encoder_attention_mask.shape}\")  # Expected: [40, 197]\n",
    "\n",
    "        # Get extended attention masks using BERT's utility function\n",
    "        extended_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            attention_mask, attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask shape: {extended_attention_mask.shape}\")  # Expected: [40, 1, 1, 32]\n",
    "\n",
    "        # Repeat the self-attention mask for each attention head\n",
    "        extended_attention_mask = extended_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask after repeat: {extended_attention_mask.shape}\")  # Expected: [40, 12, 1, 32]\n",
    "\n",
    "        # Create cross-attention mask: [batch_size, num_queries, seq_len_image]\n",
    "        cross_attention_mask = torch.ones(batch_size, query_embeddings.size(1), seq_len_image, device=image_embeddings.device)  # [40, 32, 197]\n",
    "        # print(f\"Cross-Attention mask shape: {cross_attention_mask.shape}\")  # Expected: [40, 32, 197]\n",
    "\n",
    "        # Get extended cross-attention mask\n",
    "        encoder_extended_cross_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            cross_attention_mask, cross_attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape before repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 1, 32, 197]\n",
    "\n",
    "        # Repeat the cross-attention mask for each attention head\n",
    "        encoder_extended_cross_attention_mask = encoder_extended_cross_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape after repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 12, 32, 197]\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden_states = query_embeddings  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Hidden states shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        # Iterate through each BERT layer\n",
    "        for i, layer_module in enumerate(self.bert.encoder.layer):\n",
    "            if i % self.cross_attention_frequency == 0:\n",
    "                # print(f\"Layer {i}: Applying Cross-Attention between query_embeddings and image_embeddings.\")\n",
    "                # Apply cross-attention\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=image_embeddings,\n",
    "                    encoder_attention_mask=encoder_extended_cross_attention_mask,  # Cross-attention mask for image embeddings\n",
    "                )\n",
    "            else:\n",
    "                # print(f\"Layer {i}: Applying Self-Attention only on query_embeddings.\")\n",
    "                # Apply only self-attention by setting encoder_hidden_states=None\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=None,\n",
    "                    encoder_attention_mask=None,\n",
    "                )\n",
    "\n",
    "            # Update hidden states\n",
    "            hidden_states = outputs[0]  # [batch_size, num_queries, d_model]\n",
    "            # print(f\"After Layer {i}, query_embeddings shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        return hidden_states  # Final query embeddings\n",
    "        \n",
    "\n",
    "class CLIPITMModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_decay,\n",
    "        eps,\n",
    "        warmup_steps,\n",
    "        num_training_steps,\n",
    "        text_model_name='johngiorgi/declutr-small',\n",
    "        image_model_name='google/vit-base-patch16-224',\n",
    "        learning_rate=0.00001,\n",
    "        num_negatives=5,\n",
    "        temperature=0.5,\n",
    "        num_query_tokens=32,\n",
    "        qformer_hidden_size=768,\n",
    "        cross_attention_frequency=1,  # Set to 1 to align with CustomQFormer\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(CLIPITMModel, self).__init__()\n",
    "        # Text Model\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        # Vision Model\n",
    "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
    "        # Custom Q-Former with cross-attention in every layer\n",
    "        self.qformer = QFormer(\n",
    "            num_queries=num_query_tokens,\n",
    "            d_model=qformer_hidden_size,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=12,\n",
    "            cross_attention_frequency=cross_attention_frequency,\n",
    "        )\n",
    "\n",
    "        # Loss components\n",
    "        self.itm_criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negatives = num_negatives\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, neg_pixel_values=None):\n",
    "        # Process text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "        # print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "        # Process image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        # print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "\n",
    "        # Pass through the custom Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "        # print(f\"Query embeddings shape after QFormer: {query_embeddings.shape}\")\n",
    "\n",
    "        # Process negative image embeddings if provided\n",
    "        neg_image_embeddings = None\n",
    "        if neg_pixel_values is not None:\n",
    "            neg_image_embeddings = self._process_negative_images(neg_pixel_values)\n",
    "            # print(f\"Negative image embeddings shape: {neg_image_embeddings.shape}\")\n",
    "\n",
    "        return text_embeddings, query_embeddings, neg_image_embeddings\n",
    "\n",
    "    def _process_negative_images(self, neg_pixel_values):\n",
    "        batch_size, num_negatives, _, _, _ = neg_pixel_values.shape\n",
    "        neg_pixel_values = neg_pixel_values.view(-1, *neg_pixel_values.shape[2:])  # (batch_size * num_negatives, C, H, W)\n",
    "        neg_image_outputs = self.image_model(pixel_values=neg_pixel_values)\n",
    "        neg_image_embeddings = neg_image_outputs.last_hidden_state.mean(dim=1)  # (batch_size * num_negatives, d_model)\n",
    "        neg_image_embeddings = F.normalize(neg_image_embeddings, p=2, dim=-1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch_size, num_negatives, -1)  # (batch_size, num_negatives, d_model)\n",
    "        return neg_image_embeddings\n",
    "\n",
    "    def compute_clip_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute similarities\n",
    "        # print(f\"Positive text embeddings shape: {pos_text_embeddings.shape}\")\n",
    "        # print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "        # print(f\"Negative image embeddings shape: {neg_image_embeddings.shape}\")\n",
    "        \n",
    "        # Positive similarities between text and positive image queries\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_sim = pos_sim.max(dim=1, keepdim=True).values  # (batch_size, 1)\n",
    "\n",
    "        # Negative similarities between text and negative images\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "\n",
    "        # Combine logits\n",
    "        logits = torch.cat([pos_sim, neg_sim], dim=1)  # (batch_size, 1 + num_negatives)\n",
    "        # print(f\"Logits shape (for CLIP loss): {logits.shape}\")\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(logits.device)  # (batch_size,)\n",
    "\n",
    "        # Compute CLIP loss using cross-entropy\n",
    "        clip_loss = self.itm_criterion(logits, labels)\n",
    "        return clip_loss\n",
    "\n",
    "    def compute_itm_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute positive scores\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_scores = pos_sim.max(dim=1).values  # (batch_size,)\n",
    "\n",
    "        # Compute negative scores\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "        neg_scores = neg_sim.view(-1)  # (batch_size * num_negatives,)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "        labels = torch.cat([\n",
    "            torch.ones(pos_scores.size(0), device=scores.device),\n",
    "            torch.zeros(neg_scores.size(0), device=scores.device)\n",
    "        ], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "        \n",
    "        # print(f\"Scores shape: {scores.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Compute binary cross-entropy loss with logits\n",
    "        itm_loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "        return itm_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings = self(\n",
    "            batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'], neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss\n",
    "        # print(f\"Training Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('train_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings = self(\n",
    "            batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'], neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss\n",
    "        # print(f\"Validation Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('val_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings = self(\n",
    "            batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'], neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss\n",
    "        # print(f\"Validation Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('test_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps\n",
    "        )\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if 'val_loss' in self.trainer.callback_metrics:\n",
    "            avg_val_loss = self.trainer.callback_metrics['val_loss'].mean()\n",
    "            # print(f\"[Validation End] Average validation loss: {avg_val_loss}\")\n",
    "            self.log('avg_val_loss', avg_val_loss)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if 'test_loss' in self.trainer.callback_metrics:\n",
    "            avg_test_loss = self.trainer.callback_metrics['test_loss'].mean()\n",
    "            # print(f\"[Test End] Average test loss: {avg_test_loss}\")\n",
    "            self.log('avg_test_loss', avg_test_loss)\n",
    "            \n",
    "    def get_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings for inference.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Tokenized input text IDs.\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for the input text.\n",
    "            pixel_values (torch.Tensor, optional): Preprocessed image tensor.\n",
    "            embedding_type (str): 'text', 'image', or 'multimodal'.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The requested embeddings.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "            return query_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            # Combine embeddings (e.g., concatenate)\n",
    "            \n",
    "            multimodal_embeddings = (text_embeddings + query_embeddings)/2\n",
    "            multimodal_embeddings = F.normalize(multimodal_embeddings, p=2, dim=-1)\n",
    "            return multimodal_embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7193d62-7d45-4c97-bd46-e2a1ee614bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from transformers import AutoModel, ViTModel, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_queries=32,\n",
    "        d_model=768,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=12,  # Set to 6 as per CLIPITMModel initialization\n",
    "        intermediate_size=3072,\n",
    "        cross_attention_frequency=1,  # Set to 1 to apply cross-attention in every layer\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(QFormer, self).__init__()\n",
    "\n",
    "        # Learnable query embeddings (similar to Blip-2's Q-Former)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(1, num_queries, d_model))\n",
    "\n",
    "        # Configuration for a Transformer with cross-attention\n",
    "        config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "            is_decoder=True,  # Enable cross-attention by setting is_decoder to True\n",
    "            add_cross_attention=True,\n",
    "        )\n",
    "\n",
    "        # Initialize BertModel with the above configuration\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Cross-attention frequency\n",
    "        self.cross_attention_frequency = cross_attention_frequency\n",
    "\n",
    "    def forward(self, image_embeddings):\n",
    "        \"\"\"\n",
    "        image_embeddings: The output from the vision model (e.g., ViT).\n",
    "        image_embeddings shape: (batch_size, seq_len_image, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_image, d_model = image_embeddings.shape\n",
    "\n",
    "        # Expand query embeddings to match the batch size\n",
    "        query_embeddings = self.query_embeddings.expand(batch_size, -1, -1)  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Initial query_embeddings shape: {query_embeddings.shape}\")  # Expected: [40, 32, 768]\n",
    "        # print(f\"Image embeddings shape: {image_embeddings.shape}\")  # Expected: [40, 197, 768]\n",
    "\n",
    "        # Create attention masks\n",
    "        # Self-attention mask for queries (decoder input)\n",
    "        attention_mask = torch.ones(batch_size, query_embeddings.size(1), device=query_embeddings.device)  # [batch_size, num_queries]\n",
    "        # print(f\"Attention mask shape: {attention_mask.shape}\")  # Expected: [40, 32]\n",
    "\n",
    "        # Encoder attention mask for image embeddings (encoder input)\n",
    "        encoder_attention_mask = torch.ones(batch_size, seq_len_image, device=image_embeddings.device)  # [batch_size, seq_len_image]\n",
    "        # print(f\"Encoder attention mask shape: {encoder_attention_mask.shape}\")  # Expected: [40, 197]\n",
    "\n",
    "        # Get extended attention masks using BERT's utility function\n",
    "        extended_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            attention_mask, attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask shape: {extended_attention_mask.shape}\")  # Expected: [40, 1, 1, 32]\n",
    "\n",
    "        # Repeat the self-attention mask for each attention head\n",
    "        extended_attention_mask = extended_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask after repeat: {extended_attention_mask.shape}\")  # Expected: [40, 12, 1, 32]\n",
    "\n",
    "        # Create cross-attention mask: [batch_size, num_queries, seq_len_image]\n",
    "        cross_attention_mask = torch.ones(batch_size, query_embeddings.size(1), seq_len_image, device=image_embeddings.device)  # [40, 32, 197]\n",
    "        # print(f\"Cross-Attention mask shape: {cross_attention_mask.shape}\")  # Expected: [40, 32, 197]\n",
    "\n",
    "        # Get extended cross-attention mask\n",
    "        encoder_extended_cross_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            cross_attention_mask, cross_attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape before repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 1, 32, 197]\n",
    "\n",
    "        # Repeat the cross-attention mask for each attention head\n",
    "        encoder_extended_cross_attention_mask = encoder_extended_cross_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape after repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 12, 32, 197]\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden_states = query_embeddings  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Hidden states shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        # Iterate through each BERT layer\n",
    "        for i, layer_module in enumerate(self.bert.encoder.layer):\n",
    "            if i % self.cross_attention_frequency == 0:\n",
    "                # print(f\"Layer {i}: Applying Cross-Attention between query_embeddings and image_embeddings.\")\n",
    "                # Apply cross-attention\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=image_embeddings,\n",
    "                    encoder_attention_mask=encoder_extended_cross_attention_mask,  # Cross-attention mask for image embeddings\n",
    "                )\n",
    "            else:\n",
    "                # print(f\"Layer {i}: Applying Self-Attention only on query_embeddings.\")\n",
    "                # Apply only self-attention by setting encoder_hidden_states=None\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=None,\n",
    "                    encoder_attention_mask=None,\n",
    "                )\n",
    "\n",
    "            # Update hidden states\n",
    "            hidden_states = outputs[0]  # [batch_size, num_queries, d_model]\n",
    "            # print(f\"After Layer {i}, query_embeddings shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        return hidden_states  # Final query embeddings\n",
    "\n",
    "class BLIP2Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_decay,\n",
    "        eps,\n",
    "        warmup_steps,\n",
    "        num_training_steps,\n",
    "        text_model_name='johngiorgi/declutr-small',\n",
    "        image_model_name='google/vit-base-patch16-224',\n",
    "        t5_model_name='google/flan-t5-small',\n",
    "        learning_rate=0.00001,\n",
    "        num_negatives=5,\n",
    "        temperature=0.5,\n",
    "        num_query_tokens=32,\n",
    "        qformer_hidden_size=768,\n",
    "        cross_attention_frequency=1,  # Set to 1 to align with CustomQFormer\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(BLIP2Model, self).__init__()\n",
    "        # Text Model\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        # Vision Model\n",
    "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
    "        # Custom Q-Former with cross-attention in every layer\n",
    "        self.qformer = QFormer(\n",
    "            num_queries=num_query_tokens,\n",
    "            d_model=qformer_hidden_size,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=12,\n",
    "            cross_attention_frequency=cross_attention_frequency,\n",
    "        )\n",
    "\n",
    "        # Loss components\n",
    "        self.itm_criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negatives = num_negatives\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "        # Initialize T5 model\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "        # Projects from 768 (Q-Former hidden size) to 512 (T5 hidden size).\n",
    "        self.query_proj_t5 = nn.Linear(qformer_hidden_size, self.t5_model.config.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, pos_input_ids_t5, pos_attention_mask_t5, neg_pixel_values=None):\n",
    "        # Process text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "        # print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "        # Process image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        # print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "\n",
    "        # Pass through the custom Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "        # print(f\"Query embeddings shape after QFormer: {query_embeddings.shape}\")\n",
    "\n",
    "        # Process negative image embeddings if provided\n",
    "        neg_image_embeddings = None\n",
    "        if neg_pixel_values is not None:\n",
    "            neg_image_embeddings = self._process_negative_images(neg_pixel_values)\n",
    "            # print(f\"Negative image embeddings shape: {neg_image_embeddings.shape}\")\n",
    "\n",
    "        # Use query_embeddings as encoder outputs for T5\n",
    "        # T5 expects encoder outputs in the shape (batch_size, seq_len, hidden_size)\n",
    "        # Our query_embeddings are already in this shape (batch_size, num_queries, hidden_size)\n",
    "\n",
    "        # Project unnormalized query_embeddings to match T5 hidden size\n",
    "        projected_query_embeddings = self.query_proj_t5(query_embeddings)\n",
    "        # Create T5 encoder outputs\n",
    "        encoder_outputs = BaseModelOutput(last_hidden_state=projected_query_embeddings)\n",
    "        # Create encoder attention mask\n",
    "        encoder_attention_mask = torch.ones(\n",
    "            projected_query_embeddings.size()[:-1],\n",
    "            dtype=torch.long,\n",
    "            device=projected_query_embeddings.device\n",
    "            )\n",
    "        # Pass through T5 model\n",
    "        t5_outputs = self.t5_model(\n",
    "            attention_mask=encoder_attention_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            labels=pos_input_ids_t5,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        text_generation_loss = t5_outputs.loss\n",
    "\n",
    "        return text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss\n",
    "\n",
    "    def _process_negative_images(self, neg_pixel_values):\n",
    "        batch_size, num_negatives, _, _, _ = neg_pixel_values.shape\n",
    "        neg_pixel_values = neg_pixel_values.view(-1, *neg_pixel_values.shape[2:])  # (batch_size * num_negatives, C, H, W)\n",
    "        neg_image_outputs = self.image_model(pixel_values=neg_pixel_values)\n",
    "        neg_image_embeddings = neg_image_outputs.last_hidden_state.mean(dim=1)  # (batch_size * num_negatives, d_model)\n",
    "        neg_image_embeddings = F.normalize(neg_image_embeddings, p=2, dim=-1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch_size, num_negatives, -1)  # (batch_size, num_negatives, d_model)\n",
    "        return neg_image_embeddings\n",
    "\n",
    "    def compute_clip_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute similarities\n",
    "        # print(f\"Positive text embeddings shape: {pos_text_embeddings.shape}\")\n",
    "        # print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "        # print(f\"Negative image embeddings shape: {neg_image_embeddings.shape}\")\n",
    "\n",
    "        # Positive similarities between text and positive image queries\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_sim = pos_sim.max(dim=1, keepdim=True).values  # (batch_size, 1)\n",
    "\n",
    "        # Negative similarities between text and negative images\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "\n",
    "        # Combine logits\n",
    "        logits = torch.cat([pos_sim, neg_sim], dim=1)  # (batch_size, 1 + num_negatives)\n",
    "        # print(f\"Logits shape (for CLIP loss): {logits.shape}\")\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(logits.device)  # (batch_size,)\n",
    "\n",
    "        # Compute CLIP loss using cross-entropy\n",
    "        clip_loss = self.itm_criterion(logits, labels)\n",
    "        return clip_loss\n",
    "\n",
    "    def compute_itm_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute positive scores\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_scores = pos_sim.max(dim=1).values  # (batch_size,)\n",
    "\n",
    "        # Compute negative scores\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "        neg_scores = neg_sim.view(-1)  # (batch_size * num_negatives,)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "        labels = torch.cat([\n",
    "            torch.ones(pos_scores.size(0), device=scores.device),\n",
    "            torch.zeros(neg_scores.size(0), device=scores.device)\n",
    "        ], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "\n",
    "        # print(f\"Scores shape: {scores.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Compute binary cross-entropy loss with logits\n",
    "        itm_loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "        return itm_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            batch['pos_input_ids'],\n",
    "            batch['pos_attention_mask'],\n",
    "            batch['pos_pixel_values'],\n",
    "            batch['pos_input_ids_t5'],\n",
    "            batch['pos_attention_mask_t5'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "            )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        # print(f\"Training Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('clip_loss', clip_loss)\n",
    "        self.log('itm_loss', itm_loss)\n",
    "        self.log('text_generation_loss', text_generation_loss)\n",
    "        self.log('train_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            batch['pos_input_ids'],\n",
    "            batch['pos_attention_mask'],\n",
    "            batch['pos_pixel_values'],\n",
    "            batch['pos_input_ids_t5'],\n",
    "            batch['pos_attention_mask_t5'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "            )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        # print(f\"Training Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('clip_loss', clip_loss)\n",
    "        self.log('itm_loss', itm_loss)\n",
    "        self.log('text_generation_loss', text_generation_loss)\n",
    "        self.log('val_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            batch['pos_input_ids'],\n",
    "            batch['pos_attention_mask'],\n",
    "            batch['pos_pixel_values'],\n",
    "            batch['pos_input_ids_t5'],\n",
    "            batch['pos_attention_mask_t5'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "            )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        total_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        # print(f\"Training Step: CLIP Loss: {clip_loss.item()}, ITM Loss: {itm_loss.item()}, Total Loss: {total_loss.item()}\")\n",
    "        self.log('clip_loss', clip_loss)\n",
    "        self.log('itm_loss', itm_loss)\n",
    "        self.log('text_generation_loss', text_generation_loss)\n",
    "        self.log('test_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps\n",
    "        )\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if 'val_loss' in self.trainer.callback_metrics:\n",
    "            avg_val_loss = self.trainer.callback_metrics['val_loss'].mean()\n",
    "            # print(f\"[Validation End] Average validation loss: {avg_val_loss}\")\n",
    "            self.log('avg_val_loss', avg_val_loss)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if 'test_loss' in self.trainer.callback_metrics:\n",
    "            avg_test_loss = self.trainer.callback_metrics['test_loss'].mean()\n",
    "            # print(f\"[Test End] Average test loss: {avg_test_loss}\")\n",
    "            self.log('avg_test_loss', avg_test_loss)\n",
    "\n",
    "\n",
    "    def get_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings for inference.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Tokenized input text IDs.\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for the input text.\n",
    "            pixel_values (torch.Tensor, optional): Preprocessed image tensor.\n",
    "            embedding_type (str): 'text', 'image', or 'multimodal'.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The requested embeddings.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "            return query_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            # Combine embeddings (e.g., concatenate)\n",
    "            \n",
    "            multimodal_embeddings = (text_embeddings + query_embeddings)/2\n",
    "            multimodal_embeddings = F.normalize(multimodal_embeddings, p=2, dim=-1)\n",
    "            return multimodal_embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e276f27-8530-4c75-801f-5c95432a376c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel, ViTModel, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "# import bitsandbytes as bnb\n",
    "# from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_queries=32,\n",
    "        d_model=768,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=12,  # Set to 6 as per CLIPITMModel initialization\n",
    "        intermediate_size=3072,\n",
    "        cross_attention_frequency=1,  # Set to 1 to apply cross-attention in every layer\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(QFormer, self).__init__()\n",
    "\n",
    "        # Learnable query embeddings (similar to Blip-2's Q-Former)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(1, num_queries, d_model))\n",
    "\n",
    "        # Configuration for a Transformer with cross-attention\n",
    "        config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "            is_decoder=True,  # Enable cross-attention by setting is_decoder to True\n",
    "            add_cross_attention=True,\n",
    "        )\n",
    "\n",
    "        # Initialize BertModel with the above configuration\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Cross-attention frequency\n",
    "        self.cross_attention_frequency = cross_attention_frequency\n",
    "\n",
    "    def forward(self, image_embeddings):\n",
    "        \"\"\"\n",
    "        image_embeddings: The output from the vision model (e.g., ViT).\n",
    "        image_embeddings shape: (batch_size, seq_len_image, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_image, d_model = image_embeddings.shape\n",
    "\n",
    "        # Expand query embeddings to match the batch size\n",
    "        query_embeddings = self.query_embeddings.expand(batch_size, -1, -1)  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Initial query_embeddings shape: {query_embeddings.shape}\")  # Expected: [40, 32, 768]\n",
    "        # print(f\"Image embeddings shape: {image_embeddings.shape}\")  # Expected: [40, 197, 768]\n",
    "\n",
    "        # Create attention masks\n",
    "        # Self-attention mask for queries (decoder input)\n",
    "        attention_mask = torch.ones(batch_size, query_embeddings.size(1), device=query_embeddings.device)  # [batch_size, num_queries]\n",
    "        # print(f\"Attention mask shape: {attention_mask.shape}\")  # Expected: [40, 32]\n",
    "\n",
    "        # Encoder attention mask for image embeddings (encoder input)\n",
    "        encoder_attention_mask = torch.ones(batch_size, seq_len_image, device=image_embeddings.device)  # [batch_size, seq_len_image]\n",
    "        # print(f\"Encoder attention mask shape: {encoder_attention_mask.shape}\")  # Expected: [40, 197]\n",
    "\n",
    "        # Get extended attention masks using BERT's utility function\n",
    "        extended_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            attention_mask, attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask shape: {extended_attention_mask.shape}\")  # Expected: [40, 1, 1, 32]\n",
    "\n",
    "        # Repeat the self-attention mask for each attention head\n",
    "        extended_attention_mask = extended_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, 1, num_queries]\n",
    "        # print(f\"Extended self-attention mask after repeat: {extended_attention_mask.shape}\")  # Expected: [40, 12, 1, 32]\n",
    "\n",
    "        # Create cross-attention mask: [batch_size, num_queries, seq_len_image]\n",
    "        cross_attention_mask = torch.ones(batch_size, query_embeddings.size(1), seq_len_image, device=image_embeddings.device)  # [40, 32, 197]\n",
    "        # print(f\"Cross-Attention mask shape: {cross_attention_mask.shape}\")  # Expected: [40, 32, 197]\n",
    "\n",
    "        # Get extended cross-attention mask\n",
    "        encoder_extended_cross_attention_mask = self.bert.get_extended_attention_mask(\n",
    "            cross_attention_mask, cross_attention_mask.shape, image_embeddings.device\n",
    "        )  # Shape: [batch_size, 1, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape before repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 1, 32, 197]\n",
    "\n",
    "        # Repeat the cross-attention mask for each attention head\n",
    "        encoder_extended_cross_attention_mask = encoder_extended_cross_attention_mask.repeat(1, self.bert.config.num_attention_heads, 1, 1)  # [batch_size, num_heads, num_queries, seq_len_image]\n",
    "        # print(f\"Encoder extended cross-attention mask shape after repeat: {encoder_extended_cross_attention_mask.shape}\")  # Expected: [40, 12, 32, 197]\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden_states = query_embeddings  # [batch_size, num_queries, d_model]\n",
    "        # print(f\"Hidden states shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        # Iterate through each BERT layer\n",
    "        for i, layer_module in enumerate(self.bert.encoder.layer):\n",
    "            if i % self.cross_attention_frequency == 0:\n",
    "                # print(f\"Layer {i}: Applying Cross-Attention between query_embeddings and image_embeddings.\")\n",
    "                # Apply cross-attention\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=image_embeddings,\n",
    "                    encoder_attention_mask=encoder_extended_cross_attention_mask,  # Cross-attention mask for image embeddings\n",
    "                )\n",
    "            else:\n",
    "                # print(f\"Layer {i}: Applying Self-Attention only on query_embeddings.\")\n",
    "                # Apply only self-attention by setting encoder_hidden_states=None\n",
    "                outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,  # Self-attention mask for queries\n",
    "                    encoder_hidden_states=None,\n",
    "                    encoder_attention_mask=None,\n",
    "                )\n",
    "\n",
    "            # Update hidden states\n",
    "            hidden_states = outputs[0]  # [batch_size, num_queries, d_model]\n",
    "            # print(f\"After Layer {i}, query_embeddings shape: {hidden_states.shape}\")  # Expected: [40, 32, 768]\n",
    "\n",
    "        return hidden_states  # Final query embeddings\n",
    "\n",
    "class BLIP2ConditionalModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_decay,\n",
    "        eps,\n",
    "        warmup_steps,\n",
    "        num_training_steps,\n",
    "        text_model_name='johngiorgi/declutr-small',\n",
    "        image_model_name='google/vit-base-patch16-224',\n",
    "        t5_model_name='google/flan-t5-small',\n",
    "        learning_rate=0.00001,\n",
    "        num_negatives=5,\n",
    "        temperature=0.5,\n",
    "        num_query_tokens=32,\n",
    "        qformer_hidden_size=768,\n",
    "        cross_attention_frequency=1,  # Set to 1 to align with CustomQFormer\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(BLIP2ConditionalModel, self).__init__()\n",
    "        # Text Model\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        # Vision Model\n",
    "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
    "        # Custom Q-Former with cross-attention in every layer\n",
    "        self.qformer = QFormer(\n",
    "            num_queries=num_query_tokens,\n",
    "            d_model=qformer_hidden_size,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=12,\n",
    "            cross_attention_frequency=cross_attention_frequency,\n",
    "        )\n",
    "\n",
    "        # Loss components\n",
    "        self.itm_criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negatives = num_negatives\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "        # Initialize T5 model\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "        # Ensure that pad_token_id is set\n",
    "        if self.t5_model.config.pad_token_id is None:\n",
    "            self.t5_model.config.pad_token_id = self.t5_model.config.eos_token_id\n",
    "\n",
    "        if self.t5_model.config.decoder_start_token_id is None:\n",
    "            self.t5_model.config.decoder_start_token_id = self.t5_model.config.pad_token_id\n",
    "\n",
    "\n",
    "        # Projects from 768 (Q-Former hidden size) to 512 (T5 hidden size).\n",
    "        self.query_proj_t5 = nn.Linear(qformer_hidden_size, self.t5_model.config.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, conditional_input_ids, conditional_attention_mask, target_input_ids,\n",
    "                target_attention_mask, neg_pixel_values=None):\n",
    "        # Process text embeddings for CLIP and ITM losses (full text)\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "\n",
    "        # Process image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the custom Q-Former\n",
    "        query_embeddings = self.qformer(image_embeddings)\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "\n",
    "        # Process negative image embeddings if provided\n",
    "        neg_image_embeddings = None\n",
    "        if neg_pixel_values is not None:\n",
    "            neg_image_embeddings = self._process_negative_images(neg_pixel_values)\n",
    "\n",
    "        # Text Generation Loss\n",
    "        # Encode conditional text using T5 encoder\n",
    "        conditional_text_outputs = self.t5_model.encoder(\n",
    "            input_ids=conditional_input_ids,\n",
    "            attention_mask=conditional_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        conditional_text_embeddings = conditional_text_outputs.last_hidden_state  # (batch_size, seq_len_conditional, hidden_size)\n",
    "\n",
    "        # Project query embeddings to match T5 hidden size\n",
    "        projected_query_embeddings = self.query_proj_t5(query_embeddings)\n",
    "\n",
    "        # Combine conditional text embeddings and projected query embeddings\n",
    "        combined_encoder_embeddings = torch.cat([conditional_text_embeddings, projected_query_embeddings], dim=1)  # Concatenate along sequence length\n",
    "\n",
    "        # Create encoder attention mask\n",
    "        projected_query_attention_mask = torch.ones(\n",
    "            projected_query_embeddings.size()[:-1],\n",
    "            dtype=torch.long,\n",
    "            device=self.device\n",
    "        )\n",
    "        combined_attention_mask = torch.cat([conditional_attention_mask, projected_query_attention_mask], dim=1)\n",
    "\n",
    "        # Prepare labels for T5 (target text)\n",
    "        labels = target_input_ids.clone()\n",
    "        labels[labels == self.t5_model.config.pad_token_id] = -100  # Mask padding tokens in loss computation\n",
    "\n",
    "        # Pass through T5 model\n",
    "        t5_outputs = self.t5_model(\n",
    "            encoder_outputs=BaseModelOutput(last_hidden_state=combined_encoder_embeddings),\n",
    "            attention_mask=combined_attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        text_generation_loss = t5_outputs.loss\n",
    "\n",
    "        return text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss\n",
    "\n",
    "\n",
    "    def _process_negative_images(self, neg_pixel_values):\n",
    "        batch_size, num_negatives, _, _, _ = neg_pixel_values.shape\n",
    "        neg_pixel_values = neg_pixel_values.view(-1, *neg_pixel_values.shape[2:])  # (batch_size * num_negatives, C, H, W)\n",
    "        neg_image_outputs = self.image_model(pixel_values=neg_pixel_values)\n",
    "        neg_image_embeddings = neg_image_outputs.last_hidden_state.mean(dim=1)  # (batch_size * num_negatives, d_model)\n",
    "        neg_image_embeddings = F.normalize(neg_image_embeddings, p=2, dim=-1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch_size, num_negatives, -1)  # (batch_size, num_negatives, d_model)\n",
    "        return neg_image_embeddings\n",
    "\n",
    "    def compute_clip_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute similarities\n",
    "        # print(f\"Positive text embeddings shape: {pos_text_embeddings.shape}\")\n",
    "        # print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "        # print(f\"Negative image embeddings shape: {neg_image_embeddings.shape}\")\n",
    "        \n",
    "        # Positive similarities between text and positive image queries\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_sim = pos_sim.max(dim=1, keepdim=True).values  # (batch_size, 1)\n",
    "\n",
    "        # Negative similarities between text and negative images\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "\n",
    "        # Combine logits\n",
    "        logits = torch.cat([pos_sim, neg_sim], dim=1)  # (batch_size, 1 + num_negatives)\n",
    "        # print(f\"Logits shape (for CLIP loss): {logits.shape}\")\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(logits.device)  # (batch_size,)\n",
    "\n",
    "        # Compute CLIP loss using cross-entropy\n",
    "        clip_loss = self.itm_criterion(logits, labels)\n",
    "        return clip_loss\n",
    "\n",
    "    def compute_itm_loss(self, pos_text_embeddings, query_embeddings, neg_image_embeddings):\n",
    "        # Compute positive scores\n",
    "        pos_sim = torch.einsum('bqd,bd->bq', query_embeddings, pos_text_embeddings) / self.temperature  # (batch_size, num_queries)\n",
    "        pos_scores = pos_sim.max(dim=1).values  # (batch_size,)\n",
    "\n",
    "        # Compute negative scores\n",
    "        neg_sim = torch.einsum('bd,bnd->bn', pos_text_embeddings, neg_image_embeddings) / self.temperature  # (batch_size, num_negatives)\n",
    "        neg_scores = neg_sim.view(-1)  # (batch_size * num_negatives,)\n",
    "\n",
    "        # Combine scores and labels\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "        labels = torch.cat([\n",
    "            torch.ones(pos_scores.size(0), device=scores.device),\n",
    "            torch.zeros(neg_scores.size(0), device=scores.device)\n",
    "        ], dim=0)  # (batch_size + batch_size * num_negatives,)\n",
    "        \n",
    "        # print(f\"Scores shape: {scores.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Compute binary cross-entropy loss with logits\n",
    "        itm_loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "        return itm_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            input_ids=batch['pos_input_ids'],\n",
    "            attention_mask=batch['pos_attention_mask'],\n",
    "            pixel_values=batch['pos_pixel_values'],\n",
    "            conditional_input_ids=batch['conditional_input_ids'],\n",
    "            conditional_attention_mask=batch['conditional_attention_mask'],\n",
    "            target_input_ids=batch['target_input_ids'],\n",
    "            target_attention_mask=batch['target_attention_mask'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        train_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        self.log('train_clip_loss', clip_loss)\n",
    "        self.log('train_itm_loss', itm_loss)\n",
    "        self.log('train_text_generation_loss', text_generation_loss)\n",
    "        self.log('train_loss', train_loss)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            input_ids=batch['pos_input_ids'],\n",
    "            attention_mask=batch['pos_attention_mask'],\n",
    "            pixel_values=batch['pos_pixel_values'],\n",
    "            conditional_input_ids=batch['conditional_input_ids'],\n",
    "            conditional_attention_mask=batch['conditional_attention_mask'],\n",
    "            target_input_ids=batch['target_input_ids'],\n",
    "            target_attention_mask=batch['target_attention_mask'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        val_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        self.log('val_clip_loss', clip_loss)\n",
    "        self.log('val_itm_loss', itm_loss)\n",
    "        self.log('val_text_generation_loss', text_generation_loss)\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pos_text_embeddings, query_embeddings, neg_image_embeddings, text_generation_loss = self(\n",
    "            input_ids=batch['pos_input_ids'],\n",
    "            attention_mask=batch['pos_attention_mask'],\n",
    "            pixel_values=batch['pos_pixel_values'],\n",
    "            conditional_input_ids=batch['conditional_input_ids'],\n",
    "            conditional_attention_mask=batch['conditional_attention_mask'],\n",
    "            target_input_ids=batch['target_input_ids'],\n",
    "            target_attention_mask=batch['target_attention_mask'],\n",
    "            neg_pixel_values=batch.get('neg_pixel_values')\n",
    "        )\n",
    "\n",
    "        clip_loss = self.compute_clip_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "        itm_loss = self.compute_itm_loss(pos_text_embeddings, query_embeddings, neg_image_embeddings)\n",
    "\n",
    "        test_loss = clip_loss + itm_loss + text_generation_loss\n",
    "        self.log('test_clip_loss', clip_loss)\n",
    "        self.log('test_itm_loss', itm_loss)\n",
    "        self.log('test_text_generation_loss', text_generation_loss)\n",
    "        self.log('test_loss', test_loss)\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps\n",
    "        )\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if 'val_loss' in self.trainer.callback_metrics:\n",
    "            avg_val_loss = self.trainer.callback_metrics['val_loss'].mean()\n",
    "            # print(f\"[Validation End] Average validation loss: {avg_val_loss}\")\n",
    "            self.log('avg_val_loss', avg_val_loss)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if 'test_loss' in self.trainer.callback_metrics:\n",
    "            avg_test_loss = self.trainer.callback_metrics['test_loss'].mean()\n",
    "            # print(f\"[Test End] Average test loss: {avg_test_loss}\")\n",
    "            self.log('avg_test_loss', avg_test_loss)\n",
    "            \n",
    "    # Add the generate_embeddings function to generate text and image embeddings at inference\n",
    "    def get_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='multimodal'):\n",
    "        \"\"\"\n",
    "        Generate text, image, or multimodal embeddings for inference.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Tokenized input text IDs.\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for the input text.\n",
    "            pixel_values (torch.Tensor, optional): Preprocessed image tensor.\n",
    "            embedding_type (str): 'text', 'image', or 'multimodal'.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The requested embeddings.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "            return text_embeddings\n",
    "\n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "            return query_embeddings\n",
    "\n",
    "        elif embedding_type == 'multimodal':\n",
    "            if input_ids is None or attention_mask is None or pixel_values is None:\n",
    "                raise ValueError(\"input_ids, attention_mask, and pixel_values are required for multimodal embeddings.\")\n",
    "            text_embeddings = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            image_embeddings = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            query_embeddings = self.qformer(image_embeddings).mean(dim=1)\n",
    "            # Combine embeddings (e.g., concatenate)\n",
    "            \n",
    "            multimodal_embeddings = (text_embeddings + query_embeddings)/2\n",
    "            multimodal_embeddings = F.normalize(multimodal_embeddings, p=2, dim=-1)\n",
    "            return multimodal_embeddings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose 'text', 'image', or 'multimodal'.\")\n",
    "\n",
    "    # Add the generate_text function to generate text at inference\n",
    "    def generate_text(self, conditional_input_ids, conditional_attention_mask, pixel_values=None, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate text based on input text and/or image.\n",
    "\n",
    "        Args:\n",
    "            conditional_input_ids (torch.Tensor): Tokenized conditional input text IDs of shape [batch_size, seq_len].\n",
    "            conditional_attention_mask (torch.Tensor): Attention mask for the conditional input text of shape [batch_size, seq_len].\n",
    "            pixel_values (torch.Tensor, optional): Image tensor of shape [batch_size, channels, height, width].\n",
    "            max_length (int, optional): Maximum length of generated text. Default is 50.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated text strings.\n",
    "        \"\"\"\n",
    "        # Encode conditional text using T5 encoder\n",
    "        conditional_text_outputs = self.t5_model.encoder(\n",
    "            input_ids=conditional_input_ids,\n",
    "            attention_mask=conditional_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        conditional_text_embeddings = conditional_text_outputs.last_hidden_state  # (batch_size, seq_len_conditional, hidden_size)\n",
    "\n",
    "        # If pixel values are provided, encode them with the Q-Former\n",
    "        if pixel_values is not None:\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            image_embeddings = image_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "            query_embeddings = self.qformer(image_embeddings)\n",
    "            projected_query_embeddings = self.query_proj_t5(query_embeddings)\n",
    "            combined_encoder_embeddings = torch.cat([conditional_text_embeddings, projected_query_embeddings], dim=1)\n",
    "\n",
    "            # Create encoder attention mask\n",
    "            projected_query_attention_mask = torch.ones(\n",
    "                projected_query_embeddings.size()[:-1],\n",
    "                dtype=torch.long,\n",
    "                device=self.device\n",
    "            )\n",
    "            combined_attention_mask = torch.cat([conditional_attention_mask, projected_query_attention_mask], dim=1)\n",
    "        else:\n",
    "            combined_encoder_embeddings = conditional_text_embeddings\n",
    "            combined_attention_mask = conditional_attention_mask\n",
    "\n",
    "        # Generate text using T5 decoder\n",
    "        generated_ids = self.t5_model.generate(\n",
    "            inputs_embeds=combined_encoder_embeddings,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        generated_text = [self.t5_model.config.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a136a2fe-9860-4483-a077-d426c1a633df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPCheckpointModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model  # This should be a pre-trained CLIPModel\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        self.device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Process images\n",
    "        image_inputs = self.clip_processor(images=images, return_tensors='pt').to(self.device_type)\n",
    "        image_embeddings = self.model.get_image_features(**image_inputs)\n",
    "\n",
    "        # Process texts\n",
    "        text_embeddings = self.compute_text_embeddings(texts)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "    def compute_text_embeddings(self, texts):\n",
    "        text_embeddings = []\n",
    "        for text in texts:\n",
    "            # Tokenize the text without truncation\n",
    "            tokens = self.clip_processor.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=False,\n",
    "                add_special_tokens=False\n",
    "            )['input_ids'].squeeze(0)\n",
    "\n",
    "            # Implement sliding window\n",
    "            window_size = 77\n",
    "            stride = 50\n",
    "            num_tokens = tokens.size(0)\n",
    "            window_embeddings = []\n",
    "\n",
    "            for i in range(0, num_tokens, stride):\n",
    "                window_tokens = tokens[i:i + window_size]\n",
    "                if window_tokens.size(0) == 0:\n",
    "                    break\n",
    "\n",
    "                if window_tokens.size(0) < window_size:\n",
    "                    padding = torch.full(\n",
    "                        (window_size - window_tokens.size(0),),\n",
    "                        self.clip_processor.tokenizer.pad_token_id\n",
    "                    )\n",
    "                    window_tokens = torch.cat([window_tokens, padding])\n",
    "\n",
    "                attention_mask = (window_tokens != self.clip_processor.tokenizer.pad_token_id).long()\n",
    "\n",
    "                window_tokens = window_tokens.unsqueeze(0).to(self.device)\n",
    "                attention_mask = attention_mask.unsqueeze(0).to(self.device)\n",
    "\n",
    "                # Encode window tokens\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.text_model(\n",
    "                        input_ids=window_tokens,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "                window_embeddings.append(embedding)\n",
    "\n",
    "            # Aggregate embeddings\n",
    "            if window_embeddings:\n",
    "                window_embeddings = torch.cat(window_embeddings, dim=0)\n",
    "                aggregated_embedding = window_embeddings.mean(dim=0)\n",
    "            else:\n",
    "                aggregated_embedding = torch.zeros(self.model.config.hidden_size).to(self.device)\n",
    "\n",
    "            text_embeddings.append(aggregated_embedding)\n",
    "\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        return text_embeddings\n",
    "\n",
    "    def compute_loss(self, image_embeddings, text_embeddings):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss for the image and text embeddings.\n",
    "        \"\"\"\n",
    "        # Compute similarity scores\n",
    "        logits_per_image = image_embeddings @ text_embeddings.t() * self.model.logit_scale.exp()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # Labels\n",
    "        batch_size = image_embeddings.size(0)\n",
    "        labels = torch.arange(batch_size).to(image_embeddings.device)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch['images']\n",
    "        texts = batch['texts']\n",
    "        batch_size = len(images)  # Calculate batch size\n",
    "        image_embeddings, text_embeddings = self(images, texts)\n",
    "        loss = self.compute_loss(image_embeddings, text_embeddings)\n",
    "        self.log('train_loss', loss, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch['images']\n",
    "        texts = batch['texts']\n",
    "        batch_size = len(images)  # Calculate batch size\n",
    "        image_embeddings, text_embeddings = self(images, texts)\n",
    "        loss = self.compute_loss(image_embeddings, text_embeddings)\n",
    "        self.log('val_loss', loss, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images = batch['images']\n",
    "        texts = batch['texts']\n",
    "        batch_size = len(images)  # Calculate batch size\n",
    "        image_embeddings, text_embeddings = self(images, texts)\n",
    "        loss = self.compute_loss(image_embeddings, text_embeddings)\n",
    "        self.log('test_loss', loss, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def generate_embeddings(self, images=None, texts=None):\n",
    "        \"\"\"\n",
    "        Generate embeddings for the given images or texts.\n",
    "\n",
    "        Parameters:\n",
    "        - images: List of images to generate embeddings for.\n",
    "        - texts: List of texts to generate embeddings for.\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary with 'image_embeddings' and 'text_embeddings' as keys, depending on the inputs.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Check if images are provided\n",
    "        if images is not None:\n",
    "            self.model.eval()  # Set model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                image_inputs = self.clip_processor(images=images, return_tensors='pt').to(self.device_type)\n",
    "                image_embeddings = self.model.get_image_features(**image_inputs)\n",
    "                image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "                results['image_embeddings'] = image_embeddings\n",
    "\n",
    "        # Check if texts are provided\n",
    "        if texts is not None:\n",
    "            self.model.eval()  # Set model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = self.compute_text_embeddings(texts)\n",
    "                text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "                results['text_embeddings'] = text_embeddings\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7768d94a-c67d-4604-a543-2bc78602aa01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPCLSModel(pl.LightningModule):\n",
    "    def __init__(self, weight_decay, eps, warmup_steps, num_training_steps, text_model_name='johngiorgi/declutr-small', \n",
    "                image_model_name='google/vit-base-patch16-224', learning_rate=0.00001, num_negatives=5, temperature=0.5):\n",
    "        super(CLIPCLSModel, self).__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negatives = num_negatives\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "        # Store outputs for validation and testing\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Get text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        # Get text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        eos_mask = input_ids.eq(self.text_model.config.eos_token_id)\n",
    "        eos_indices = eos_mask.nonzero(as_tuple=False)\n",
    "        text_embeddings = text_outputs.last_hidden_state[eos_indices[:, 0], eos_indices[:, 1]]\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Get image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "        image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "        \n",
    "        return text_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings):\n",
    "        # Normalize embeddings\n",
    "        pos_text_embeddings = F.normalize(pos_text_embeddings, p=2, dim=1)\n",
    "        pos_image_embeddings = F.normalize(pos_image_embeddings, p=2, dim=1)\n",
    "        neg_text_embeddings = F.normalize(neg_text_embeddings, p=2, dim=2)  # Normalized over the last dimension\n",
    "        neg_image_embeddings = F.normalize(neg_image_embeddings, p=2, dim=2)  # Normalized over the last dimension\n",
    "\n",
    "        # Positive pairs similarity\n",
    "        pos_sim = torch.exp(torch.sum(pos_text_embeddings * pos_image_embeddings, dim=-1) / self.temperature)\n",
    "        \n",
    "        # Negative pairs similarity (text to image)\n",
    "        neg_sim_text_image = torch.exp(torch.einsum('bij,bj->bi', neg_text_embeddings, pos_image_embeddings) / self.temperature)\n",
    "        # Negative pairs similarity (image to text)\n",
    "        neg_sim_image_text = torch.exp(torch.einsum('bi,bkj->bk', pos_text_embeddings, neg_image_embeddings) / self.temperature)\n",
    "\n",
    "        # Calculate the loss for text-to-image\n",
    "        denominator_text_image = pos_sim + neg_sim_text_image.sum(dim=1)\n",
    "        loss_text_image = -torch.log(pos_sim / denominator_text_image)\n",
    "\n",
    "        # Calculate the loss for image-to-text\n",
    "        denominator_image_text = pos_sim.unsqueeze(1) + neg_sim_image_text\n",
    "        loss_image_text = -torch.log(pos_sim.unsqueeze(1) / denominator_image_text).sum(dim=1)\n",
    "\n",
    "        # Combine both losses\n",
    "        loss = (loss_text_image + loss_image_text).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        pos_text_embeddings, pos_image_embeddings = self(batch['pos_input_ids'], batch['pos_attention_mask'], batch['pos_pixel_values'])\n",
    "        neg_text_embeddings, neg_image_embeddings = self(batch['neg_input_ids'].view(-1, batch['neg_input_ids'].shape[-1]), \n",
    "                                                         batch['neg_attention_mask'].view(-1, batch['neg_attention_mask'].shape[-1]), \n",
    "                                                         batch['neg_pixel_values'].view(-1, batch['neg_pixel_values'].shape[-3], \n",
    "                                                                                        batch['neg_pixel_values'].shape[-2], batch['neg_pixel_values'].shape[-1]))\n",
    "        \n",
    "        # Reshape negative embeddings\n",
    "        neg_text_embeddings = neg_text_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        neg_image_embeddings = neg_image_embeddings.view(batch['neg_input_ids'].shape[0], self.num_negatives, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(pos_text_embeddings, pos_image_embeddings, neg_text_embeddings, neg_image_embeddings)\n",
    "\n",
    "        self.test_outputs.append(loss)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if self.test_outputs:\n",
    "            avg_loss = torch.stack(self.test_outputs).mean()\n",
    "            self.log('avg_test_loss', avg_loss)\n",
    "        self.test_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.eps)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def get_embeddings(self, input_ids=None, attention_mask=None, pixel_values=None, embedding_type='text'):\n",
    "        \"\"\"\n",
    "        Generate text or image embeddings for inference.\n",
    "\n",
    "        Args:\n",
    "        input_ids: Tokenized input text (for text embeddings).\n",
    "        attention_mask: Attention mask for input text (for text embeddings).\n",
    "        pixel_values: Preprocessed image (for image embeddings).\n",
    "        embedding_type: Specify 'text' or 'image' to generate the respective embeddings.\n",
    "\n",
    "        Returns:\n",
    "        Normalized embeddings.\n",
    "        \"\"\"\n",
    "        if embedding_type == 'text':\n",
    "            if input_ids is None or attention_mask is None:\n",
    "                raise ValueError(\"input_ids and attention_mask are required for text embeddings.\")\n",
    "            \n",
    "            # Get text embeddings\n",
    "            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            eos_mask = input_ids.eq(self.text_model.config.eos_token_id)\n",
    "            eos_indices = eos_mask.nonzero(as_tuple=False)\n",
    "            text_embeddings = text_outputs.last_hidden_state[eos_indices[:, 0], eos_indices[:, 1]]\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "            return text_embeddings\n",
    "        \n",
    "        elif embedding_type == 'image':\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"pixel_values are required for image embeddings.\")\n",
    "            \n",
    "            # Get image embeddings\n",
    "            image_outputs = self.image_model(pixel_values=pixel_values)\n",
    "            image_embeddings = image_outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "            image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)  # Normalize embeddings\n",
    "            return image_embeddings\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid embedding_type. Choose either 'text' or 'image'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2a339bcb-7e5b-48bf-9cef-6316f79bd22e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.model_type = \"BLIP2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b203f38-dcc5-4baf-aaa6-2c2ccd9bb677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "if args.model_type == \"CLIP\":\n",
    "\n",
    "    sys.path.append('../architectures/')\n",
    "    # from CLIPLayer import CLIPModel\n",
    "    \n",
    "    model = CLIPModel(\n",
    "        text_model_name='johngiorgi/declutr-small', \n",
    "        image_model_name='google/vit-base-patch16-224', \n",
    "        learning_rate=args.learning_rate, \n",
    "        num_negatives=args.nb_negatives,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_training_steps=1000,\n",
    "        temperature=args.temp\n",
    "    )\n",
    "    \n",
    "# Initialize the model\n",
    "if args.model_type == \"CLIPCLS\":\n",
    "\n",
    "    sys.path.append('../architectures/')\n",
    "    # from CLIPLayer import CLIPModel\n",
    "    \n",
    "    model = CLIPCLSModel(\n",
    "        text_model_name='johngiorgi/declutr-small', \n",
    "        image_model_name='google/vit-base-patch16-224', \n",
    "        learning_rate=args.learning_rate, \n",
    "        num_negatives=args.nb_negatives,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_training_steps=1000,\n",
    "        temperature=args.temp\n",
    "    )\n",
    "    \n",
    "if args.model_type == \"BigCLIP\":\n",
    "        model = CLIPModel(\n",
    "        text_model_name='johngiorgi/declutr-base', \n",
    "        image_model_name='openai/clip-vit-base-patch32', \n",
    "        learning_rate=args.learning_rate, \n",
    "        num_negatives=args.nb_negatives,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_training_steps=1000,\n",
    "        temperature=args.temp\n",
    "    )\n",
    "        \n",
    "if args.model_type == \"BigCLIPITM\":\n",
    "        model = CLIPITMModel(\n",
    "        text_model_name='johngiorgi/declutr-base', \n",
    "        image_model_name='openai/clip-vit-base-patch32', \n",
    "        learning_rate=args.learning_rate, \n",
    "        num_negatives=args.nb_negatives,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_training_steps=1000,\n",
    "        temperature=args.temp\n",
    "    )\n",
    "        \n",
    "elif args.model_type == \"CLIPCheckpoint\":\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        clip_model_name = 'openai/clip-vit-base-patch32'\n",
    "        clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "        model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        \n",
    "        model = CLIPCheckpointModel(model)\n",
    "\n",
    "elif args.model_type == \"CLIPITM\":\n",
    "\n",
    "    sys.path.append('../architectures/')\n",
    "    # from CLIPITMLayer import CLIPITMModel\n",
    "\n",
    "    model = CLIPITMModel(\n",
    "    text_model_name='johngiorgi/declutr-small', \n",
    "    image_model_name='google/vit-base-patch16-224', \n",
    "    learning_rate=args.learning_rate, \n",
    "    num_negatives=args.nb_negatives,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_training_steps=1000,\n",
    "    temperature=args.temp\n",
    "    )\n",
    "\n",
    "elif args.model_type == \"BLIP2\":\n",
    "    sys.path.append('../architectures/')\n",
    "    # from BLIP2Layer import BLIP2Model\n",
    "\n",
    "    model = BLIP2Model(\n",
    "    text_model_name='johngiorgi/declutr-small', \n",
    "    image_model_name='google/vit-base-patch16-224', \n",
    "    t5_model_name='google/flan-t5-small',\n",
    "    learning_rate=args.learning_rate, \n",
    "    num_negatives=args.nb_negatives,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_training_steps=1000,\n",
    "    temperature=args.temp,\n",
    "    num_query_tokens=32,\n",
    "    qformer_hidden_size=768,\n",
    "    cross_attention_frequency=1\n",
    "    )\n",
    "    \n",
    "elif args.model_type == \"ConditionalBLIP\":\n",
    "    sys.path.append('../architectures/')\n",
    "    # from BLIP2Layer import BLIP2Model\n",
    "\n",
    "    model = BLIP2ConditionalModel(\n",
    "    text_model_name='johngiorgi/declutr-small', \n",
    "    image_model_name='google/vit-base-patch16-224', \n",
    "    t5_model_name='google/flan-t5-small',\n",
    "    learning_rate=args.learning_rate, \n",
    "    num_negatives=args.nb_negatives,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_training_steps=1000,\n",
    "    temperature=args.temp,\n",
    "    num_query_tokens=32,\n",
    "    qformer_hidden_size=768,\n",
    "    cross_attention_frequency=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "26adf4f3-a423-44fe-a5aa-7a3f6193389a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/workspace/persistent/HTClipper/models/grouped-and-masked/multimodal-baselines/pre-training/BLIP2/non-associated/seed:1111/lr-0.0001/NTXENT/0.1/negatives-5/final_model.ckpt\")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c0493-cd71-4e30-9bd9-f1daa57063ea",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0cb314-da70-4857-b1af-6afb7b65a7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b59e15-0357-4ecd-8df1-faeef6eb66af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to map images with text for CLIP model\n",
    "def map_images_with_text_for_clip_model(df, img_dir, filter_by=\"vendor\"):\n",
    "    # Initialize a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['TEXT']\n",
    "        all_images = str(row['IMAGES']).split('|')\n",
    "        if filter_by == \"vendor\":\n",
    "            vendor = row['VENDOR']\n",
    "        elif filter_by == \"id\":\n",
    "            vendor = row['ID']\n",
    "        region = row['region']\n",
    "        \n",
    "        # Create a new entry for each image\n",
    "        for image in all_images:\n",
    "            full_image_path = os.path.join(img_dir, region, \"image\", \"image\", image)\n",
    "            \n",
    "            # Only add the row if the image exists at the specified path\n",
    "            if os.path.exists(full_image_path):\n",
    "                new_rows.append({\n",
    "                    'TEXT': text,\n",
    "                    'IMAGES': full_image_path,  # Store the full image path\n",
    "                    'VENDOR': vendor,\n",
    "                    'region' : region\n",
    "                })\n",
    "\n",
    "    # Create a new dataframe from the list of new rows\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54777ef-f295-4b62-a7e5-d1cb0dae4651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to map images with text for BLIP2 model\n",
    "def map_images_with_text_for_blip2_model(df, img_dir, filter_by=\"vendor\"):\n",
    "    # Similar to your function, adjust if needed\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['TEXT']\n",
    "        all_images = str(row['IMAGES']).split('|')\n",
    "        if filter_by == \"vendor\":\n",
    "            vendor = row['VENDOR']\n",
    "        elif filter_by == \"id\":\n",
    "            vendor = row['ID']\n",
    "        region = row['region']\n",
    "\n",
    "        for image in all_images:\n",
    "            full_image_path = os.path.join(img_dir, region, \"image\", \"image\", image)\n",
    "\n",
    "            if os.path.exists(full_image_path):\n",
    "                new_rows.append({\n",
    "                    'TEXT': text,\n",
    "                    'IMAGES': full_image_path,\n",
    "                    'VENDOR': vendor,\n",
    "                    'region' : region\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9444cc81-df49-4288-98f8-67363fbda7eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference Dataset\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, image_processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['TEXT'])\n",
    "        image_path = row['IMAGES']\n",
    "        vendor = row['VENDOR']\n",
    "\n",
    "        # Tokenize the text\n",
    "        text_inputs = self.text_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Load and process the image\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "        image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': image,\n",
    "            'label': vendor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dbf1c03-d291-4204-96a8-e1be45ea51fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Inference Dataset for BLIP2\n",
    "class BLIP2InferenceDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, image_processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['TEXT'])\n",
    "        image_path = row['IMAGES']\n",
    "        vendor = row['VENDOR']\n",
    "\n",
    "        # Tokenize the text\n",
    "        text_inputs = self.text_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Load and process the image\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "        image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': image,\n",
    "            'label': vendor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16363dc8-09d4-4ded-9c25-c3ca157a42ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BLIP2ConditionalInferenceDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, t5_tokenizer, image_processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        full_text = str(row['TEXT'])\n",
    "        image_path = row['IMAGES']\n",
    "        vendor = row['VENDOR']\n",
    "\n",
    "        # Split the text on [SEP] for text generation\n",
    "        if '[SEP]' in full_text:\n",
    "            conditional_text, target_text = full_text.split('[SEP]', 1)\n",
    "            conditional_text = conditional_text.strip()\n",
    "            target_text = target_text.strip()\n",
    "        else:\n",
    "            # If [SEP] is not present, handle accordingly\n",
    "            conditional_text = ''\n",
    "            target_text = full_text.strip()\n",
    "\n",
    "        # Tokenize the full text for CLIP and ITM losses\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            full_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Tokenize the conditional text (text prompt) for text generation\n",
    "        conditional_text_inputs = self.t5_tokenizer(\n",
    "            conditional_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        conditional_input_ids = conditional_text_inputs['input_ids'].squeeze(0)\n",
    "        conditional_attention_mask = conditional_text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Tokenize the target text for text generation\n",
    "        target_text_inputs = self.t5_tokenizer(\n",
    "            target_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        target_input_ids = target_text_inputs['input_ids'].squeeze(0)\n",
    "        target_attention_mask = target_text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Load and process the image\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "        image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': image,\n",
    "            'conditional_input_ids': conditional_input_ids,\n",
    "            'conditional_attention_mask': conditional_attention_mask,\n",
    "            'target_input_ids': target_input_ids,\n",
    "            'target_attention_mask': target_attention_mask,\n",
    "            'label': vendor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de645c0d-b447-4714-9e7d-7c1f9db73085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only run use them in our further experiments.\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-small')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# text_tokenizer = AutoTokenizer.from_pretrained('johngiorgi/declutr-base')\n",
    "# image_processor = ViTImageProcessor.from_pretrained('openai/clip-vit-base-patch32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fc43f-5041-46ec-943b-3bf3dfc360b3",
   "metadata": {},
   "source": [
    "# Extract emnbeddings from unique text and image ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d9c52aae-ea45-469d-8c29-a2f591032060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_for_CLIPModel(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    assert filter_by in [\"vendor\", \"ids\"]\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_clip_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"CLIP\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), np.array(train_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), np.array(train_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), np.array(test_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), np.array(test_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "        \n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), np.array(train_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), np.array(train_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), np.array(test_text_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), np.array(test_image_embeddings))\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels\n",
    "\n",
    "def process_dataset_for_CLIPITMModel(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_clip_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"CLIPITM\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "\n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels\n",
    "\n",
    "def process_dataset_for_BLIP2Model(region_name, data_dir, image_dir, model, text_tokenizer, image_processor, filter_by=\"vendor\", batch_size=32):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(os.path.join(data_dir, f\"{region_name}.csv\"))\n",
    "    df['region'] = region_name\n",
    "    df = map_images_with_text_for_blip2_model(df, img_dir=image_dir, filter_by=filter_by).drop_duplicates()\n",
    "\n",
    "    df_filtered = df\n",
    "\n",
    "    # Get unique text embeddings\n",
    "    unique_texts = df_filtered['TEXT'].unique()\n",
    "    text_embeddings = {}\n",
    "    text_labels = []\n",
    "\n",
    "    # Extract text embeddings with tqdm progress bar\n",
    "    for text in tqdm(unique_texts, desc=\"Extracting Text Embeddings\"):\n",
    "        inputs = text_tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        text_embed = model.get_embeddings(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], embedding_type='text')\n",
    "        text_embeddings[text] = text_embed.detach().cpu().numpy()\n",
    "        text_labels.append(df_filtered[df_filtered['TEXT'] == text]['VENDOR'].values[0])  # Get vendor for the text\n",
    "\n",
    "    # Get unique images and their embeddings\n",
    "    unique_images = df_filtered['IMAGES'].unique()\n",
    "    image_embeddings = {}\n",
    "    image_labels = []\n",
    "    seen_embeddings = set()  # To track unique embeddings\n",
    "\n",
    "    # Extract image embeddings with tqdm progress bar\n",
    "    for image_path in tqdm(unique_images, desc=\"Extracting Image Embeddings\"):\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB format\n",
    "        image_tensor = image_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)  # Preprocess the image\n",
    "        image_embed = model.get_embeddings(pixel_values=image_tensor, embedding_type='image')\n",
    "\n",
    "        # Convert the embedding to a tuple to make it hashable for the set\n",
    "        embedding_tuple = tuple(image_embed.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if embedding_tuple not in seen_embeddings:\n",
    "            seen_embeddings.add(embedding_tuple)  # Track the unique embedding\n",
    "            image_embeddings[image_path] = image_embed.detach().cpu().numpy()  # Store the unique embedding\n",
    "            image_labels.append(df_filtered[df_filtered['IMAGES'] == image_path]['VENDOR'].values[0])  # Get vendor for the image\n",
    "\n",
    "    # Train-test split\n",
    "    train_text_embeddings, test_text_embeddings, train_text_labels, test_text_labels = train_test_split(\n",
    "        list(text_embeddings.values()), text_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "    train_image_embeddings, test_image_embeddings, train_image_labels, test_image_labels = train_test_split(\n",
    "        list(image_embeddings.values()), image_labels, test_size=0.2, random_state=1111\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/trained_declutr_vit/\", \"BLIP2\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure embeddings are in the desired shape (batch_size, embedding_dim)\n",
    "    train_text_embeddings = np.array(train_text_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_text_embeddings = np.array(test_text_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "    train_image_embeddings = np.array(train_image_embeddings).squeeze()  # Shape: (train_size, 768)\n",
    "    test_image_embeddings = np.array(test_image_embeddings).squeeze()    # Shape: (test_size, 768)\n",
    "\n",
    "    if filter_by == \"vendor\":\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_vendors.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_vendors.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_vendors.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_vendors.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_vendors.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_vendors.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_vendors.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_vendors.npy'), np.array(test_image_labels))\n",
    "\n",
    "    else:\n",
    "        np.save(os.path.join(output_dir, f'train_text_embeddings_{region_name}_ids.npy'), train_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_image_embeddings_{region_name}_ids.npy'), train_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'train_text_labels_{region_name}_ids.npy'), np.array(train_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'train_image_labels_{region_name}_ids.npy'), np.array(train_image_labels))\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'test_text_embeddings_{region_name}_ids.npy'), test_text_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_image_embeddings_{region_name}_ids.npy'), test_image_embeddings)\n",
    "        np.save(os.path.join(output_dir, f'test_text_labels_{region_name}_ids.npy'), np.array(test_text_labels))\n",
    "        np.save(os.path.join(output_dir, f'test_image_labels_{region_name}_ids.npy'), np.array(test_image_labels))\n",
    "\n",
    "    print(f\"Processed region: {region_name}\")\n",
    "    print(f\"Number of training samples: {len(train_text_labels)}\")\n",
    "    print(f\"Number of testing samples: {len(test_text_labels)}\\n\")\n",
    "    \n",
    "    return train_text_embeddings, train_image_embeddings, train_text_labels, train_image_labels, test_text_embeddings, test_image_embeddings, test_text_labels, test_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568f82-4e6d-4db0-91b1-dec9df710796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------south--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Text Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13677/13677 [02:05<00:00, 109.31it/s]\n",
      "Extracting Image Embeddings:  16%|â–ˆâ–Œ        | 10529/65544 [09:36<52:20, 17.52it/s]  "
     ]
    }
   ],
   "source": [
    "# List of regions to process\n",
    "regions = ['south', 'midwest', 'west', 'northeast']\n",
    "\n",
    "# , 'northeast'\n",
    "# Process each regiond\n",
    "for region in regions:\n",
    "    print(\"-\"*50 + region + \"-\"*50)\n",
    "    _, _, _, _, _, _, _, _ = process_dataset_for_BLIP2Model(\n",
    "        region_name=region,\n",
    "        data_dir=args.data_dir,\n",
    "        image_dir=args.image_dir,\n",
    "        model=model,\n",
    "        text_tokenizer=text_tokenizer,\n",
    "        image_processor=image_processor,\n",
    "        filter_by = \"vendor\",\n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798159cb-5681-4a23-adfc-30e6ab56a57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of regions to process\n",
    "regions = ['south', 'midwest', 'west', 'northeast']\n",
    "\n",
    "# , 'northeast'\n",
    "# Process each region\n",
    "for region in regions:\n",
    "    print(\"-\"*50 + region + \"-\"*50)\n",
    "    _, _, _, _, _, _, _, _ = process_dataset_for_BLIP2Model(\n",
    "        region_name=region,\n",
    "        data_dir=args.data_dir,\n",
    "        image_dir=args.image_dir,\n",
    "        model=model,\n",
    "        text_tokenizer=text_tokenizer,\n",
    "        image_processor=image_processor,\n",
    "        filter_by = \"ids\",\n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a56b7f-351d-4473-86d4-0c16f478a5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
