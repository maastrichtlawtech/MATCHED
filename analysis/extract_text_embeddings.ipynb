{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fcbbd5-7b36-48a7-8d5d-e0a765981bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re\n",
    "# import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b4bd51-e1be-4ef5-90ba-964718231a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b763ea-5fee-4365-a302-ac18e222fba8",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf364b2-844d-45c6-be2c-4433d66aef0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% Importing Libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split, TensorDataset\n",
    "\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "# from lightning.pytorch.strategies import DeepSpeedStrategy\n",
    "# from lightning.pytorch.plugins.precision import DeepSpeedPrecisionPlugin\n",
    "\n",
    "# from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "import t5_encoder\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "# from loadData import HTClassifierDataModule\n",
    "from loadData import HTContraDataModule\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from HTClassifier import HTClassifierModel\n",
    "from ContraLayer import HTContraClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8288b-1058-44b3-833e-f7e3f31062f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HTClassifierDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the class attributes\n",
    "        if isinstance(args, tuple) and len(args) > 0: \n",
    "            self.args = args[0]\n",
    "\n",
    "        # Handling the padding token in distilgpt2 by substituting it with eos_token_id\n",
    "        if self.args.tokenizer_name_or_path == \"distilgpt2\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.args.tokenizer_name_or_path, use_fast=True)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.args.tokenizer_name_or_path, use_fast=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        # Load the data from a CSV file\n",
    "        data_df = pd.read_csv(os.path.join(self.args.data_dir, self.args.demography + '.csv'))\n",
    "        # Replacing all the numbers in the training dataset with the letter \"N\"\n",
    "        data_df['TEXT'] = data_df['TEXT'].apply(lambda x: re.sub(r'\\d', 'N', str(x)))\n",
    "        text = data_df.TEXT.values.tolist()\n",
    "        vendors = data_df.VENDOR.values.tolist()\n",
    "        \n",
    "        # Tokenizing the data with padding and truncation\n",
    "        encodings = self.tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', return_token_type_ids=False, truncation=True, \n",
    "                                   return_attention_mask=True, return_tensors='pt') \n",
    "                                   \n",
    "        # Convert the lists into tensors.\n",
    "        input_ids = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "        \n",
    "        # Since the vendor IDs are not the current representations of the class labels, we remap these label IDs to avoid falling into out-of-bounds problem\n",
    "        vendors_dict = {}\n",
    "        i = 0\n",
    "        for vendor in vendors:\n",
    "            if vendor not in vendors_dict.keys():\n",
    "                vendors_dict[vendor] = i\n",
    "                i += 1\n",
    "        vendors = [vendors_dict[vendor] for vendor in vendors]\n",
    "        labels = torch.tensor(vendors)\n",
    "        \n",
    "        # Combine the inputs into a TensorDataset.\n",
    "        dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "                                   \n",
    "        # Getting an 0.75-0.05-0.20 split for training-val-test dataset\n",
    "        train_dataset, test_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(42))\n",
    "            \n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    # Returning the pytorch-lightning default training DataLoader \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, sampler=RandomSampler(self.train_dataset), batch_size=self.args.batch_size) \n",
    "\n",
    "    # Returning the pytorch-lightning default val DataLoader \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.args.batch_size) \n",
    "         \n",
    "    # Returning the pytorch-lightning default test DataLoader \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.args.batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a60260-9e88-42dc-9d69-6eb9331883d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating directories if they don't exist\n",
    "# ('../pickled/embeddings').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616e036-1329-4e9a-acdd-63545c72bfb5",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acb4f77-05f1-4c1c-bc91-7121accd930e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path = 'johngiorgi/declutr-small'\n",
    "        self.tokenizer_name_or_path = 'johngiorgi/declutr-small'\n",
    "        self.data_dir = \"../data/processed/\"\n",
    "        self.demography = \"south\"\n",
    "        self.temp = 0.07 # Temperature for softmax\n",
    "        self.max_seq_length = 512\n",
    "        self.learning_rate = 3e-5 \n",
    "        self.adam_epsilon = 1e-6\n",
    "        self.warmup_steps = 0\n",
    "        self.dropout = 0.3\n",
    "        self.weight_decay = 0.01\n",
    "        self.num_train_epochs = 1\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.pad_to_max_length = True\n",
    "        self.batch_size = 32\n",
    "        self.output_dir = '../models/text-classifier-baselines/'\n",
    "        self.overwrite = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "        self.loss1_type = \"CE\"\n",
    "        self.loss2_type = \"SupCon-negatives\"\n",
    "        self.num_hard_negatives = 5\n",
    "        self.nb_epochs = 40\n",
    "        self.coefficient = 1.0\n",
    "        self.pooling = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "seed_everything(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac4e4e7-731b-4a95-9c4a-812135ffd7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dm = HTContraDataModule(file_dir=\"../data/processed/south.csv\", tokenizer_name_or_path=args.tokenizer_name_or_path, seed=1111, train_batch_size=32, eval_batch_size=32)\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "# dm = HTClassifierDataModule(args)\n",
    "# dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca23b53-50f5-498d-b93d-448444a7797d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.num_classes = pd.read_csv(os.path.join(args.data_dir, args.demography + '.csv')).VENDOR.nunique()\n",
    "\n",
    "args.num_training_steps = len(dm.train_dataloader()) * 32\n",
    "# Setting the warmup steps to 1/10th the size of training data\n",
    "args.warmup_steps = int(len(dm.train_dataloader()) * 10/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb7837-74a6-4b4d-b87d-220879c95439",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a3ecb5-4743-4d75-9780-9431c71f02f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HTClassifierModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        if isinstance(args, tuple) and len(args) > 0: \n",
    "            self.args = args[0]\n",
    "            self.hparams.learning_rate = self.args.learning_rate\n",
    "            self.hparams.eps = self.args.adam_epsilon\n",
    "            self.hparams.weight_decay = self.args.weight_decay\n",
    "            self.hparams.model_name_or_path = self.args.model_name_or_path\n",
    "            self.hparams.num_classes = self.args.num_classes\n",
    "            self.hparams.num_training_steps = self.args.num_training_steps\n",
    "            self.hparams.warmup_steps = self.args.warmup_steps\n",
    "        \n",
    "        # freeze\n",
    "        self._frozen = False\n",
    "\n",
    "        # Handling the padding token in distilgpt2 by substituting it with eos_token_id\n",
    "        if self.hparams.model_name_or_path == \"distilgpt2\":\n",
    "            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        else:\n",
    "            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # The batch contains the input_ids, the input_put_mask and the labels (for training)\n",
    "        input_ids = batch[0]\n",
    "        input_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        return outputs, loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class stipulates you to overwrite. This we do here, by virtue of this definition\n",
    "        outputs = self(batch)  # self refers to the model, which in turn acceses the forward method\n",
    "        train_loss = outputs[0]\n",
    "        self.log_dict({\"train_loss\": train_loss, \"learning_rate\":self.hparams.learning_rate}, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "        # the training_step method expects a dictionary, which should at least contain the loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        val_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        # Evaluating the performance\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n",
    "        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n",
    "        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n",
    "        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n",
    "        \n",
    "        self.log_dict({\"val_loss\": val_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy}, \n",
    "                      on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do test. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        test_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        # Evaluating the performance\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n",
    "        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n",
    "        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n",
    "        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n",
    "        \n",
    "        self.log_dict({\"test_loss\": test_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy}, \n",
    "                      on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def predict_step(self, batch, batch_nb):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n",
    "\n",
    "        outputs = self(batch)\n",
    "        # self refers to the model, which in turn accesses the forward method\n",
    "\n",
    "        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n",
    "        # model training has achieved \"in real terms\".\n",
    "        val_loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        return predictions.detach().cpu().numpy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n",
    "        # pl.LightningModule class wants you to overwrite.\n",
    "\n",
    "        # In this case we define that some parameters are optimized in a different way than others. In\n",
    "        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n",
    "        # we do not use an optimization technique called weight decay.\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':self.hparams.weight_decay}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "        # optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.eps)\n",
    "        optimizer = DeepSpeedCPUAdam(optimizer_grouped_parameters, adamw_mode=True, lr=self.hparams.learning_rate, betas=(0.9, 0.999), eps=self.hparams.eps)\n",
    "\n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.num_training_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        # freeze all layers, except the final classifier layers\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'classifier' not in name:  # classifier layer\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self._frozen = True\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        if self._frozen:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'classifier' not in name:  # classifier layer\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        self._frozen = False\n",
    "\n",
    "    def train_epoch_start(self):\n",
    "        \"\"\"pytorch lightning hook\"\"\"\n",
    "        if self.current_epoch < self.hparams.nr_frozen_epochs:\n",
    "            self.freeze()\n",
    "\n",
    "        if self.current_epoch >= self.hparams.nr_frozen_epochs:\n",
    "            self.unfreeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6cfedb-9526-43a3-b16a-86a41034b434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.num_classes = pd.read_csv(\"../data/processed/south.csv\").VENDOR.nunique()\n",
    "args.num_training_steps = len(dm.train_dataloader()) * 32\n",
    "# Setting the warmup steps to 1/10th the size of training data\n",
    "args.warmup_steps = int(len(dm.train_dataloader()) * 10/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d88252-d02f-4b04-94ad-65d9aff817c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.emb_len = 768\n",
    "args.hidden_dim = 512\n",
    "args.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fafbb96-0b1e-4f47-b043-330474e08bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.nb_triplets = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200f58d4-56ad-4527-8ba9-77bf641c59f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model = HTContraClassifierModel.load_from_checkpoint(\"/workspace/persistent/HTClipper/models/grouped-and-masked/text-baselines/contra-learn/declutr-small/south/pooled/seed:1111/lr-0.0001/coeff-1.0/temp:0.1/CE-SupCon-negatives/final_model.ckpt\").eval()\n",
    "\n",
    "# model = HTClassifierModel.load_from_checkpoint(\"/workspace/persistent/HTClipper/models/grouped-and-masked/text-baselines/declutr-small/south/seed:1111/lr-0.0001/final_model.ckpt\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10190575-3c67-4cb1-8985-3364ea359bd0",
   "metadata": {},
   "source": [
    "# Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6eb2d87c-5daf-41d6-8760-44eaa6e6870e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chicago_df = pd.read_csv(\"../data/processed/chicago.csv\")\n",
    "atlanta_df = pd.read_csv(\"../data/processed/atlanta.csv\")\n",
    "dallas_df = pd.read_csv(\"../data/processed/dallas.csv\")\n",
    "detroit_df = pd.read_csv(\"../data/processed/detroit.csv\")\n",
    "houston_df = pd.read_csv(\"../data/processed/houston.csv\")\n",
    "ny_df = pd.read_csv(\"../data/processed/ny.csv\")\n",
    "sf_df = pd.read_csv(\"../data/processed/sf.csv\")\n",
    "canada_df = pd.read_csv(\"../data/processed/canada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbd5467-359b-4630-b1c3-d102650d5d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "south_df = pd.read_csv(\"../data/processed/south.csv\")\n",
    "midwest_df = pd.read_csv(\"../data/processed/midwest.csv\")\n",
    "west_df = pd.read_csv(\"../data/processed/west.csv\")\n",
    "northeast_df = pd.read_csv(\"../data/processed/northeast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea18d44c-a029-45ed-aec7-3b0a27b58168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cc526e-637f-4c52-a046-5053817afbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_of_trained_checkpoints(df, model, tokenizer, city, model_name):\n",
    "    df = df[[\"TEXT\", \"VENDOR\"]].drop_duplicates()\n",
    "    \n",
    "    # Since the vendor IDs are not the current representations of the class labels, we remap these label IDs to avoid falling into out-of-bounds problem\n",
    "    vendors_dict = {}\n",
    "    i = 0\n",
    "    for vendor in df.VENDOR.values.tolist():\n",
    "        if vendor not in vendors_dict.keys():\n",
    "            vendors_dict[vendor] = i\n",
    "            i += 1\n",
    "\n",
    "    df.replace({\"VENDOR\": vendors_dict}, inplace=True)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111)\n",
    "    \n",
    "    embeddings, labels = extract_embeddings(train_df, model, vendors_dict)\n",
    "    directory = os.path.join(os.getcwd(), \"../models/pickled/embeddings/grouped-and-masked\", \"trained_\" + model_name + \"_all\")\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    label_filename = city + \"_labels_train.pt\"\n",
    "    data_filename = city + \"_data_train.pt\"\n",
    "    torch.save(embeddings, os.path.join(directory, data_filename))\n",
    "    torch.save(labels, os.path.join(directory, label_filename))\n",
    "    \n",
    "    embeddings, labels = extract_embeddings(test_df, model, vendors_dict)\n",
    "    label_filename = city + \"_labels_test.pt\"\n",
    "    data_filename = city + \"_data_test.pt\"\n",
    "    torch.save(embeddings, os.path.join(directory, data_filename))\n",
    "    torch.save(labels, os.path.join(directory, label_filename))\n",
    "\n",
    "def extract_embeddings(df, model, vendors_dict, device=\"cpu\", pooling_type=\"mean\"):\n",
    "    text = df.TEXT.values.tolist()\n",
    "    vendors = df.VENDOR.values.tolist()\n",
    "\n",
    "    # Tokenizing the data with padding and truncation\n",
    "    encodings = tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', return_token_type_ids=False, truncation=True, \n",
    "                               return_attention_mask=True, return_tensors='pt') \n",
    "\n",
    "    # Move the encodings to the device\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    labels = torch.tensor(vendors).to(device)\n",
    "\n",
    "    # Combine the inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    pooled_output_list, labels_list = [], []\n",
    "    \n",
    "    pbar = tqdm(total=len(test_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            attention_mask = batch[1]\n",
    "            labels = batch[2]\n",
    "\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Extracting the output from last hidden state\n",
    "            hidden_states = torch.stack(outputs[0][2])[-1]\n",
    "\n",
    "            # Generating the pooled output\n",
    "            if pooling_type == \"mean\":\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                pooled_output = sum_embeddings / sum_mask\n",
    "            elif pooling_type == \"max\":\n",
    "                last_hidden_state = hidden_states\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n",
    "                pooled_output = torch.max(last_hidden_state, 1)[0]\n",
    "            else:\n",
    "                # Mean-max pooling\n",
    "                last_hidden_state = hidden_states\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                mean_pooled_output = sum_embeddings / sum_mask\n",
    "                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n",
    "                max_pooled_output = torch.max(last_hidden_state, 1)[0]\n",
    "                pooled_output = torch.cat((mean_pooled_output, max_pooled_output), 1)\n",
    "\n",
    "            pooled_output_list.append(pooled_output)\n",
    "            labels_list.append(labels)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    # Concatenate the pooled outputs and labels into tensors\n",
    "    pooled_outputs = torch.cat(pooled_output_list)\n",
    "    labels = torch.cat(labels_list)\n",
    "    return pooled_outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7b5f96-5fa2-4a98-ad6d-27ecf8ff2eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353/353 [15:58<00:00,  2.72s/it]\n",
      "100%|██████████| 89/89 [04:03<00:00,  2.74s/it]\n",
      "100%|██████████| 215/215 [09:51<00:00,  2.75s/it]\n",
      "100%|██████████| 54/54 [02:18<00:00,  2.56s/it]\n",
      "100%|██████████| 82/82 [03:36<00:00,  2.64s/it]\n",
      "100%|██████████| 21/21 [00:55<00:00,  2.62s/it]\n",
      "100%|██████████| 65/65 [02:52<00:00,  2.66s/it]\n",
      "100%|██████████| 17/17 [00:43<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_embedding_of_trained_checkpoints(south_df, model, tokenizer, city=\"south\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(midwest_df, model, tokenizer, city=\"midwest\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(west_df, model, tokenizer, city=\"west\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(northeast_df, model, tokenizer, city=\"northeast\", model_name=\"declutr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14264360-459a-4900-b911-b0b9d5b2b1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [07:19<00:00,  2.50s/it]\n",
      "100%|██████████| 44/44 [01:47<00:00,  2.45s/it]\n",
      "100%|██████████| 102/102 [04:27<00:00,  2.62s/it]\n",
      "100%|██████████| 26/26 [01:12<00:00,  2.80s/it]\n",
      "100%|██████████| 39/39 [01:47<00:00,  2.75s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n",
      "100%|██████████| 128/128 [05:36<00:00,  2.63s/it]\n",
      "100%|██████████| 32/32 [01:22<00:00,  2.58s/it]\n",
      "100%|██████████| 124/124 [05:21<00:00,  2.60s/it]\n",
      "100%|██████████| 31/31 [01:21<00:00,  2.63s/it]\n",
      "100%|██████████| 65/65 [02:51<00:00,  2.64s/it]\n",
      "100%|██████████| 17/17 [00:41<00:00,  2.45s/it]\n",
      "100%|██████████| 82/82 [03:28<00:00,  2.54s/it]\n",
      "100%|██████████| 21/21 [00:51<00:00,  2.45s/it]\n",
      "100%|██████████| 29/29 [01:13<00:00,  2.53s/it]\n",
      "100%|██████████| 8/8 [00:17<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_embedding_of_trained_checkpoints(chicago_df, model, tokenizer, city=\"chicago\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(dallas_df, model, tokenizer, city=\"dallas\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(detroit_df, model, tokenizer, city=\"detroit\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(houston_df, model, tokenizer, city=\"houston\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(atlanta_df, model, tokenizer, city=\"atlanta\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(ny_df, model, tokenizer, city=\"ny\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(sf_df, model, tokenizer, city=\"df\", model_name=\"declutr\")\n",
    "extract_embedding_of_trained_checkpoints(canada_df, model, tokenizer, city=\"canada\", model_name=\"declutr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "331fc7ab-234e-44fe-847a-f20423bb1885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [01:16<00:00,  2.63s/it]\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_embedding_of_trained_checkpoints(canada_df, model, tokenizer, city=\"canada\", model_name=\"declutr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "459f8548-90c0-48a1-9b7a-211de425b4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [14:24<00:00,  4.91s/it]\n",
      "100%|██████████| 44/44 [03:22<00:00,  4.60s/it]\n",
      "100%|██████████| 102/102 [07:35<00:00,  4.47s/it]\n",
      "100%|██████████| 26/26 [02:00<00:00,  4.65s/it]\n",
      "100%|██████████| 39/39 [02:57<00:00,  4.56s/it]\n",
      "100%|██████████| 10/10 [00:43<00:00,  4.39s/it]\n",
      "100%|██████████| 128/128 [09:47<00:00,  4.59s/it]\n",
      "100%|██████████| 32/32 [02:22<00:00,  4.45s/it]\n",
      "100%|██████████| 124/124 [09:51<00:00,  4.77s/it]\n",
      "100%|██████████| 31/31 [02:21<00:00,  4.55s/it]\n",
      "100%|██████████| 65/65 [04:53<00:00,  4.51s/it]\n",
      "100%|██████████| 17/17 [01:12<00:00,  4.25s/it]\n",
      "100%|██████████| 82/82 [06:13<00:00,  4.55s/it]\n",
      "100%|██████████| 21/21 [01:29<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_embedding_of_trained_checkpoints(chicago_df, model, tokenizer, city=\"chicago\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(dallas_df, model, tokenizer, city=\"dallas\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(detroit_df, model, tokenizer, city=\"detroit\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(houston_df, model, tokenizer, city=\"houston\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(atlanta_df, model, tokenizer, city=\"atlanta\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(ny_df, model, tokenizer, city=\"ny\", model_name=\"styleEmbedding\")\n",
    "extract_embedding_of_trained_checkpoints(sf_df, model, tokenizer, city=\"df\", model_name=\"styleEmbedding\")\n",
    "# extract_embedding_of_trained_checkpoints(canada_df, model, tokenizer, city=\"canada\", model_name=\"styleEmbedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e0063-4626-42af-b9e6-b5cef6273ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed08-5636-4819-916f-16167c273c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the embeddings from an un-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26cf6da-3818-4605-96ae-99e8697362cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/johngiorgi_declutr-small. Creating a new one with MEAN pooling.\n",
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b8fa7f-7d58-4972-8a8f-214c1d785796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'johngiorgi/declutr-small'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b6a815-29f0-407c-8258-06381a04ccfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "715fbce7-890a-40c1-87a5-e3a3811448b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_of_pretrained_chechpoints(df, model, city, model_name):\n",
    "    df = df[[\"TEXT\", \"VENDOR\"]].drop_duplicates()\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111)\n",
    "    embeddings = model.encode(train_df[\"TEXT\"].to_list())\n",
    "    labels = torch.tensor(train_df.VENDOR.to_list())\n",
    "    assert embeddings.shape[0] == labels.shape[0]\n",
    "    \n",
    "    train_label_filename = \"pretrained_checkpoint_\" + model_name + \"_\" + city + \"_labels_train.pt\"\n",
    "    train_data_filename = \"pretrained_checkpoint_\" + model_name  + \"_\" + city + \"_data_train.pt\"\n",
    "    \n",
    "    torch.save(embeddings, os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/pretrained_declutr\", train_data_filename))\n",
    "    torch.save(labels, os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/pretrained_declutr\", train_label_filename))\n",
    "    \n",
    "    embeddings = model.encode(test_df[\"TEXT\"].to_list())\n",
    "    labels = torch.tensor(test_df.VENDOR.to_list())\n",
    "    assert embeddings.shape[0] == labels.shape[0]\n",
    "    \n",
    "    train_label_filename = \"pretrained_checkpoint_\" + model_name + \"_\" + city + \"_labels_test.pt\"\n",
    "    train_data_filename = \"pretrained_checkpoint_\" + model_name  + \"_\" + city + \"_data_test.pt\"\n",
    "    \n",
    "    torch.save(embeddings, os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/pretrained_declutr\", train_data_filename))\n",
    "    torch.save(labels, os.path.join(\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/pretrained_declutr\" , train_label_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d4d541-51fa-4941-b0d2-d018f59edb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Normalize text\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Convert emojis to text descriptions\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # Replace or remove special characters\n",
    "    # This regex removes non-ASCII characters except basic punctuation\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Replace sequences of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49ed1bac-fbfc-4168-a15b-99c562ae4afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chicago_df = pd.read_csv(\"../data/processed/chicago.csv\")\n",
    "atlanta_df = pd.read_csv(\"../data/processed/atlanta.csv\")\n",
    "dallas_df = pd.read_csv(\"../data/processed/dallas.csv\")\n",
    "detroit_df = pd.read_csv(\"../data/processed/detroit.csv\")\n",
    "houston_df = pd.read_csv(\"../data/processed/houston.csv\")\n",
    "ny_df = pd.read_csv(\"../data/processed/ny.csv\")\n",
    "sf_df = pd.read_csv(\"../data/processed/sf.csv\")\n",
    "canada_df = pd.read_csv(\"../data/processed/canada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f4103ff-de15-46d2-adcb-5281da668373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "south_df = pd.read_csv(\"../data/processed/south.csv\")\n",
    "midwest_df = pd.read_csv(\"../data/processed/midwest.csv\")\n",
    "west_df = pd.read_csv(\"../data/processed/west.csv\")\n",
    "northeast_df = pd.read_csv(\"../data/processed/northeast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8067bc7-d07c-4a3f-b81c-6dbc9b559f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada_df[\"TEXT\"] = canada_df[\"TEXT\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efe849bb-2e69-4c75-a2df-485bd80fa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_embedding_of_pretrained_chechpoints(chicago_df, model, \"chicago\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(atlanta_df, model, \"atlanta\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(dallas_df, model, \"dallas\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(detroit_df, model, \"detroit\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(houston_df, model, \"houston\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(ny_df, model, \"NY\", \"styleEmbedding\")\n",
    "extract_embedding_of_pretrained_chechpoints(sf_df, model, \"SF\", \"styleEmbedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6696c59-3d59-4db2-94ea-a85db65e840e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_embedding_of_pretrained_chechpoints(canada_df, model, \"canada\", \"declutr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82f05484-2806-4025-8f25-f5b0e2ece9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_embedding_of_pretrained_chechpoints(south_df, model, \"south\", \"declutr\")\n",
    "extract_embedding_of_pretrained_chechpoints(midwest_df, model, \"midwest\", \"declutr\")\n",
    "extract_embedding_of_pretrained_chechpoints(west_df, model, \"west\", \"declutr\")\n",
    "extract_embedding_of_pretrained_chechpoints(northeast_df, model, \"northeast\", \"declutr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
