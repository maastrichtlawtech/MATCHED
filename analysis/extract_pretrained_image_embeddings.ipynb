{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5fd4553-f346-4977-89d3-e4a9feb01929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/HT/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% Loading libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import timm\n",
    "from timm import create_model\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from imageUtilities import load_images_and_labels\n",
    "from loadData import ImageDataModule\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from visionClassifierLayer import PreTrainedVisionModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2bace8-dec1-42b5-834c-a35c36f063ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate the command-line arguments\n",
    "args_list = [\n",
    "    '--model_name_or_path', 'vit_base_patch16_224',\n",
    "    '--logged_entry_name', 'vgg16-seed:1111',\n",
    "    '--data_dir', '/workspace/persistent/HTClipper/data/processed',\n",
    "    '--data_type', 'all',\n",
    "    '--city', 'south',\n",
    "    '--save_dir', os.path.join(os.getcwd(), \"../models/image-baselines\"),\n",
    "    '--batch_size', '32',\n",
    "    '--nb_epochs', '40',\n",
    "    '--patience', '3',\n",
    "    '--seed', '1111',\n",
    "    '--warmup_steps', '0',\n",
    "    '--grad_steps', '4',\n",
    "    '--learning_rate', '6e-4',\n",
    "    '--train_data_percentage', '1.0',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--min_delta_change', '0.01',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--augment_data', 'False',\n",
    "    '--nb_augmented_samples', '5'\n",
    "]\n",
    "\n",
    "# Create the argument parser\n",
    "parser = argparse.ArgumentParser(description=\"Trains a image classifier to establish baselines for Authorship tasks on Backpage advertisements.\")\n",
    "parser.add_argument('--model_name_or_path', type=str, default=\"vit_base_patch16_224\", help=\"Name of the model to be trained (can only be between distilbert-base-cased)\")\n",
    "parser.add_argument('--logged_entry_name', type=str, default=\"vgg16-seed:1111\", help=\"Logged entry name visible on weights and biases\")\n",
    "parser.add_argument('--data_dir', type=str, default='/workspace/persistent/HTClipper/data/processed', help=\"\"\"Data directory\"\"\")\n",
    "parser.add_argument('--city', type=str, default='south', help=\"\"\"Demography of data, can be only between chicago, atlanta, houston, dallas, detroit, ny, or sf\"\"\")\n",
    "parser.add_argument('--data_type', type=str, default=\"all\", help=\"can be faces for the dataset with human faces or nofaces for body parts dataset\")\n",
    "parser.add_argument('--save_dir', type=str, default=os.path.join(os.getcwd(), \"../models/image-baselines\"), help=\"\"\"Directory for models to be saved\"\"\")\n",
    "parser.add_argument('--batch_size', type=int, default=32, help=\"Batch Size\")\n",
    "parser.add_argument('--nb_epochs', type=int, default=40, help=\"Number of Epochs\")\n",
    "parser.add_argument('--patience', type=int, default=3, help=\"Patience for Early Stopping\")\n",
    "parser.add_argument('--seed', type=int, default=1111, help='Random seed value')\n",
    "parser.add_argument('--warmup_steps', type=int, default=0, help=\"Warmup proportion\")\n",
    "parser.add_argument('--grad_steps', type=int, default=4, help=\"Gradient accumulating step\")\n",
    "parser.add_argument('--learning_rate', type=float, default=6e-4, help=\"learning rate\")\n",
    "parser.add_argument('--train_data_percentage', type=float, default=1.0, help=\"Percentage of training data to be used\")\n",
    "parser.add_argument('--adam_epsilon', type=float, default=1e-6, help=\"Epsilon value for adam optimizer\")\n",
    "parser.add_argument('--min_delta_change', type=float, default=0.01, help=\"Minimum change in delta in validation loss for Early Stopping\")\n",
    "parser.add_argument('--weight_decay', type=float, default=0.01, help=\"Weight decay\")\n",
    "parser.add_argument('--augment_data', type=bool, default=False, help='Enables data augmentation')\n",
    "parser.add_argument('--nb_augmented_samples', type=int, default=5, help='Number of augmented samples to be generated')\n",
    "\n",
    "# Parse the arguments using the simulated args_list\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564ea9dc-7a58-4eea-b90a-207da99bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting seed value for reproducibility    \n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd49863b-6bd0-4c63-83ed-c89c3b62ae7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making sure that the input variables are right\n",
    "assert args.data_type in [\"faces\", \"nofaces\", \"all\"]\n",
    "assert args.model_name_or_path in ['vgg16', 'vgg19', \"resnet50\", \"resnet101\", \"resnet152\", \"mobilenet\", \"mobilenetv2\", \"densenet121\", \"densenet169\", \n",
    "                                \"efficientnet-b0\", \"efficientnet-b1\", \"efficientnet-b2\", \"efficientnet-b3\", \"efficientnet-b4\", \"efficientnet-b5\", \"efficientnet-b6\",\n",
    "                                \"efficientnet-b7\", \"efficientnetv2_rw_m\", \"efficientnetv2_rw_s\", \"efficientnetv2_rw_t\", \"convnext_tiny\", \"convnext_small\", \n",
    "                                \"convnext_base\", \"convnext_large\", \"convnext_xlarge\", \"vit_base_patch16_224\", \"vit_large_patch16_224\", \"vit_base_patch32_224\", \n",
    "                                \"vit_large_patch32_224\", \"inception_v3\", \"inception_resnet_v2\" ]\n",
    "\n",
    "# Creating directories\n",
    "directory = os.path.join(args.save_dir, args.model_name_or_path.split(\"/\")[-1], args.city, args.data_type, \n",
    "                        \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate))\n",
    "Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbffdd6e-9477-4113-b691-840abeb329e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract cls_token embeddings and labels from the model\n",
    "def extract_cls_embeddings_and_labels_from_vit(model, dataloader):\n",
    "    cls_embeddings = []\n",
    "    all_labels = []\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)  # Transfer images to the device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward_features(images)\n",
    "            cls_token_embeddings = outputs[:, 0, :]  # Extract cls_token embeddings\n",
    "\n",
    "        cls_embeddings.append(cls_token_embeddings.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate the embeddings and labels along the first dimension\n",
    "    cls_embeddings = torch.cat(cls_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    return cls_embeddings.numpy(), all_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69cf8e0-6f9b-4de3-af1b-81ddd95790d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract embeddings and labels (assuming this is defined somewhere else)\n",
    "def extract_cls_embeddings_and_labels_from_convnext(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            embeddings.append(outputs.cpu())\n",
    "            labels.append(label.cpu())\n",
    "\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    labels = torch.cat(labels)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a96715-7591-4094-bd58-210952365241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to map cities and data types to their respective file paths\n",
    "file_paths = {\n",
    "    \"chicago\": {\n",
    "        \"faces\": \"chicago_faces.csv\",\n",
    "        \"nofaces\": \"chicago_nofaces.csv\",\n",
    "        \"all\": \"chicago_images.csv\"\n",
    "    },\n",
    "    \"dallas\": {\n",
    "        \"faces\": \"dallas_faces.csv\",\n",
    "        \"nofaces\": \"dallas_nofaces.csv\",\n",
    "        \"all\": \"dallas_images.csv\"\n",
    "    },\n",
    "    \"houston\": {\n",
    "        \"faces\": \"houston_faces.csv\",\n",
    "        \"nofaces\": \"houston_nofaces.csv\",\n",
    "        \"all\": \"houston_images.csv\"\n",
    "    },\n",
    "    \"detroit\": {\n",
    "        \"faces\": \"detroit_faces.csv\",\n",
    "        \"nofaces\": \"detroit_nofaces.csv\",\n",
    "        \"all\": \"detroit_images.csv\"\n",
    "    },\n",
    "    \"atlanta\": {\n",
    "        \"faces\": \"atlanta_faces.csv\",\n",
    "        \"nofaces\": \"atlanta_nofaces.csv\",\n",
    "        \"all\": \"atlanta_images.csv\"\n",
    "    },\n",
    "    \"sf\": {\n",
    "        \"faces\": \"sf_faces.csv\",\n",
    "        \"nofaces\": \"sf_nofaces.csv\",\n",
    "        \"all\": \"sf_images.csv\"\n",
    "    },\n",
    "    \"ny\": {\n",
    "        \"faces\": \"ny_faces.csv\",\n",
    "        \"nofaces\": \"ny_nofaces.csv\",\n",
    "        \"all\": \"ny_images.csv\"\n",
    "    },\n",
    "    \"south\" : {\n",
    "        \"all\": \"south_images.csv\"\n",
    "    },\n",
    "    \"midwest\" : {\n",
    "        \"all\": \"midwest_images.csv\"\n",
    "    },\n",
    "    \"west\" : {\n",
    "        \"all\": \"west_images.csv\"\n",
    "    },\n",
    "    \"northeast\" : {\n",
    "        \"all\": \"northeast_images.csv\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def generate_embeddings(model, model_name):\n",
    "    all_cities = [\"south\", \"midwest\", \"west\", \"northeast\"]\n",
    "    \n",
    "    for city in tqdm(all_cities):\n",
    "        for data_type in tqdm([\"all\"], leave=False):\n",
    "            tqdm.write(f\"Processing {city} - {data_type}\")  # Print the current city and data type being processed\n",
    "            # Construct the file path and read the CSV file\n",
    "            file_path = os.path.join(args.data_dir, file_paths[city][data_type])\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Remove vendors that have less than 2 ads\n",
    "            vendors_of_interest = {k: v for k, v in Counter(df.VENDOR).items() if v > 1}\n",
    "            df = df[df['VENDOR'].isin(vendors_of_interest.keys())]\n",
    "\n",
    "            # Remap new vendor IDs\n",
    "            all_vendors = df.VENDOR.unique()\n",
    "            vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "            df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "            # Load and preprocess images\n",
    "            images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                                    num_augmented_samples=args.nb_augmented_samples)\n",
    "            assert images.shape[0] == labels.shape[0]\n",
    "\n",
    "            # Split data into training and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111)\n",
    "\n",
    "            # Instantiate DataModule and Model\n",
    "            num_classes = df.VENDOR.nunique()\n",
    "            data_module = ImageDataModule(X_train, y_train, X_test, y_test, X_test, y_test, \n",
    "                                          batch_size=args.batch_size, augment_data=False)\n",
    "\n",
    "            # Setup the data module for training/validation and testing\n",
    "            data_module.setup('fit')\n",
    "            data_module.setup('test')\n",
    "\n",
    "            # Extract embeddings and labels\n",
    "            if model_name == \"pretrained_vit_patch16\":\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.test_dataloader())\n",
    "            \n",
    "            else:\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.test_dataloader())\n",
    "\n",
    "            assert train_embeddings.shape[0] == train_labels.shape[0]\n",
    "            assert test_embeddings.shape[0] == test_labels.shape[0]\n",
    "\n",
    "            # Save the embeddings and labels to disk\n",
    "            base_path = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/\" + model_name\n",
    "            torch.save(train_embeddings, os.path.join(base_path, f\"{model_name}_{city}_{data_type}_train_embeddings.pt\"))\n",
    "            torch.save(train_labels, os.path.join(base_path, f\"{model_name}_{city}_{data_type}_train_labels.pt\"))\n",
    "            torch.save(test_embeddings, os.path.join(base_path, f\"{model_name}_{city}_{data_type}_test_embeddings.pt\"))\n",
    "            torch.save(test_labels, os.path.join(base_path, f\"{model_name}_{city}_{data_type}_test_labels.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ed32a-266d-4aa3-ad00-7b816a1cf397",
   "metadata": {},
   "source": [
    "# ViT Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6532619-e905-4eb2-b320-88f2e6d7f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Load the ViT model\n",
    "model = timm.create_model(\n",
    "    'vit_base_patch16_224',\n",
    "    pretrained=True,\n",
    "    num_classes=0  # remove classifier nn.Linear\n",
    ").cuda()\n",
    "model = model.eval()\n",
    "\n",
    "# Get model-specific transforms\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b513ba-fa5f-4921-942b-d05100cae445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing south - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [02:02<00:00, 122.76s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [02:02<06:08, 122.76s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 25%|██▌       | 1/4 [02:02<06:08, 122.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing midwest - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:04<00:00, 64.39s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [03:07<02:56, 88.43s/it] [A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                             \n",
      " 50%|█████     | 2/4 [03:07<02:56, 88.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing west - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.39s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [03:33<01:00, 60.11s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                             \n",
      " 75%|███████▌  | 3/4 [03:33<01:00, 60.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing northeast - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.69s/it]\u001b[A\n",
      "100%|██████████| 4/4 [04:01<00:00, 60.32s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(model, \"pretrained_vit_patch16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb964bc8-2076-4131-af93-d2d01428cd7c",
   "metadata": {},
   "source": [
    "# Loading the Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5633e-e69d-4598-87bc-a996e90c87c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ConvNext Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20baf12d-7418-4d69-9cb2-1c4ed2354633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/convnext_small.in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/convnext_small.in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Load the ViT model\n",
    "model = timm.create_model(\n",
    "    'convnext_small',\n",
    "    pretrained=True,\n",
    "    num_classes=0  # remove classifier nn.Linear\n",
    ").cuda()\n",
    "model = model.eval()\n",
    "\n",
    "# Get model-specific transforms\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1a1804-a964-4b32-9917-4d40521c07a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "                                     \u001b[A\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chicago - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     .56s/it]\u001b[A\n",
      "  0%|          | 0/7 [00:24<?, ?it/s]        \n",
      " 33%|███▎      | 1/3 [00:24<00:49, 24.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chicago - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     .62s/it]\u001b[A\n",
      "  0%|          | 0/7 [00:42<?, ?it/s]        \n",
      " 67%|██████▋   | 2/3 [00:42<00:20, 20.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chicago - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [01:25<00:00, 31.02s/it]\u001b[A\n",
      " 14%|█▍        | 1/7 [01:25<08:34, 85.82s/it]\u001b[A\n",
      "                                             \n",
      " 14%|█▍        | 1/7 [01:25<08:34, 85.82s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dallas - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 14%|█▍        | 1/7 [01:42<08:34, 85.82s/it]\n",
      " 33%|███▎      | 1/3 [00:16<00:33, 16.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dallas - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 14%|█▍        | 1/7 [01:57<08:34, 85.82s/it]\n",
      " 67%|██████▋   | 2/3 [00:31<00:15, 15.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dallas - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [01:01<00:00, 22.01s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [02:27<05:56, 71.36s/it]\u001b[A\n",
      "                                             \n",
      " 29%|██▊       | 2/7 [02:27<05:56, 71.36s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing houston - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 29%|██▊       | 2/7 [02:48<05:56, 71.36s/it]\n",
      " 33%|███▎      | 1/3 [00:21<00:42, 21.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing houston - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 29%|██▊       | 2/7 [03:07<05:56, 71.36s/it]\n",
      " 67%|██████▋   | 2/3 [00:40<00:20, 20.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing houston - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [01:20<00:00, 28.96s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [03:47<05:01, 75.31s/it]\u001b[A\n",
      "                                             \n",
      " 43%|████▎     | 3/7 [03:47<05:01, 75.31s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing detroit - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 43%|████▎     | 3/7 [03:52<05:01, 75.31s/it]\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing detroit - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 43%|████▎     | 3/7 [03:59<05:01, 75.31s/it]\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing detroit - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:23<00:00,  8.60s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [04:10<02:44, 54.95s/it]\u001b[A\n",
      "                                             \n",
      " 57%|█████▋    | 4/7 [04:10<02:44, 54.95s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing atlanta - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 57%|█████▋    | 4/7 [04:25<02:44, 54.95s/it]\n",
      " 33%|███▎      | 1/3 [00:14<00:29, 14.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing atlanta - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 57%|█████▋    | 4/7 [04:43<02:44, 54.95s/it]\n",
      " 67%|██████▋   | 2/3 [00:32<00:16, 16.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing atlanta - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [01:03<00:00, 23.33s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [05:14<01:56, 58.22s/it]\u001b[A\n",
      "                                             \n",
      " 71%|███████▏  | 5/7 [05:14<01:56, 58.22s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sf - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 71%|███████▏  | 5/7 [05:28<01:56, 58.22s/it]\n",
      " 33%|███▎      | 1/3 [00:13<00:26, 13.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sf - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 71%|███████▏  | 5/7 [05:38<01:56, 58.22s/it]\n",
      " 67%|██████▋   | 2/3 [00:23<00:11, 11.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sf - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:44<00:00, 15.99s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [05:59<00:53, 53.66s/it]\u001b[A\n",
      "                                             \n",
      " 86%|████████▌ | 6/7 [05:59<00:53, 53.66s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 86%|████████▌ | 6/7 [06:14<00:53, 53.66s/it]\n",
      " 33%|███▎      | 1/3 [00:14<00:28, 14.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                             \u001b[A\n",
      " 86%|████████▌ | 6/7 [06:23<00:53, 53.66s/it]\n",
      " 67%|██████▋   | 2/3 [00:23<00:11, 11.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:47<00:00, 17.18s/it]\u001b[A\n",
      "100%|██████████| 7/7 [06:47<00:00, 58.20s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "a = generate_embeddings(model, \"pretrained_convNext-s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52129c63-6ccb-431d-bb7e-0f6aacd7340d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trained models on chicago dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb46293-3bbf-432d-a215-a08b9923a271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define global lists for capturing hidden states\n",
    "hidden_states = []\n",
    "hidden_states_vit = []\n",
    "\n",
    "# Function to load the model from local directory and adjust state_dict keys\n",
    "def load_model(model_name, checkpoint_path, num_classes=1000):\n",
    "    model = create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Adjust the keys in the state_dict\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        # Remove 'model.' prefix from keys if present\n",
    "        if k.startswith('model.'):\n",
    "            k = k[6:]\n",
    "        new_state_dict[k] = v\n",
    "    \n",
    "    # Modify the final fully connected layer to match the checkpoint's layer size\n",
    "    if 'head.fc.weight' in new_state_dict:\n",
    "        num_classes_checkpoint = new_state_dict['head.fc.weight'].shape[0]\n",
    "        model.head.fc = torch.nn.Linear(model.head.fc.in_features, num_classes_checkpoint)\n",
    "    elif 'head.weight' in new_state_dict:\n",
    "        num_classes_checkpoint = new_state_dict['head.weight'].shape[0]\n",
    "        model.head = torch.nn.Linear(model.head.in_features, num_classes_checkpoint)\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Hook function to capture hidden states\n",
    "def get_hidden_states(module, input, output):\n",
    "    hidden_states.append(output)\n",
    "\n",
    "# Hook function to capture hidden states for ViT model\n",
    "def get_vit_hidden_states(module, input, output):\n",
    "    hidden_states_vit.append(output)\n",
    "    \n",
    "# Function to extract embeddings and labels from ConvNext model\n",
    "def extract_cls_embeddings_and_labels_from_convnext(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        hidden_states.clear()  # Clear hidden states for each batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            if hidden_states:\n",
    "                last_hidden_state = hidden_states[-1]\n",
    "                if last_hidden_state.ndim == 4:\n",
    "                    mean_pooled = torch.mean(last_hidden_state, dim=[2, 3])  # Mean pooling over spatial dimensions\n",
    "                else:\n",
    "                    mean_pooled = last_hidden_state  # If already flattened, use directly\n",
    "            else:\n",
    "                mean_pooled = outputs\n",
    "            embeddings.append(mean_pooled.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "    return torch.cat(embeddings), torch.cat(labels)\n",
    "\n",
    "# Function to extract embeddings and labels from Vision Transformer model\n",
    "def extract_cls_embeddings_and_labels_from_vit(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        hidden_states_vit.clear()  # Clear hidden states for each batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if hidden_states_vit:\n",
    "                last_hidden_state = hidden_states_vit[-1]\n",
    "                cls_token_embedding = last_hidden_state[:, 0, :]  # CLS token is the first token\n",
    "            else:\n",
    "                cls_token_embedding = outputs[:, 0, :]\n",
    "            embeddings.append(cls_token_embedding.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "    return torch.cat(embeddings), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b43f9f-c59a-47e5-ab96-d2f4e58b2b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to map cities and data types to their respective file paths\n",
    "file_paths = {\n",
    "    \"chicago\": {\n",
    "        \"faces\": \"chicago_faces.csv\",\n",
    "        \"nofaces\": \"chicago_nofaces.csv\",\n",
    "        \"all\": \"chicago_images.csv\"\n",
    "    },\n",
    "    \"dallas\": {\n",
    "        \"faces\": \"dallas_faces.csv\",\n",
    "        \"nofaces\": \"dallas_nofaces.csv\",\n",
    "        \"all\": \"dallas_images.csv\"\n",
    "    },\n",
    "    \"houston\": {\n",
    "        \"faces\": \"houston_faces.csv\",\n",
    "        \"nofaces\": \"houston_nofaces.csv\",\n",
    "        \"all\": \"houston_images.csv\"\n",
    "    },\n",
    "    \"detroit\": {\n",
    "        \"faces\": \"detroit_faces.csv\",\n",
    "        \"nofaces\": \"detroit_nofaces.csv\",\n",
    "        \"all\": \"detroit_images.csv\"\n",
    "    },\n",
    "    \"atlanta\": {\n",
    "        \"faces\": \"atlanta_faces.csv\",\n",
    "        \"nofaces\": \"atlanta_nofaces.csv\",\n",
    "        \"all\": \"atlanta_images.csv\"\n",
    "    },\n",
    "    \"sf\": {\n",
    "        \"faces\": \"sf_faces.csv\",\n",
    "        \"nofaces\": \"sf_nofaces.csv\",\n",
    "        \"all\": \"sf_images.csv\"\n",
    "    },\n",
    "    \"ny\": {\n",
    "        \"faces\": \"ny_faces.csv\",\n",
    "        \"nofaces\": \"ny_nofaces.csv\",\n",
    "        \"all\": \"ny_images.csv\"\n",
    "    },\n",
    "    \"south\" : {\n",
    "        \"all\": \"south_images.csv\"\n",
    "    },\n",
    "    \"midwest\" : {\n",
    "        \"all\": \"midwest_images.csv\"\n",
    "    },\n",
    "    \"west\" : {\n",
    "        \"all\": \"west_images.csv\"\n",
    "    },\n",
    "    \"northeast\" : {\n",
    "        \"all\": \"northeast_images.csv\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def generate_embeddings(model, model_save_dir, trained_on):\n",
    "    # all_cities = [\"chicago\", \"dallas\", \"houston\", \"detroit\", \"atlanta\", \"sf\", \"ny\"]\n",
    "    all_cities = [\"south\", \"midwest\", \"west\", \"northeast\"]\n",
    "    # all_cities = [\"ny\"]\n",
    "    if trained_on == \"faces\":\n",
    "        data_types = [\"faces\"]\n",
    "    elif trained_on == \"nofaces\":\n",
    "        data_types = [\"nofaces\"]\n",
    "    elif trained_on == \"all\":\n",
    "        # data_types = [\"faces\", \"nofaces\", \"all\"]\n",
    "        data_types = [\"all\"]\n",
    "    else:\n",
    "        raise Exception(\"Implementation only carried out for faces, nofaces, and all datasets.\")\n",
    "    \n",
    "    for city in tqdm(all_cities):\n",
    "        for data_type in tqdm(data_types, leave=False):\n",
    "            tqdm.write(f\"Processing {city} - {data_type}\")  # Print the current city and data type being processed\n",
    "            # Construct the file path and read the CSV file\n",
    "            file_path = os.path.join(args.data_dir, file_paths[city][data_type])\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Remove vendors that have less than 2 ads\n",
    "            vendors_of_interest = {k: v for k, v in Counter(df.VENDOR).items() if v > 1}\n",
    "            df = df[df['VENDOR'].isin(vendors_of_interest.keys())]\n",
    "\n",
    "            # Remap new vendor IDs\n",
    "            all_vendors = df.VENDOR.unique()\n",
    "            vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "            df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "            # Load and preprocess images\n",
    "            images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                                    num_augmented_samples=args.nb_augmented_samples)\n",
    "            assert images.shape[0] == labels.shape[0]\n",
    "\n",
    "            # Split data into training and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111)\n",
    "\n",
    "            # Instantiate DataModule and Model\n",
    "            num_classes = df.VENDOR.nunique()\n",
    "            data_module = ImageDataModule(X_train, y_train, X_test, y_test, X_test, y_test, \n",
    "                                          batch_size=args.batch_size, augment_data=False)\n",
    "\n",
    "            # Setup the data module for training/validation and testing\n",
    "            data_module.setup('fit')\n",
    "            data_module.setup('test')\n",
    "            \n",
    "            # Extract embeddings and labels\n",
    "            hidden_states.clear()  # Clear previous hooks\n",
    "            hidden_states_vit.clear()  # Clear previous hooks\n",
    "\n",
    "            # Extract embeddings and labels\n",
    "            if model_save_dir in [\"trained_vit_patch16_chicago\", \"trained_contra_vit_patch16_all\", \"trained_contra_vit_patch16_chicago\", \n",
    "                                  \"trained_vit_contraonly_chicago\", \"trained_vit_contraonly_all\", \"trained_vit_contraonly_chicago_temp:0.3\", \n",
    "                                 \"trained_vit_contramixed\", \"trained_vit_contraonly\", \"trained_vit_patch16\", \"trained_vit_contramix_CE+triplets\", \"trained_vit_contramix_triplets\"]:\n",
    "                model.blocks[-1].register_forward_hook(get_vit_hidden_states)\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.test_dataloader())\n",
    "            \n",
    "            elif model_save_dir == \"trained_convNext-s_all\" or model_save_dir == \"trained_convNext-s_all/all\":\n",
    "                for name, layer in model.named_modules():\n",
    "                    if isinstance(layer, torch.nn.Sequential):\n",
    "                        layer.register_forward_hook(get_hidden_states)\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.test_dataloader())\n",
    "            else:\n",
    "                raise Exception(\"Script to be extended for other models. \")\n",
    "                \n",
    "            assert train_embeddings.shape[0] == train_labels.shape[0]\n",
    "            assert test_embeddings.shape[0] == test_labels.shape[0]\n",
    "\n",
    "            # Save the embeddings and labels to disk\n",
    "            base_path = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/\" + model_save_dir + \"/\" + data_type\n",
    "            # /workspace/persistent/HTClipper/models/pickled/embeddings/image_embeddings/trained_convNext-s_all/all/all\n",
    "            os.makedirs(base_path, exist_ok=True)\n",
    "            \n",
    "            # model_save_dir1 = \"trained_vit_patch16__all\"\n",
    "            \n",
    "            torch.save(train_embeddings, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_train_embeddings.pt\"))\n",
    "            torch.save(train_labels, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_train_labels.pt\"))\n",
    "            torch.save(test_embeddings, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_test_embeddings.pt\"))\n",
    "            torch.save(test_labels, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_test_labels.pt\"))\n",
    "            \n",
    "            \n",
    "def generate_embeddings_for_semi_supervised_models(model, model_save_dir, trained_on):\n",
    "    # all_cities = [\"chicago\", \"dallas\", \"houston\", \"detroit\", \"atlanta\", \"sf\", \"ny\"]\n",
    "    all_cities = [\"south\", \"midwest\", \"west\", \"northeast\"]\n",
    "    # all_cities = [\"ny\"]\n",
    "    if trained_on == \"faces\":\n",
    "        data_types = [\"faces\"]\n",
    "    elif trained_on == \"nofaces\":\n",
    "        data_types = [\"nofaces\"]\n",
    "    elif trained_on == \"all\":\n",
    "        # data_types = [\"faces\", \"nofaces\", \"all\"]\n",
    "        data_types = [\"all\"]\n",
    "    else:\n",
    "        raise Exception(\"Implementation only carried out for faces, nofaces, and all datasets.\")\n",
    "    \n",
    "    for city in tqdm(all_cities):\n",
    "        for data_type in tqdm(data_types, leave=False):\n",
    "            tqdm.write(f\"Processing {city} - {data_type}\")  # Print the current city and data type being processed\n",
    "            # Construct the file path and read the CSV file\n",
    "            file_path = os.path.join(args.data_dir, file_paths[city][data_type])\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Remove vendors that have less than 2 ads\n",
    "            vendors_of_interest = {k: v for k, v in Counter(df.VENDOR).items() if v > 1}\n",
    "            df = df[df['VENDOR'].isin(vendors_of_interest.keys())]\n",
    "\n",
    "            # Remap new vendor IDs\n",
    "            all_vendors = df.VENDOR.unique()\n",
    "            vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "            df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "            # Load and preprocess images\n",
    "            images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                                    num_augmented_samples=args.nb_augmented_samples)\n",
    "            assert images.shape[0] == labels.shape[0]\n",
    "\n",
    "            # Split data into training and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111)\n",
    "\n",
    "            # Instantiate DataModule and Model\n",
    "            num_classes = df.VENDOR.nunique()\n",
    "            data_module = ImageDataModule(X_train, y_train, X_test, y_test, X_test, y_test, \n",
    "                                          batch_size=args.batch_size, augment_data=False)\n",
    "\n",
    "            # Setup the data module for training/validation and testing\n",
    "            data_module.setup('fit')\n",
    "            data_module.setup('test')\n",
    "            \n",
    "            # Extract embeddings and labels\n",
    "            hidden_states.clear()  # Clear previous hooks\n",
    "            hidden_states_vit.clear()  # Clear previous hooks\n",
    "\n",
    "            # Extract embeddings and labels\n",
    "            if model_save_dir in [\"trained_vit_patch16_chicago\", \"trained_contra_vit_patch16_all\", \"trained_contra_vit_patch16_chicago\", \n",
    "                                  \"trained_vit_contraonly_chicago\", \"trained_vit_contraonly_all\", \"trained_vit_contraonly_chicago_temp:0.3\", \n",
    "                                 \"trained_vit_contramixed\", \"trained_vit_contraonly\", \"trained_vit_patch16\", \"trained_vit_contramix_CE+triplets\", \n",
    "                                  \"trained_vit_contramix_triplets\", \"trained_vit_tripletonly\"]:\n",
    "                model.model.blocks[-1].register_forward_hook(get_vit_hidden_states)\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.test_dataloader())\n",
    "            \n",
    "            elif model_save_dir == \"trained_convNext-s_all\" or model_save_dir == \"trained_convNext-s_all/all\":\n",
    "                for name, layer in model.named_modules():\n",
    "                    if isinstance(layer, torch.nn.Sequential):\n",
    "                        layer.register_forward_hook(get_hidden_states)\n",
    "                train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.train_dataloader()) \n",
    "                test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_convnext(model, data_module.test_dataloader())\n",
    "            else:\n",
    "                raise Exception(\"Script to be extended for other models. \")\n",
    "                \n",
    "            assert train_embeddings.shape[0] == train_labels.shape[0]\n",
    "            assert test_embeddings.shape[0] == test_labels.shape[0]\n",
    "\n",
    "            # Save the embeddings and labels to disk\n",
    "            base_path = \"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/\" + model_save_dir + \"/\" + data_type\n",
    "            # /workspace/persistent/HTClipper/models/pickled/embeddings/image_embeddings/trained_convNext-s_all/all/all\n",
    "            os.makedirs(base_path, exist_ok=True)\n",
    "            \n",
    "            # model_save_dir1 = \"trained_vit_patch16__all\"\n",
    "            \n",
    "            torch.save(train_embeddings, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_train_embeddings.pt\"))\n",
    "            torch.save(train_labels, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_train_labels.pt\"))\n",
    "            torch.save(test_embeddings, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_test_embeddings.pt\"))\n",
    "            torch.save(test_labels, os.path.join(base_path, f\"{model_save_dir}_{city}_{data_type}_test_labels.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eb41774-a96b-4b53-8095-6f0f371144f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained models\n",
    "# Generate embeddings for both models\n",
    "# model = load_model('convnext_small', '/workspace/persistent/HTClipper/models/image-baselines/convnext_small/all/faces/seed:1111/lr-0.0001-all-FacesImages/final_model.ckpt')\n",
    "# generate_embeddings(model, \"trained_convNext-s_all\", \"faces\")\n",
    "\n",
    "# model = load_model('convnext_small', '/workspace/persistent/HTClipper/models/image-baselines/convnext_small/all/nofaces/seed:1111/lr-0.0001-all-NoFacesImages/final_model.ckpt')\n",
    "# generate_embeddings(model, \"trained_convNext-s_all\", \"nofaces\")\n",
    "\n",
    "# model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/image-baselines/vit_base_patch16_224/chicago/faces/seed:1111/lr-0.0001/final_model.ckpt')\n",
    "# generate_embeddings(model, \"trained_vit_patch16_chicago\", \"faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dac556-b3ca-46d2-afbc-90478664358b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing south - all\n"
     ]
    }
   ],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-CE+SupCon/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_patch16\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e07a96-9442-4a10-945c-158e23eb3ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "                                     \u001b[A\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing south - all\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../architectures/')\n",
    "from visionContraLayer import SemiConstrativeVisionModel\n",
    "model = SemiConstrativeVisionModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/semi-supervised/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-SupCon/final_model.ckpt\",\n",
    "    model_name=\"vit_base_patch16_224\",  # Pass other required arguments as needed\n",
    "    num_training_steps=200,\n",
    ")\n",
    "generate_embeddings_for_semi_supervised_models(model, \"trained_vit_contraonly\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bc843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"head.weight\", \"head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_base_patch16_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/semi-supervised/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-SupCon/final_model.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m generate_embeddings(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_vit_contraonly\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, checkpoint_path, num_classes)\u001b[0m\n\u001b[1;32m     28\u001b[0m     num_classes_checkpoint \u001b[38;5;241m=\u001b[39m new_state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39min_features, num_classes_checkpoint)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/workspace/persistent/yes/envs/HT/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"head.weight\", \"head.bias\". "
     ]
    }
   ],
   "source": [
    "model = SemiConstrativeVisionModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/semi-supervised/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-triplet/final_model.ckpt\",\n",
    "    model_name=\"vit_base_patch16_224\",  # Pass other required arguments as needed\n",
    "    num_training_steps=200,\n",
    ")\n",
    "generate_embeddings_for_semi_supervised_models(model, \"trained_vit_tripletonly\", \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c08f0-707f-4ea6-82f5-fd8c1b25cc02",
   "metadata": {},
   "source": [
    "contrastive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05149b91-2ba3-4591-abd5-d016584d307b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-CE+triplet/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_contramix_CE+triplets\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b9913-bc24-493c-ac75-b2bd8f786599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-triplet/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_contramix_triplets\", \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103bc06-b2b5-4a17-af65-b5936a60286d",
   "metadata": {},
   "source": [
    "# ViT model trained on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a51f28-fd0e-4d4d-b1c5-2c399e07b4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chicago - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [08:03<00:00, 483.09s/it]\u001b[A\n",
      " 14%|█▍        | 1/7 [08:03<48:18, 483.10s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 14%|█▍        | 1/7 [08:03<48:18, 483.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dallas - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [03:00<00:00, 180.63s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [11:03<25:25, 305.18s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 29%|██▊       | 2/7 [11:03<25:25, 305.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing houston - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [03:52<00:00, 232.36s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [14:56<18:07, 271.93s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 43%|████▎     | 3/7 [14:56<18:07, 271.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing detroit - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:57<00:00, 57.74s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [15:53<09:22, 187.38s/it][A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 57%|█████▋    | 4/7 [15:53<09:22, 187.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing atlanta - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [02:51<00:00, 171.89s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [18:45<06:03, 181.79s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 71%|███████▏  | 5/7 [18:45<06:03, 181.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sf - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [02:44<00:00, 164.04s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [21:29<02:55, 175.76s/it]\u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                              \n",
      " 86%|████████▌ | 6/7 [21:29<02:55, 175.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [03:08<00:00, 188.71s/it]\u001b[A\n",
      "100%|██████████| 7/7 [24:38<00:00, 211.22s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/image-baselines/vit_base_patch16_224/all/faces/seed:1111/lr-0.0001-all-FacesImages/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_patch16_all\", \"faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fd3e6-08bc-48b5-9fb4-bc2129615c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "                                     \u001b[A\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chicago - nofaces\n"
     ]
    }
   ],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/image-baselines/vit_base_patch16_224/all/nofaces/seed:1111/lr-0.0001-all-NoFacesImages/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_patch16_all\", \"nofaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d29fee2-1f9b-4f47-9b49-f95bf100477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                     \u001b[A\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     7.87s/it]\u001b[A\n",
      "  0%|          | 0/1 [03:57<?, ?it/s]         \n",
      " 33%|███▎      | 1/3 [03:57<07:55, 237.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - nofaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     4.12s/it]\u001b[A\n",
      "  0%|          | 0/1 [05:33<?, ?it/s]         \n",
      " 67%|██████▋   | 2/3 [05:33<02:34, 154.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ny - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [10:12<00:00, 211.19s/it]\u001b[A\n",
      "100%|██████████| 1/1 [10:12<00:00, 612.48s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/image-baselines/vit_base_patch16_224/all/all/seed:1111/lr-0.0001-all-allImages/final_model.ckpt')\n",
    "generate_embeddings(model, \"trained_vit_patch16_all/all\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7656810-4201-4d22-8c5a-184090b8c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac254760-9804-4e56-84ca-0c2781479945",
   "metadata": {},
   "source": [
    "# Getting the false positive and true positive retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b5a3d71-7105-480f-8068-e6a7f20afbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_cls_embeddings_and_labels_from_vit(model, dataloader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward_features(inputs)\n",
    "            cls_token_embedding = outputs[:, 0, :]  # CLS token is the first token\n",
    "            embeddings.append(cls_token_embedding.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "    return torch.cat(embeddings), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4cbdcb9-7a8b-4c1b-89c1-3796ec9ce72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_face_embeddings(model, model_name, mode=\"face\"):\n",
    "    all_cities = [\"south\", \"midwest\", \"west\", \"northeast\"]\n",
    "    \n",
    "    for city in tqdm(all_cities):\n",
    "        for data_type in tqdm([\"all\"], leave=False):\n",
    "            tqdm.write(f\"Processing {city} - {data_type}\")  # Print the current city and data type being processed\n",
    "            # Construct the file path and read the CSV file\n",
    "            file_path = os.path.join(args.data_dir, file_paths[city][data_type])\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Removing vendors that have less than 2 ads\n",
    "            vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "            df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "\n",
    "            # Remapping new vendor ids\n",
    "            all_vendors = df.VENDOR.unique()\n",
    "            vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "            df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "            train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111, stratify=df['VENDOR'])\n",
    "            train_df, val_df = train_test_split(train_df, test_size=0.05, random_state=1111, stratify=train_df['VENDOR'])\n",
    "\n",
    "            # Faces Dataset\n",
    "            train_images, train_labels = load_images_and_labels(train_df, target_size=(224, 224), augment=False,\n",
    "                                                     num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "            val_images, val_labels = load_images_and_labels(val_df, target_size=(224, 224), augment=False,\n",
    "                                                     num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "            if mode == \"face\":\n",
    "                test_df = test_df[test_df['IF_FACE'] == \"yes\"]\n",
    "            else:\n",
    "                test_df = test_df[test_df['IF_FACE'] == \"no\"]\n",
    "\n",
    "            test_images, test_labels = load_images_and_labels(test_df, target_size=(224, 224), augment=False, num_augmented_samples=args.nb_augmented_samples)\n",
    "            data_module = ImageDataModule(train_images, train_labels, val_images, val_labels, test_images, test_labels, batch_size=args.batch_size, augment_data=args.augment_data)\n",
    "            # Setup the data module for training/validation and testing\n",
    "            data_module.setup('fit')\n",
    "            # data_module.setup('test')\n",
    "\n",
    "            # Extract embeddings and labels\n",
    "            train_embeddings, train_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.train_dataloader()) \n",
    "            test_embeddings, test_labels = extract_cls_embeddings_and_labels_from_vit(model, data_module.test_dataloader())\n",
    "\n",
    "            assert train_embeddings.shape[0] == train_labels.shape[0]\n",
    "            assert test_embeddings.shape[0] == test_labels.shape[0]\n",
    "\n",
    "            # Save the embeddings and labels to disk\n",
    "            file_dir = f\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/error_analysis/vision_baselines/trained_{model_name}/{mode}\"\n",
    "            Path(file_dir).mkdir(parents=True, exist_ok=True)            \n",
    "            torch.save(train_embeddings, os.path.join(file_dir, f\"{model_name}_{city}_train_embeddings.pt\"))\n",
    "            torch.save(train_labels, os.path.join(file_dir, f\"{model_name}_{city}_train_labels.pt\"))\n",
    "            torch.save(test_embeddings, os.path.join(file_dir, f\"{model_name}_{city}_test_embeddings.pt\"))\n",
    "            torch.save(test_labels, os.path.join(file_dir, f\"{model_name}_{city}_test_labels.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a4fd78d-afa1-4009-b17b-1002039ece42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model('vit_base_patch16_224', '/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/contra-learn/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-CE+SupCon/final_model.ckpt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb254245-9c45-4f0c-be3b-db68ec57c3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "                                     \u001b[A\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing south - all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [17:35<00:00, 1055.16s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [17:35<52:45, 1055.16s/it]\u001b[A\n",
      "                                               \n",
      " 25%|██▌       | 1/4 [17:35<52:45, 1055.16s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing midwest - all\n"
     ]
    }
   ],
   "source": [
    "generate_face_embeddings(model, \"vit_patch16\", mode=\"face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b486b5e-089a-41e3-b96d-0fdf81d52cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_face_embeddings(model, \"vit_patch16\", mode=\"noface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760cc47e-c822-44e9-be9d-51840bcb6f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
