{
 "cells": [
  {
   "cell_type": "raw",
   "id": "26343c5d-4a27-48b8-bcc8-65792063639f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Code Overview\n",
    "# Performs similarity search for all the trained vision-only baselines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Loading libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import timm\n",
    "\n",
    "# Custom library\n",
    "sys.path.append('../process/')\n",
    "from imageUtilities import load_images_and_labels\n",
    "from loadData import ImageDataModule\n",
    "\n",
    "sys.path.append('../architectures/')\n",
    "from visionClassifierLayer import PreTrainedVisionModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9a876f3-4c4d-4512-b53b-6ce9ed2760e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dictionary of arguments\n",
    "args_dict = {\n",
    "    \"model_name_or_path\": \"vgg16\",\n",
    "    \"logged_entry_name\": \"vgg16-seed:1111\",\n",
    "    \"data_dir\": \"/workspace/persistent/HTClipper/data/processed\",\n",
    "    \"data_type\": \"all\",\n",
    "    \"city\": \"south\",\n",
    "    \"save_dir\": \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines\",\n",
    "    \"model_dir_name\": None,\n",
    "    \"batch_size\": 32,\n",
    "    \"nb_epochs\": 40,\n",
    "    \"patience\": 3,\n",
    "    \"seed\": 1111,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"grad_steps\": 1,\n",
    "    \"learning_rate\": 6e-4,\n",
    "    \"train_data_percentage\": 1.0,\n",
    "    \"adam_epsilon\": 1e-6,\n",
    "    \"min_delta_change\": 0.01,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"augment_data\": False,\n",
    "    \"nb_augmented_samples\": 1\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an argparse Namespace\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a82f3e1-394f-4dae-8ac9-eccfda97859a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1111\n"
     ]
    }
   ],
   "source": [
    "# Setting seed value for reproducibility    \n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Making sure that the input variables are right\n",
    "assert args.data_type in [\"all\"]\n",
    "assert args.city in [\"midwest\", \"northeast\", \"south\", \"west\"]\n",
    "assert args.model_name_or_path in ['vgg16', 'vgg19', \"resnet50\", \"resnet101\", \"resnet152\", \"mobilenet\", \"mobilenetv2\", \"densenet121\", \"densenet169\", \n",
    "                                \"efficientnet-b0\", \"efficientnet-b1\", \"efficientnet-b2\", \"efficientnet-b3\", \"efficientnet-b4\", \"efficientnet-b5\", \"efficientnet-b6\",\n",
    "                                \"efficientnet-b7\", \"efficientnetv2_rw_m\", \"efficientnetv2_rw_s\", \"efficientnetv2_rw_t\", \"convnext_tiny\", \"convnext_small\", \n",
    "                                \"convnext_base\", \"convnext_large\", \"convnext_xlarge\", \"vit_base_patch16_224\", \"vit_large_patch16_224\", \"vit_base_patch32_224\", \n",
    "                                \"vit_large_patch32_224\", \"inception_v3\", \"inception_resnet_v2\" ]\n",
    "\n",
    "# Creating directories\n",
    "if args.model_dir_name == None:\n",
    "    directory = os.path.join(args.save_dir, args.model_name_or_path.split(\"/\")[-1], args.city, args.data_type, \n",
    "                            \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate))\n",
    "else:\n",
    "    directory = os.path.join(args.save_dir, args.model_name_or_path.split(\"/\")[-1], args.city, args.data_type, \n",
    "                            \"seed:\" + str(args.seed), \"lr-\" + str(args.learning_rate) + \"-\" + args.model_dir_name)\n",
    "Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %% Loading dataset\n",
    "# Map city and data_type combinations to file paths\n",
    "file_paths = {\n",
    "    \"chicago\": {\n",
    "        \"faces\": \"chicago_faces.csv\",\n",
    "        \"nofaces\": \"chicago_nofaces.csv\",\n",
    "        \"all\": \"chicago_images.csv\"\n",
    "    },\n",
    "    \"all\": {\n",
    "        \"faces\": \"all_faces.csv\",\n",
    "        \"nofaces\": \"all_nofaces.csv\",\n",
    "        \"all\": \"all_images.csv\"\n",
    "    },\n",
    "    \"south\": {\n",
    "        \"all\": \"south_images.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Construct the file path and read the CSV file\n",
    "file_path = os.path.join(args.data_dir, file_paths[args.city][args.data_type])\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Removing vendors that have less than 2 ads\n",
    "vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "\n",
    "# Remapping new vendor ids\n",
    "all_vendors = df.VENDOR.unique()\n",
    "vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "num_classes = df.VENDOR.nunique()\n",
    "assert df['VENDOR'].min() >= 0 and df['VENDOR'].max() < num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20aed395-8fd6-4ed7-b700-28e3a081dc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/vgg16/south/all/seed:1111/lr-0.0006'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68e9d868-5fda-4d90-b20c-b8bb44934785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Load and preprocess images\n",
    "# The target size is fixed to 224x224 for a fair comparison with the ViT models.\n",
    "# Turn the augment parameter to True only if you want to perform augmentation for the entire dataset\n",
    "# Otherwise, the augmentation to training data only is implemented in the ImageDataModule class\n",
    "images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)\n",
    "assert images.shape[0] == labels.shape[0]\n",
    "\n",
    "# %% Split data\n",
    "# Split ratio is set to 0.20 between training and test data, and 0.05 between training and val data\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111, stratify=labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, random_state=1111, stratify=y_train)\n",
    "\n",
    "# %% Instantiate DataModule and Model\n",
    "if args.augment_data == True:\n",
    "    data_module = ImageDataModule(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=args.batch_size, augment_data=args.augment_data, num_augmented_samples=args.nb_augmented_samples)\n",
    "else:\n",
    "    data_module = ImageDataModule(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=args.batch_size, augment_data=args.augment_data)\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbee39fe-8f21-4b1c-97c5-15f469fb2510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Loading \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import timm\n",
    "\n",
    "class PreTrainedVisionModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_classes, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = self.load_model(model_name, num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def load_model(self, model_name, num_classes):\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        if 'efficientnet' in model_name or 'efficientnetv2' in model_name:\n",
    "            num_ftrs = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        elif 'convnext' in model_name:\n",
    "            num_ftrs = model.head.fc.in_features\n",
    "            model.head.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        elif 'vit' in model_name:\n",
    "            num_ftrs = model.head.in_features\n",
    "            model.head = nn.Linear(num_ftrs, num_classes)\n",
    "        elif 'vgg' in model_name or 'densenet' in model_name:\n",
    "            model.reset_classifier(num_classes=num_classes)\n",
    "        else:\n",
    "            num_ftrs = model.get_classifier().in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        metrics = self.compute_metrics(preds, y, 'train')\n",
    "        metrics['train_loss'] = loss\n",
    "        self.log_dict(metrics)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        metrics = self.compute_metrics(preds, y, 'val')\n",
    "        metrics['val_loss'] = loss\n",
    "        self.log_dict(metrics)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        metrics = self.compute_metrics(preds, y, 'test')\n",
    "        metrics['test_loss'] = loss\n",
    "        self.log_dict(metrics)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def compute_metrics(self, preds, labels, stage):\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        precision = precision_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        recall = recall_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "        balanced_acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "        macro_f1 = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "        weighted_f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        \n",
    "        return {\n",
    "            f'{stage}_accuracy': acc,\n",
    "            f'{stage}_precision': precision,\n",
    "            f'{stage}_recall': recall,\n",
    "            f'{stage}_f1': f1,\n",
    "            f'{stage}_balanced_accuracy': balanced_acc,\n",
    "            f'{stage}_macro_f1': macro_f1,\n",
    "            f'{stage}_weighted_f1': weighted_f1,\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        model_name = self.model_name.lower()\n",
    "\n",
    "        if 'vgg' in model_name:\n",
    "            # VGG16\n",
    "            x = self.model.features(x)\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "            features = torch.flatten(x, 1)  # Shape: [batch_size, 512]\n",
    "\n",
    "        elif 'resnet' in model_name:\n",
    "            # ResNet50\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.model.global_pool(x)\n",
    "            features = torch.flatten(x, 1)  # Shape: [batch_size, 2048]\n",
    "\n",
    "        elif 'densenet' in model_name:\n",
    "            # Densenet121\n",
    "            x = self.model.features(x)\n",
    "            x = F.relu(x, inplace=True)\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "            features = torch.flatten(x, 1)  # Shape: [batch_size, 1024]\n",
    "\n",
    "        elif 'efficientnet' in model_name:\n",
    "            # EfficientNetV2_RW_S\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.model.global_pool(x)\n",
    "            features = torch.flatten(x, 1)  # Shape depends on the variant\n",
    "\n",
    "        elif 'convnext' in model_name:\n",
    "            # ConvNeXt_Small\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.model.norm_pre(x)\n",
    "            x = x.mean(dim=(2, 3))  # Global average pooling\n",
    "            features = x  # Shape: [batch_size, 768]\n",
    "\n",
    "        elif 'vit' in model_name:\n",
    "            # ViT_Base_Patch16_224\n",
    "            x = self.model.forward_features(x)\n",
    "            features = x[:, 0]  # Use the [CLS] token representation\n",
    "\n",
    "        elif 'inception' in model_name:\n",
    "            # Inception_v3\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.model.global_pool(x)\n",
    "            features = torch.flatten(x, 1)  # Shape: [batch_size, 2048]\n",
    "\n",
    "        else:\n",
    "            # Default method\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.model.global_pool(x)\n",
    "            features = torch.flatten(x, 1)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f2f5170-9bb0-4177-8a16-a9de376115f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_checkpoint(model_name):\n",
    "    if model_name == \"vgg16\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/vgg16/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "    \n",
    "    elif model_name == \"vit_base_patch16_224\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/resnet50/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "\n",
    "    elif model_name == \"densenet121\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/densenet121/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "\n",
    "    elif model_name == \"efficientnetv2_rw_s\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/efficientnetv2_rw_s/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "\n",
    "    elif model_name == \"convnext_small\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/convnext_small/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "\n",
    "    elif model_name == \"inception_v3\":\n",
    "        checkpoint = \"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/inception_v3/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\"\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Model not trained....\")\n",
    "        \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e68d30f-239e-4180-a0d2-786fe7b04983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instantiate the model architecture\n",
    "    model = PreTrainedVisionModel(\n",
    "        model_name=model_name,      # e.g., 'vgg16'\n",
    "        num_classes=num_classes,      # Replace with the actual number of classes\n",
    "    )\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(load_pretrained_checkpoint(model_name), map_location=device)\n",
    "\n",
    "    # Load the state dict into the model\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf407049-d4aa-4410-8e3c-986269d0e016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PreTrainedVisionModel.load_from_checkpoint(\"/workspace/persistent/HTClipper/models/grouped-and-masked/image-baselines/vit_base_patch16_224/south/all/seed:1111/lr-0.0001-CE/final_model.ckpt\", \n",
    "                                                  model_name=\"vit_base_patch16_224\", num_classes=num_classes).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4bd9cdc-64d9-4b29-857a-0c2ed5f3b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_embeddings(city, model, model_name, args=args):\n",
    "    # Construct the file path and read the CSV file\n",
    "    file_path = os.path.join(args.data_dir, f\"{city}_images.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Removing vendors that have less than 2 ads\n",
    "    vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "    df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "    \n",
    "    # Remapping new vendor ids\n",
    "    all_vendors = df.VENDOR.unique()\n",
    "    vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "    df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "    # %% Load and preprocess images\n",
    "    # The target size is fixed to 224x224 for a fair comparison with the ViT models.\n",
    "    # Turn the augment parameter to True only if you want to perform augmentation for the entire dataset\n",
    "    # Otherwise, the augmentation to training data only is implemented in the ImageDataModule class\n",
    "    images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                             num_augmented_samples=args.nb_augmented_samples)\n",
    "    assert images.shape[0] == labels.shape[0]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111)\n",
    "    data_module = ImageDataModule(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=args.batch_size, augment_data=False)\n",
    "    data_module.setup()\n",
    "    \n",
    "    train_dm = data_module.train_dataloader()\n",
    "    test_dm = data_module.test_dataloader()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_dm, desc=\"Extracting Train embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    train_embeddings = torch.cat(all_embeddings)\n",
    "    train_labels = torch.cat(all_labels)\n",
    "    file_dir = f\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/vision_baselines/trained_{model_name}\"\n",
    "    Path(file_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    torch.save(train_embeddings, os.path.join(file_dir, f\"{city}_visiondata_train.pt\"))\n",
    "    torch.save(train_labels, os.path.join(file_dir, f\"{city}_visionlabels_train.pt\"))\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dm, desc=\"Extracting test embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    test_embeddings = torch.cat(all_embeddings)\n",
    "    test_labels = torch.cat(all_labels)\n",
    "    \n",
    "    torch.save(test_embeddings, os.path.join(file_dir, f\"{city}_visiondata_test.pt\"))\n",
    "    torch.save(test_labels, os.path.join(file_dir, f\"{city}_visionlabels_test.pt\"))\n",
    "    \n",
    "    # return train_embeddings, train_labels, test_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa1a6c-282c-4e05-bd2e-b5aa9c98b44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in [\"vgg16\",  \"resnet50\",  \"densenet121\",  \"efficientnetv2_rw_s\",  \"convnext_small\",  \"vit_base_patch16_224\",  \"inception_v3\"]:\n",
    "    print(f\"---------------------------------------{model_name}---------------------------------------------------\")\n",
    "    model = load_pretrained_model(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    for region in [\"south\", \"northeast\", \"west\", \"midwest\"]:\n",
    "        print(f\"Region:{region}\")\n",
    "        save_embeddings(region, model=model, model_name=model_name, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b81d4d5e-70b6-4d72-b7f4-01e26f8cd38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16: 138,357,544 trainable parameters\n",
      "resnet50: 25,557,032 trainable parameters\n",
      "densenet121: 7,978,856 trainable parameters\n",
      "efficientnetv2_rw_s: 23,941,296 trainable parameters\n",
      "convnext_small: 50,223,688 trainable parameters\n",
      "vit_base_patch16_224: 86,567,656 trainable parameters\n",
      "inception_v3: 23,834,568 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "# List of model names\n",
    "model_names = [\n",
    "    \"vgg16\", \n",
    "    \"resnet50\", \n",
    "    \"densenet121\", \n",
    "    \"efficientnetv2_rw_s\", \n",
    "    \"convnext_small\", \n",
    "    \"vit_base_patch16_224\", \n",
    "    \"inception_v3\"\n",
    "]\n",
    "\n",
    "# Function to calculate total trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Dictionary to store model parameters\n",
    "model_parameters = {}\n",
    "\n",
    "# Calculate parameters for each model\n",
    "for name in model_names:\n",
    "    model = timm.create_model(name, pretrained=True, num_classes=1000)  # Adjust num_classes as needed\n",
    "    model_parameters[name] = count_trainable_parameters(model)\n",
    "\n",
    "# Print model parameters\n",
    "for name, params in model_parameters.items():\n",
    "    print(f\"{name}: {params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6392078-57d6-4fb1-917b-ccce1de0b7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a24d68-668c-44e1-bbbf-6fcea5a63a8c",
   "metadata": {},
   "source": [
    "# Generating true positive and false positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c392e3ba-201f-4754-88fc-b661abe7f7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_pretrained_model(\"vit_base_patch16_224\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3283bd28-4fea-4f89-879a-82617284da7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "trainer = L.Trainer(max_epochs=32, accelerator=\"gpu\", fast_dev_run=False, \n",
    "                    accumulate_grad_batches = 4, # To run the backward step after n batches, helps to increase the batch size\n",
    "                    benchmark = True, # Fastens the training process\n",
    "                    deterministic=True, # Ensures reproducibility \n",
    "                    limit_train_batches=1.0, # trains on 10% of the data,\n",
    "                    check_val_every_n_epoch = 1, # run val loop every 1 training epochs\n",
    "                    # callbacks=[model_checkpoint, early_stop_callback], # Enables model checkpoint and early stopping\n",
    "                    # callbacks=[early_stop_callback],\n",
    "                    # logger = wandb_logger,\n",
    "                    # strategy=DeepSpeedStrategy(stage=3, offload_optimizer=True, offload_parameters=True, offload_params_device='cpu'), # Enable CPU Offloading, and offload parameters to CPU\n",
    "                    # plugins=DeepSpeedPrecisionPlugin(precision='16-mixed') # Mixed Precision system\n",
    "                    precision='16-mixed' # Mixed Precision system\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad03ff11-a223-43f5-8787-7ae11f4fe2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 410/410 [00:10<00:00, 38.75it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.7506102919578552\n",
      " test_balanced_accuracy     0.7459064722061157\n",
      "         test_f1            0.7506102919578552\n",
      "        test_loss           1.7013839483261108\n",
      "      test_macro_f1         0.6042097806930542\n",
      "     test_precision         0.7506102919578552\n",
      "       test_recall          0.7506102919578552\n",
      "    test_weighted_f1        0.7493311166763306\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_accuracy': 0.7506102919578552,\n",
       "  'test_precision': 0.7506102919578552,\n",
       "  'test_recall': 0.7506102919578552,\n",
       "  'test_f1': 0.7506102919578552,\n",
       "  'test_balanced_accuracy': 0.7459064722061157,\n",
       "  'test_macro_f1': 0.6042097806930542,\n",
       "  'test_weighted_f1': 0.7493311166763306,\n",
       "  'test_loss': 1.7013839483261108}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, dataloaders=data_module.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40982bbf-f5ff-4c51-bcbb-41098b7eae8e",
   "metadata": {},
   "source": [
    "# Saving TP and FP results from model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8992517-53cb-47d7-ac24-81e45c630cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "train_dm = data_module.train_dataloader()\n",
    "# test_dm = data_module.test_dataloader()\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for images, labels in tqdm(train_dm, desc=\"Extracting Train predictions\"):    \n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1da0fbeb-20e0-4caf-9d82-99c9fa744407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pred_labels = [int(item) for array in pred for item in array]\n",
    "train_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f501676-3e72-4308-9e44-cf20dcfd6e36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6070953204108801)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(train_actual_labels, train_pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78b73e77-9ea8-49df-b7ee-1fe838d34f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e0fb69d-9e9e-4c5c-b922-3d5b524df5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49809, 13108)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_actual_labels), len(test_actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de3c3247-c336-454a-b271-3b8426d0ce54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../error_analysis/vit_text_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_actual_labels, f)\n",
    "    \n",
    "with open('../error_analysis/vit_classification_text_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/vit_classification_text_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed7f19fc-2bd3-43f2-b009-2ebc20c68bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pred_labels) == len(train_actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe6245-6b79-448a-9802-8cebdd49da3f",
   "metadata": {},
   "source": [
    "# Saving TP and FP results for images with and without faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cebe4da7-4e85-4c91-a058-0320a68233a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/south_images.csv\")\n",
    "\n",
    "# Removing vendors that have less than 2 ads\n",
    "vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "\n",
    "# Remapping new vendor ids\n",
    "all_vendors = df.VENDOR.unique()\n",
    "vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "num_classes = df.VENDOR.nunique()\n",
    "assert df['VENDOR'].min() >= 0 and df['VENDOR'].max() < num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "739d9731-02c9-4d36-b76d-cf77501f78f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111, stratify=df['VENDOR'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.05, random_state=1111, stratify=train_df['VENDOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2f2de8f-4655-4ff9-91ad-b6c263b8ea05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_face_vendor_dict = dict(Counter(train_df[train_df['IF_FACE'] == \"yes\"]['VENDOR']))\n",
    "train_noface_vendor_dict = dict(Counter(train_df[train_df['IF_FACE'] == \"no\"]['VENDOR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6514c047-50c4-464b-8319-8d42f5ecaec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../error_analysis/vit_face_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_face_vendor_dict, f)\n",
    "    \n",
    "with open('../error_analysis/vit_noface_train_class_freq.pkl', 'wb') as f:\n",
    "    pickle.dump(train_noface_vendor_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54644f33-4024-4d54-9bc0-cf6f6c5822cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Faces Dataset\n",
    "train_images, train_labels = load_images_and_labels(train_df, target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "val_images, val_labels = load_images_and_labels(val_df, target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "test_images, test_labels = load_images_and_labels(test_df[test_df['IF_FACE'] == \"yes\"], target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "798e446d-3d42-4c70-8756-3dc1b1292429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_module = ImageDataModule(train_images, train_labels, val_images, val_labels, test_images, test_labels, batch_size=args.batch_size, augment_data=args.augment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17f65164-1629-494a-8728-42d1452f5525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "515b599d-9e33-4071-a058-1ffedb57c5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Train predictions: 100%|██████████| 202/202 [01:42<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# train_dm = data_module.train_dataloader()\n",
    "test_dm = data_module.test_dataloader()\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for images, labels in tqdm(test_dm, desc=\"Extracting Train predictions\"):    \n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "852454d8-c48c-492c-90a7-090275451b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f32e0247-0fa1-421b-9876-125b34ea9fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../error_analysis/vit_faceclassification_text_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/vit_faceclassification_text_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffdb34c8-ce3a-4a7a-827b-c64920163bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1033, 309, 1228, 249, 876, 905, 0, 404, 1, 214],\n",
       " [1033, 309, 1228, 249, 876, 31, 0, 404, 1, 214])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_labels[:10], test_actual_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c63d51c3-3280-4b98-95db-463686b5a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Faces Dataset\n",
    "train_images, train_labels = load_images_and_labels(train_df, target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "val_images, val_labels = load_images_and_labels(val_df, target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "test_images, test_labels = load_images_and_labels(test_df[test_df['IF_FACE'] == \"yes\"], target_size=(224, 224), augment=False,\n",
    "                                         num_augmented_samples=args.nb_augmented_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73a831a6-a4ed-481e-afcc-75153298d71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_module = ImageDataModule(train_images, train_labels, val_images, val_labels, test_images, test_labels, batch_size=args.batch_size, augment_data=args.augment_data)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31a754ee-cbf7-4a60-a2a3-7e74462c2ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Train predictions: 100%|██████████| 208/208 [01:30<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pred' and 'actual' are lists intended to collect predictions and actual labels\n",
    "pred, actual = ([] for i in range(2))\n",
    "\n",
    "# train_dm = data_module.train_dataloader()\n",
    "test_dm = data_module.test_dataloader()\n",
    "\n",
    "# Iterate over the test dataloader with a tqdm progress bar\n",
    "for images, labels in tqdm(test_dm, desc=\"Extracting Train predictions\"):    \n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Append predictions and labels to their respective lists\n",
    "    pred.append(preds.cpu().numpy())\n",
    "    actual.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35735056-5e3b-466b-a8db-526c7535992e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [int(item) for array in pred for item in array]\n",
    "test_actual_labels = [int(item) for array in actual for item in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2aa0252-7d94-42fc-98a0-d102a3acf044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../error_analysis/vit_nofaceclassification_text_test_pred_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pred_labels, f)\n",
    "    \n",
    "with open('../error_analysis/vit_nofaceclassification_text_test_act_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_actual_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89537b91-e636-4b9e-ad4f-c74867c598d7",
   "metadata": {},
   "source": [
    "# Getting the TP and FP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f5729-c5bc-42a2-868d-1b314e22352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(\"../data/processed/south\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ead601-b410-4e06-9a2a-c73ad3982138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_embeddings(city, model, model_name, args=args):\n",
    "    # Construct the file path and read the CSV file\n",
    "    file_path = os.path.join(args.data_dir, f\"{city}_images.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Removing vendors that have less than 2 ads\n",
    "    vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "    df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "    \n",
    "    # Remapping new vendor ids\n",
    "    all_vendors = df.VENDOR.unique()\n",
    "    vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "    df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "\n",
    "    # %% Load and preprocess images\n",
    "    # The target size is fixed to 224x224 for a fair comparison with the ViT models.\n",
    "    # Turn the augment parameter to True only if you want to perform augmentation for the entire dataset\n",
    "    # Otherwise, the augmentation to training data only is implemented in the ImageDataModule class\n",
    "    images, labels = load_images_and_labels(df, target_size=(224, 224), augment=False,\n",
    "                                             num_augmented_samples=args.nb_augmented_samples)\n",
    "    assert images.shape[0] == labels.shape[0]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=1111)\n",
    "    \n",
    "    data_module = ImageDataModule(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=args.batch_size, augment_data=False)\n",
    "    data_module.setup()\n",
    "    \n",
    "    train_dm = data_module.train_dataloader()\n",
    "    test_dm = data_module.test_dataloader()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_dm, desc=\"Extracting Train embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    train_embeddings = torch.cat(all_embeddings)\n",
    "    train_labels = torch.cat(all_labels)\n",
    "    file_dir = f\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/vision_baselines/trained_{model_name}\"\n",
    "    Path(file_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    torch.save(train_embeddings, os.path.join(file_dir, f\"{city}_visiondata_train.pt\"))\n",
    "    torch.save(train_labels, os.path.join(file_dir, f\"{city}_visionlabels_train.pt\"))\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dm, desc=\"Extracting test embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    test_embeddings = torch.cat(all_embeddings)\n",
    "    test_labels = torch.cat(all_labels)\n",
    "    \n",
    "    torch.save(test_embeddings, os.path.join(file_dir, f\"{city}_visiondata_test.pt\"))\n",
    "    torch.save(test_labels, os.path.join(file_dir, f\"{city}_visionlabels_test.pt\"))\n",
    "    \n",
    "    # return train_embeddings, train_labels, test_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23a021-d417-4945-a081-9959e8e9b07a",
   "metadata": {},
   "source": [
    "# Retrieval Faces and NoFaces embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e518c45-98da-49a7-8f4d-6e61c395c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_face_embeddings(city, model, model_name, mode=\"face\", args=args):\n",
    "    # Construct the file path and read the CSV file\n",
    "    file_path = os.path.join(args.data_dir, f\"{city}_images.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Removing vendors that have less than 2 ads\n",
    "    vendors_of_interest = {k:v for k, v in dict(Counter(df.VENDOR)).items() if v>1}\n",
    "    df = df[df['VENDOR'].isin(list(vendors_of_interest.keys()))]\n",
    "    \n",
    "    # Remapping new vendor ids\n",
    "    all_vendors = df.VENDOR.unique()\n",
    "    vendor_to_idx_dict = {vendor: idx for idx, vendor in enumerate(all_vendors)}\n",
    "    df[\"VENDOR\"] = df[\"VENDOR\"].replace(vendor_to_idx_dict)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.20, random_state=1111, stratify=df['VENDOR'])\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.05, random_state=1111, stratify=train_df['VENDOR'])\n",
    "    \n",
    "    # Faces Dataset\n",
    "    train_images, train_labels = load_images_and_labels(train_df, target_size=(224, 224), augment=False,\n",
    "                                             num_augmented_samples=args.nb_augmented_samples)\n",
    "\n",
    "    val_images, val_labels = load_images_and_labels(val_df, target_size=(224, 224), augment=False,\n",
    "                                             num_augmented_samples=args.nb_augmented_samples)\n",
    "    \n",
    "    if mode == \"face\":\n",
    "        test_df = test_df[test_df['IF_FACE'] == \"yes\"]\n",
    "    else:\n",
    "        test_df = test_df[test_df['IF_FACE'] == \"yes\"]\n",
    "\n",
    "    test_images, test_labels = load_images_and_labels(test_df, target_size=(224, 224), augment=False, num_augmented_samples=args.nb_augmented_samples)\n",
    "    data_module = ImageDataModule(train_images, train_labels, val_images, val_labels, test_images, test_labels, batch_size=args.batch_size, augment_data=args.augment_data)\n",
    "    data_module.setup()\n",
    "    \n",
    "    train_dm = data_module.train_dataloader()\n",
    "    test_dm = data_module.test_dataloader()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_dm, desc=\"Extracting Train embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    train_embeddings = torch.cat(all_embeddings)\n",
    "    train_labels = torch.cat(all_labels)\n",
    "    file_dir = f\"/workspace/persistent/HTClipper/models/pickled/embeddings/grouped-and-masked/error_analysis/vision_baselines/trained_{model_name}/{mode}\"\n",
    "    Path(file_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    torch.save(train_embeddings, os.path.join(file_dir, f\"{city}_visiondata_train.pt\"))\n",
    "    torch.save(train_labels, os.path.join(file_dir, f\"{city}_visionlabels_train.pt\"))\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dm, desc=\"Extracting test embeddings\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings = model.extract_features(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    test_embeddings = torch.cat(all_embeddings)\n",
    "    test_labels = torch.cat(all_labels)\n",
    "    \n",
    "    torch.save(test_embeddings, os.path.join(file_dir, f\"{city}_visiondata_test.pt\"))\n",
    "    torch.save(test_labels, os.path.join(file_dir, f\"{city}_visionlabels_test.pt\"))\n",
    "    \n",
    "    # return train_embeddings, train_labels, test_embeddings, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec7f1a-3aa4-40e7-9ee4-28d10956a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"vit_base_patch16_224\"]:\n",
    "    print(f\"---------------------------------------{model_name}---------------------------------------------------\")\n",
    "    model = load_pretrained_model(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    for region in [\"south\", \"northeast\", \"west\", \"midwest\"]:\n",
    "        print(f\"Region:{region}\")\n",
    "        save_embeddings(region, model=model, model_name=model_name, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76f9fd49-9379-483d-aca5-1bab180e61a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(\"../data/processed/northeast_images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9149867-fd34-4f01-86e2-2a812f70deed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>IMAGE</th>\n",
       "      <th>VENDOR</th>\n",
       "      <th>IF_FACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45885</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>538</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>45885</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>538</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>45885</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>538</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45886</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>539</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>45886</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>539</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>14742</td>\n",
       "      <td>58748</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>671</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14743</th>\n",
       "      <td>14743</td>\n",
       "      <td>58748</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>671</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14744</th>\n",
       "      <td>14744</td>\n",
       "      <td>58748</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>671</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14745</th>\n",
       "      <td>14745</td>\n",
       "      <td>58748</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>671</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14746</th>\n",
       "      <td>14746</td>\n",
       "      <td>58748</td>\n",
       "      <td>/workspace/persistent/HTClipper/data/IMAGES/ny...</td>\n",
       "      <td>671</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14747 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     ID                                              IMAGE  \\\n",
       "0               0  45885  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "1               1  45885  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "2               2  45885  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "3               3  45886  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "4               4  45886  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "...           ...    ...                                                ...   \n",
       "14742       14742  58748  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "14743       14743  58748  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "14744       14744  58748  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "14745       14745  58748  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "14746       14746  58748  /workspace/persistent/HTClipper/data/IMAGES/ny...   \n",
       "\n",
       "       VENDOR IF_FACE  \n",
       "0         538     yes  \n",
       "1         538     yes  \n",
       "2         538      no  \n",
       "3         539     yes  \n",
       "4         539     yes  \n",
       "...       ...     ...  \n",
       "14742     671     yes  \n",
       "14743     671     yes  \n",
       "14744     671     yes  \n",
       "14745     671     yes  \n",
       "14746     671     yes  \n",
       "\n",
       "[14747 rows x 5 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29535667-7f36-4920-a05a-66d08b7156c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HT",
   "language": "python",
   "name": "ht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
